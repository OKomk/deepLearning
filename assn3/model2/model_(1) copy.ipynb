{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam, SGD, Adagrad, RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, Callback\n",
    "from PIL import Image\n",
    "import os\n",
    "from keras.utils import to_categorical\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapDict = {0: 0, 3: 1, 5: 2, 6: 3, 7: 4}\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "folder_path = '../Group_18/train'\n",
    "\n",
    "for foldername in os.listdir('../Group_18/train/'):\n",
    "    for filename in os.listdir(os.path.join(folder_path, foldername)):\n",
    "        flattened = np.array(Image.open(os.path.join(folder_path, foldername, filename))).flatten()\n",
    "        X_train.append(flattened)\n",
    "        y_train.append(int(foldername))\n",
    "\n",
    "X_train = np.array(X_train) / 255\n",
    "y_train = np.array(y_train)\n",
    "for i, n in enumerate(y_train):\n",
    "    y_train[i] = mapDict[y_train[i]]\n",
    "y_train = to_categorical(y_train, num_classes=5)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "folder_path = '../Group_18/test'\n",
    "\n",
    "for foldername in os.listdir(folder_path):\n",
    "    for filename in os.listdir(os.path.join(folder_path, foldername)):\n",
    "        flattened = np.array(Image.open(os.path.join(folder_path, foldername, filename))).flatten()\n",
    "        X_test.append(flattened)\n",
    "        y_test.append(int(foldername))\n",
    "\n",
    "X_test = np.array(X_test) / 255\n",
    "y_test = np.array(y_test)\n",
    "for i, n in enumerate(y_test):\n",
    "    y_test[i] = mapDict[y_test[i]]\n",
    "y_test = to_categorical(y_test, num_classes=5)\n",
    "\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "\n",
    "folder_path = '../Group_18/val'\n",
    "\n",
    "for foldername in os.listdir(folder_path):\n",
    "    for filename in os.listdir(os.path.join(folder_path, foldername)):\n",
    "        flattened = np.array(Image.open(os.path.join(folder_path, foldername, filename))).flatten()\n",
    "        X_val.append(flattened)\n",
    "        y_val.append(int(foldername))\n",
    "\n",
    "X_val = np.array(X_val) / 255\n",
    "y_val = np.array(y_val)\n",
    "for i, n in enumerate(y_val):\n",
    "    y_val[i] = mapDict[y_val[i]]\n",
    "y_val = to_categorical(y_val, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=42)\n",
    "model = keras.Sequential([\n",
    "        layers.Dense(30, activation=\"sigmoid\", name=\"layer1\",kernel_initializer=initializer),\n",
    "        layers.Dense(20, activation=\"sigmoid\", name=\"layer2\",kernel_initializer=initializer),\n",
    "        layers.Dense(10, activation=\"sigmoid\", name=\"layer3\",kernel_initializer=initializer),\n",
    "        layers.Dense(5, activation=\"softmax\", name=\"output\",kernel_initializer=initializer),\n",
    "        ])\n",
    "model.build((None, 784))\n",
    "initial_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_optimizer = SGD(learning_rate=0.001,name='SGD') #Normal Gradient Descent\n",
    "momentum_optimizer = SGD(learning_rate=0.001, momentum=0.9, name='Momentum_SGD') #Momentum Based\n",
    "nag_optimizer = SGD(learning_rate=0.001, momentum=0.9, nesterov=True, name='NAG') #NAG\n",
    "rms_optimizer = RMSprop(learning_rate=0.001, rho=0.99, momentum=0.0, epsilon=1e-08, name=\"RMSProp\") #RMSProp\n",
    "adagrad_optimizer = Adagrad(learning_rate=0.001, epsilon=1e-08, name=\"Adagrad\") #AdaGrad\n",
    "adam_optimizer = Adam(learning_rate=0.001) #Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(\"model_checkpoint_Adagrad.h5\")\n",
    "class AverageLossCallback(Callback):\n",
    "   def __init__(self, patience=2, threshold=0.0005):\n",
    "      super().__init__()\n",
    "      self.patience = patience\n",
    "      self.threshold = threshold\n",
    "      self.best_loss = np.Inf\n",
    "      self.wait = 0\n",
    "      self.losses = []\n",
    "\n",
    "   def on_epoch_end(self, epoch, logs=None):\n",
    "      if logs is None:\n",
    "         logs = {}\n",
    "      train_loss = logs.get('loss')\n",
    "      if train_loss is None:\n",
    "         return\n",
    "      self.losses.append(train_loss)\n",
    "      if len(self.losses) > self.patience:\n",
    "         cur_loss = np.mean(self.losses[-self.patience:])\n",
    "         prev_loss = np.mean(self.losses[-self.patience-1:-1])\n",
    "         if abs(cur_loss - prev_loss) < self.threshold:\n",
    "               self.wait += 1\n",
    "               if self.wait >= self.patience:\n",
    "                  self.model.stop_training = True\n",
    "         else:\n",
    "               self.wait = 0\n",
    "               \n",
    "average_loss_callback = AverageLossCallback()\n",
    "my_callbacks = [\n",
    "   average_loss_callback]\n",
    "\n",
    "# ,\n",
    "#    ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "#    TensorBoard(log_dir='./logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_dict = {} # number of epochs each optimizer took till convergence\n",
    "loss_dict = {} # loss at each epoch for each optimizer\n",
    "eval_dict = {} # (train_accuracy, val_accuracy) for each optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "    1/11385 [..............................] - ETA: 57:31 - loss: 0.6730 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.0020s). Check your callbacks.\n",
      "11385/11385 [==============================] - 14s 1ms/step - loss: 0.5502 - accuracy: 0.2000 - val_loss: 0.5053 - val_accuracy: 0.2000\n",
      "Epoch 2/1000\n",
      "11385/11385 [==============================] - 13s 1ms/step - loss: 0.5018 - accuracy: 0.1983 - val_loss: 0.5005 - val_accuracy: 0.2000\n",
      "Epoch 3/1000\n",
      "11385/11385 [==============================] - 13s 1ms/step - loss: 0.5005 - accuracy: 0.1954 - val_loss: 0.5004 - val_accuracy: 0.2000\n",
      "Epoch 4/1000\n",
      "11385/11385 [==============================] - 16s 1ms/step - loss: 0.5005 - accuracy: 0.1934 - val_loss: 0.5004 - val_accuracy: 0.2000\n",
      "Epoch 5/1000\n",
      "11385/11385 [==============================] - 15s 1ms/step - loss: 0.5005 - accuracy: 0.1955 - val_loss: 0.5004 - val_accuracy: 0.2000\n",
      "Epoch 6/1000\n",
      "11385/11385 [==============================] - 13s 1ms/step - loss: 0.5005 - accuracy: 0.1960 - val_loss: 0.5004 - val_accuracy: 0.2000\n"
     ]
    }
   ],
   "source": [
    "# SGD Optimizer\n",
    "model.set_weights(initial_weights)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd_optimizer)\n",
    "model_fit = model.fit(x=X_train, y=y_train, batch_size=1, epochs=1000, verbose=\"auto\", callbacks=[\n",
    "   average_loss_callback,\n",
    "   ModelCheckpoint(filepath=f\"model_checkpoint_{model.optimizer.get_config()['name']}.h5\",\n",
    "                   save_best_only=True,  # Only save the best performing model\n",
    "                    monitor='loss',  # Monitor the validation loss\n",
    "                    mode='min',          # Minimize the validation loss\n",
    "                    save_weights_only=False, # Save the entire model (including optimizer state)\n",
    "                    append_history=True,),\n",
    "   TensorBoard(log_dir='./logs')],\n",
    "    validation_split=0.0, validation_data=(X_val, y_val), shuffle=True, validation_batch_size=None)\n",
    "\n",
    "train_loss_list = model_fit.history['loss']\n",
    "\n",
    "num_epochs = len(model_fit.epoch)\n",
    "num_epochs_dict['SGD'] = num_epochs\n",
    "loss_dict['SGD'] = train_loss_list\n",
    "\n",
    "train_accuracy, val_accuracy = model_fit.history['accuracy'][-1], model_fit.history['val_accuracy'][-1]\n",
    "\n",
    "eval_dict['SGD'] = (train_accuracy, val_accuracy) \n",
    "\n",
    "with open('model_history_sgd.pkl', 'wb') as f:\n",
    "    pickle.dump(model_fit.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 1s 984ms/step - loss: 0.6263 - accuracy: 0.2000 - val_loss: 0.6257 - val_accuracy: 0.2000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 429ms/step - loss: 0.6258 - accuracy: 0.2000 - val_loss: 0.6253 - val_accuracy: 0.2000\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 398ms/step - loss: 0.6253 - accuracy: 0.2000 - val_loss: 0.6248 - val_accuracy: 0.2000\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 0.6248 - accuracy: 0.2000 - val_loss: 0.6243 - val_accuracy: 0.2000\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 0.6244 - accuracy: 0.2000 - val_loss: 0.6238 - val_accuracy: 0.2000\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 405ms/step - loss: 0.6239 - accuracy: 0.2000 - val_loss: 0.6233 - val_accuracy: 0.2000\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 405ms/step - loss: 0.6234 - accuracy: 0.2000 - val_loss: 0.6228 - val_accuracy: 0.2000\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 398ms/step - loss: 0.6229 - accuracy: 0.2000 - val_loss: 0.6223 - val_accuracy: 0.2000\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 0.6224 - accuracy: 0.2000 - val_loss: 0.6219 - val_accuracy: 0.2000\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 395ms/step - loss: 0.6219 - accuracy: 0.2000 - val_loss: 0.6214 - val_accuracy: 0.2000\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 0.6215 - accuracy: 0.2000 - val_loss: 0.6209 - val_accuracy: 0.2000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 422ms/step - loss: 0.6210 - accuracy: 0.2000 - val_loss: 0.6204 - val_accuracy: 0.2000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 404ms/step - loss: 0.6205 - accuracy: 0.2000 - val_loss: 0.6200 - val_accuracy: 0.2000\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 0.6200 - accuracy: 0.2000 - val_loss: 0.6195 - val_accuracy: 0.2000\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 395ms/step - loss: 0.6196 - accuracy: 0.2000 - val_loss: 0.6190 - val_accuracy: 0.2000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 0.6191 - accuracy: 0.2000 - val_loss: 0.6186 - val_accuracy: 0.2000\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 0.6186 - accuracy: 0.2000 - val_loss: 0.6181 - val_accuracy: 0.2000\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 0.6182 - accuracy: 0.2000 - val_loss: 0.6176 - val_accuracy: 0.2000\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 395ms/step - loss: 0.6177 - accuracy: 0.2000 - val_loss: 0.6172 - val_accuracy: 0.2000\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 395ms/step - loss: 0.6173 - accuracy: 0.2000 - val_loss: 0.6167 - val_accuracy: 0.2000\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 0.6168 - accuracy: 0.2000 - val_loss: 0.6163 - val_accuracy: 0.2000\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.6163 - accuracy: 0.2000 - val_loss: 0.6158 - val_accuracy: 0.2000\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 395ms/step - loss: 0.6159 - accuracy: 0.2000 - val_loss: 0.6154 - val_accuracy: 0.2000\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.6154 - accuracy: 0.2000 - val_loss: 0.6149 - val_accuracy: 0.2000\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.6150 - accuracy: 0.2000 - val_loss: 0.6145 - val_accuracy: 0.2000\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.6145 - accuracy: 0.2000 - val_loss: 0.6140 - val_accuracy: 0.2000\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.6141 - accuracy: 0.2000 - val_loss: 0.6136 - val_accuracy: 0.2000\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.6137 - accuracy: 0.2000 - val_loss: 0.6131 - val_accuracy: 0.2000\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.6132 - accuracy: 0.2000 - val_loss: 0.6127 - val_accuracy: 0.2000\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 0.6128 - accuracy: 0.2000 - val_loss: 0.6123 - val_accuracy: 0.2000\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.6123 - accuracy: 0.2000 - val_loss: 0.6118 - val_accuracy: 0.2000\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.6119 - accuracy: 0.2000 - val_loss: 0.6114 - val_accuracy: 0.2000\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.6115 - accuracy: 0.2000 - val_loss: 0.6110 - val_accuracy: 0.2000\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.6110 - accuracy: 0.2000 - val_loss: 0.6105 - val_accuracy: 0.2000\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.6106 - accuracy: 0.2000 - val_loss: 0.6101 - val_accuracy: 0.2000\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.6102 - accuracy: 0.2000 - val_loss: 0.6097 - val_accuracy: 0.2000\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.6097 - accuracy: 0.2000 - val_loss: 0.6092 - val_accuracy: 0.2000\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.6093 - accuracy: 0.2000 - val_loss: 0.6088 - val_accuracy: 0.2000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.6089 - accuracy: 0.2000 - val_loss: 0.6084 - val_accuracy: 0.2000\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.6085 - accuracy: 0.2000 - val_loss: 0.6080 - val_accuracy: 0.2000\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.6080 - accuracy: 0.2000 - val_loss: 0.6076 - val_accuracy: 0.2000\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.6076 - accuracy: 0.2000 - val_loss: 0.6071 - val_accuracy: 0.2000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.6072 - accuracy: 0.2000 - val_loss: 0.6067 - val_accuracy: 0.2000\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.6068 - accuracy: 0.2000 - val_loss: 0.6063 - val_accuracy: 0.2000\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.6064 - accuracy: 0.2000 - val_loss: 0.6059 - val_accuracy: 0.2000\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.6060 - accuracy: 0.2000 - val_loss: 0.6055 - val_accuracy: 0.2000\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.6056 - accuracy: 0.2000 - val_loss: 0.6051 - val_accuracy: 0.2000\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.6051 - accuracy: 0.2000 - val_loss: 0.6047 - val_accuracy: 0.2000\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.6047 - accuracy: 0.2000 - val_loss: 0.6043 - val_accuracy: 0.2000\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.6043 - accuracy: 0.2000 - val_loss: 0.6039 - val_accuracy: 0.2000\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.6039 - accuracy: 0.2000 - val_loss: 0.6035 - val_accuracy: 0.2000\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.6035 - accuracy: 0.2000 - val_loss: 0.6031 - val_accuracy: 0.2000\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.6031 - accuracy: 0.2000 - val_loss: 0.6027 - val_accuracy: 0.2000\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.6027 - accuracy: 0.2000 - val_loss: 0.6023 - val_accuracy: 0.2000\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.6023 - accuracy: 0.2000 - val_loss: 0.6019 - val_accuracy: 0.2000\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 401ms/step - loss: 0.6019 - accuracy: 0.2000 - val_loss: 0.6015 - val_accuracy: 0.2000\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.6015 - accuracy: 0.2000 - val_loss: 0.6011 - val_accuracy: 0.2000\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.6012 - accuracy: 0.2000 - val_loss: 0.6007 - val_accuracy: 0.2000\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.6008 - accuracy: 0.2000 - val_loss: 0.6003 - val_accuracy: 0.2000\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.6004 - accuracy: 0.2000 - val_loss: 0.5999 - val_accuracy: 0.2000\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.6000 - accuracy: 0.2000 - val_loss: 0.5995 - val_accuracy: 0.2000\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5996 - accuracy: 0.2000 - val_loss: 0.5991 - val_accuracy: 0.2000\n",
      "Epoch 63/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5992 - accuracy: 0.2000 - val_loss: 0.5988 - val_accuracy: 0.2000\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5988 - accuracy: 0.2000 - val_loss: 0.5984 - val_accuracy: 0.2000\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 0.5984 - accuracy: 0.2000 - val_loss: 0.5980 - val_accuracy: 0.2000\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5981 - accuracy: 0.2000 - val_loss: 0.5976 - val_accuracy: 0.2000\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5977 - accuracy: 0.2000 - val_loss: 0.5972 - val_accuracy: 0.2000\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5973 - accuracy: 0.2000 - val_loss: 0.5969 - val_accuracy: 0.2000\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5969 - accuracy: 0.2000 - val_loss: 0.5965 - val_accuracy: 0.2000\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5966 - accuracy: 0.2000 - val_loss: 0.5961 - val_accuracy: 0.2000\n",
      "Epoch 71/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5962 - accuracy: 0.2000 - val_loss: 0.5957 - val_accuracy: 0.2000\n",
      "Epoch 72/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5958 - accuracy: 0.2000 - val_loss: 0.5954 - val_accuracy: 0.2000\n",
      "Epoch 73/1000\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 0.5955 - accuracy: 0.2000 - val_loss: 0.5950 - val_accuracy: 0.2000\n",
      "Epoch 74/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5951 - accuracy: 0.2000 - val_loss: 0.5946 - val_accuracy: 0.2000\n",
      "Epoch 75/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5947 - accuracy: 0.2000 - val_loss: 0.5943 - val_accuracy: 0.2000\n",
      "Epoch 76/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5944 - accuracy: 0.2000 - val_loss: 0.5939 - val_accuracy: 0.2000\n",
      "Epoch 77/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5940 - accuracy: 0.2000 - val_loss: 0.5936 - val_accuracy: 0.2000\n",
      "Epoch 78/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5936 - accuracy: 0.2000 - val_loss: 0.5932 - val_accuracy: 0.2000\n",
      "Epoch 79/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5933 - accuracy: 0.2000 - val_loss: 0.5928 - val_accuracy: 0.2000\n",
      "Epoch 80/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5929 - accuracy: 0.2000 - val_loss: 0.5925 - val_accuracy: 0.2000\n",
      "Epoch 81/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5926 - accuracy: 0.2000 - val_loss: 0.5921 - val_accuracy: 0.2000\n",
      "Epoch 82/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5922 - accuracy: 0.2000 - val_loss: 0.5918 - val_accuracy: 0.2000\n",
      "Epoch 83/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5919 - accuracy: 0.2000 - val_loss: 0.5914 - val_accuracy: 0.2000\n",
      "Epoch 84/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5915 - accuracy: 0.2000 - val_loss: 0.5911 - val_accuracy: 0.2000\n",
      "Epoch 85/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5912 - accuracy: 0.2000 - val_loss: 0.5907 - val_accuracy: 0.2000\n",
      "Epoch 86/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5908 - accuracy: 0.2000 - val_loss: 0.5904 - val_accuracy: 0.2000\n",
      "Epoch 87/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5905 - accuracy: 0.2000 - val_loss: 0.5900 - val_accuracy: 0.2000\n",
      "Epoch 88/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5901 - accuracy: 0.2000 - val_loss: 0.5897 - val_accuracy: 0.2000\n",
      "Epoch 89/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5898 - accuracy: 0.2000 - val_loss: 0.5893 - val_accuracy: 0.2000\n",
      "Epoch 90/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5894 - accuracy: 0.2000 - val_loss: 0.5890 - val_accuracy: 0.2000\n",
      "Epoch 91/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5891 - accuracy: 0.2000 - val_loss: 0.5887 - val_accuracy: 0.2000\n",
      "Epoch 92/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5887 - accuracy: 0.2000 - val_loss: 0.5883 - val_accuracy: 0.2000\n",
      "Epoch 93/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5884 - accuracy: 0.2000 - val_loss: 0.5880 - val_accuracy: 0.2000\n",
      "Epoch 94/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5881 - accuracy: 0.2000 - val_loss: 0.5876 - val_accuracy: 0.2000\n",
      "Epoch 95/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5877 - accuracy: 0.2000 - val_loss: 0.5873 - val_accuracy: 0.2000\n",
      "Epoch 96/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5874 - accuracy: 0.2000 - val_loss: 0.5870 - val_accuracy: 0.2000\n",
      "Epoch 97/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5871 - accuracy: 0.2000 - val_loss: 0.5866 - val_accuracy: 0.2000\n",
      "Epoch 98/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5867 - accuracy: 0.2000 - val_loss: 0.5863 - val_accuracy: 0.2000\n",
      "Epoch 99/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5864 - accuracy: 0.2000 - val_loss: 0.5860 - val_accuracy: 0.2000\n",
      "Epoch 100/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5861 - accuracy: 0.2000 - val_loss: 0.5857 - val_accuracy: 0.2000\n",
      "Epoch 101/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5857 - accuracy: 0.2000 - val_loss: 0.5853 - val_accuracy: 0.2000\n",
      "Epoch 102/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5854 - accuracy: 0.2000 - val_loss: 0.5850 - val_accuracy: 0.2000\n",
      "Epoch 103/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5851 - accuracy: 0.2000 - val_loss: 0.5847 - val_accuracy: 0.2000\n",
      "Epoch 104/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5848 - accuracy: 0.2000 - val_loss: 0.5844 - val_accuracy: 0.2000\n",
      "Epoch 105/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5844 - accuracy: 0.2000 - val_loss: 0.5840 - val_accuracy: 0.2000\n",
      "Epoch 106/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5841 - accuracy: 0.2000 - val_loss: 0.5837 - val_accuracy: 0.2000\n",
      "Epoch 107/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5838 - accuracy: 0.2000 - val_loss: 0.5834 - val_accuracy: 0.2000\n",
      "Epoch 108/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5835 - accuracy: 0.2000 - val_loss: 0.5831 - val_accuracy: 0.2000\n",
      "Epoch 109/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5832 - accuracy: 0.2000 - val_loss: 0.5828 - val_accuracy: 0.2000\n",
      "Epoch 110/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5828 - accuracy: 0.2000 - val_loss: 0.5824 - val_accuracy: 0.2000\n",
      "Epoch 111/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5825 - accuracy: 0.2000 - val_loss: 0.5821 - val_accuracy: 0.2000\n",
      "Epoch 112/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5822 - accuracy: 0.2000 - val_loss: 0.5818 - val_accuracy: 0.2000\n",
      "Epoch 113/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5819 - accuracy: 0.2000 - val_loss: 0.5815 - val_accuracy: 0.2000\n",
      "Epoch 114/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5816 - accuracy: 0.2000 - val_loss: 0.5812 - val_accuracy: 0.2000\n",
      "Epoch 115/1000\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 0.5813 - accuracy: 0.2000 - val_loss: 0.5809 - val_accuracy: 0.2000\n",
      "Epoch 116/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5810 - accuracy: 0.2000 - val_loss: 0.5806 - val_accuracy: 0.2000\n",
      "Epoch 117/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5807 - accuracy: 0.2000 - val_loss: 0.5803 - val_accuracy: 0.2000\n",
      "Epoch 118/1000\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 0.5804 - accuracy: 0.2000 - val_loss: 0.5800 - val_accuracy: 0.2000\n",
      "Epoch 119/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5800 - accuracy: 0.2000 - val_loss: 0.5797 - val_accuracy: 0.2000\n",
      "Epoch 120/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5797 - accuracy: 0.2000 - val_loss: 0.5794 - val_accuracy: 0.2000\n",
      "Epoch 121/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5794 - accuracy: 0.2000 - val_loss: 0.5791 - val_accuracy: 0.2000\n",
      "Epoch 122/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5791 - accuracy: 0.2000 - val_loss: 0.5788 - val_accuracy: 0.2000\n",
      "Epoch 123/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5788 - accuracy: 0.2000 - val_loss: 0.5785 - val_accuracy: 0.2000\n",
      "Epoch 124/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5785 - accuracy: 0.2000 - val_loss: 0.5782 - val_accuracy: 0.2000\n",
      "Epoch 125/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5782 - accuracy: 0.2000 - val_loss: 0.5779 - val_accuracy: 0.2000\n",
      "Epoch 126/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5779 - accuracy: 0.2000 - val_loss: 0.5776 - val_accuracy: 0.2000\n",
      "Epoch 127/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5777 - accuracy: 0.2000 - val_loss: 0.5773 - val_accuracy: 0.2000\n",
      "Epoch 128/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5774 - accuracy: 0.2000 - val_loss: 0.5770 - val_accuracy: 0.2000\n",
      "Epoch 129/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5771 - accuracy: 0.2000 - val_loss: 0.5767 - val_accuracy: 0.2000\n",
      "Epoch 130/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5768 - accuracy: 0.2000 - val_loss: 0.5764 - val_accuracy: 0.2000\n",
      "Epoch 131/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5765 - accuracy: 0.2000 - val_loss: 0.5761 - val_accuracy: 0.2000\n",
      "Epoch 132/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5762 - accuracy: 0.2000 - val_loss: 0.5758 - val_accuracy: 0.2000\n",
      "Epoch 133/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5759 - accuracy: 0.2000 - val_loss: 0.5755 - val_accuracy: 0.2000\n",
      "Epoch 134/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5756 - accuracy: 0.2000 - val_loss: 0.5753 - val_accuracy: 0.2000\n",
      "Epoch 135/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5753 - accuracy: 0.2000 - val_loss: 0.5750 - val_accuracy: 0.2000\n",
      "Epoch 136/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5751 - accuracy: 0.2000 - val_loss: 0.5747 - val_accuracy: 0.2000\n",
      "Epoch 137/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5748 - accuracy: 0.2000 - val_loss: 0.5744 - val_accuracy: 0.2000\n",
      "Epoch 138/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5745 - accuracy: 0.2000 - val_loss: 0.5741 - val_accuracy: 0.2000\n",
      "Epoch 139/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5742 - accuracy: 0.2000 - val_loss: 0.5739 - val_accuracy: 0.2000\n",
      "Epoch 140/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5739 - accuracy: 0.2000 - val_loss: 0.5736 - val_accuracy: 0.2000\n",
      "Epoch 141/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5737 - accuracy: 0.2000 - val_loss: 0.5733 - val_accuracy: 0.2000\n",
      "Epoch 142/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5734 - accuracy: 0.2000 - val_loss: 0.5730 - val_accuracy: 0.2000\n",
      "Epoch 143/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5731 - accuracy: 0.2000 - val_loss: 0.5727 - val_accuracy: 0.2000\n",
      "Epoch 144/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5728 - accuracy: 0.2000 - val_loss: 0.5725 - val_accuracy: 0.2000\n",
      "Epoch 145/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5725 - accuracy: 0.2000 - val_loss: 0.5722 - val_accuracy: 0.2000\n",
      "Epoch 146/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5723 - accuracy: 0.2000 - val_loss: 0.5719 - val_accuracy: 0.2000\n",
      "Epoch 147/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5720 - accuracy: 0.2000 - val_loss: 0.5717 - val_accuracy: 0.2000\n",
      "Epoch 148/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5717 - accuracy: 0.2000 - val_loss: 0.5714 - val_accuracy: 0.2000\n",
      "Epoch 149/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5715 - accuracy: 0.2000 - val_loss: 0.5711 - val_accuracy: 0.2000\n",
      "Epoch 150/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5712 - accuracy: 0.2000 - val_loss: 0.5709 - val_accuracy: 0.2000\n",
      "Epoch 151/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5709 - accuracy: 0.2000 - val_loss: 0.5706 - val_accuracy: 0.2000\n",
      "Epoch 152/1000\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 0.5707 - accuracy: 0.2000 - val_loss: 0.5703 - val_accuracy: 0.2000\n",
      "Epoch 153/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5704 - accuracy: 0.2000 - val_loss: 0.5701 - val_accuracy: 0.2000\n",
      "Epoch 154/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5701 - accuracy: 0.2000 - val_loss: 0.5698 - val_accuracy: 0.2000\n",
      "Epoch 155/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5699 - accuracy: 0.2000 - val_loss: 0.5695 - val_accuracy: 0.2000\n",
      "Epoch 156/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5696 - accuracy: 0.2000 - val_loss: 0.5693 - val_accuracy: 0.2000\n",
      "Epoch 157/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5693 - accuracy: 0.2000 - val_loss: 0.5690 - val_accuracy: 0.2000\n",
      "Epoch 158/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5691 - accuracy: 0.2000 - val_loss: 0.5688 - val_accuracy: 0.2000\n",
      "Epoch 159/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5688 - accuracy: 0.2000 - val_loss: 0.5685 - val_accuracy: 0.2000\n",
      "Epoch 160/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5686 - accuracy: 0.2000 - val_loss: 0.5682 - val_accuracy: 0.2000\n",
      "Epoch 161/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5683 - accuracy: 0.2000 - val_loss: 0.5680 - val_accuracy: 0.2000\n",
      "Epoch 162/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5681 - accuracy: 0.2000 - val_loss: 0.5677 - val_accuracy: 0.2000\n",
      "Epoch 163/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5678 - accuracy: 0.2000 - val_loss: 0.5675 - val_accuracy: 0.2000\n",
      "Epoch 164/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5676 - accuracy: 0.2000 - val_loss: 0.5672 - val_accuracy: 0.2000\n",
      "Epoch 165/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5673 - accuracy: 0.2000 - val_loss: 0.5670 - val_accuracy: 0.2000\n",
      "Epoch 166/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5670 - accuracy: 0.2000 - val_loss: 0.5667 - val_accuracy: 0.2000\n",
      "Epoch 167/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5668 - accuracy: 0.2000 - val_loss: 0.5665 - val_accuracy: 0.2000\n",
      "Epoch 168/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5665 - accuracy: 0.2000 - val_loss: 0.5662 - val_accuracy: 0.2000\n",
      "Epoch 169/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5663 - accuracy: 0.2000 - val_loss: 0.5660 - val_accuracy: 0.2000\n",
      "Epoch 170/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5661 - accuracy: 0.2000 - val_loss: 0.5657 - val_accuracy: 0.2000\n",
      "Epoch 171/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5658 - accuracy: 0.2000 - val_loss: 0.5655 - val_accuracy: 0.2000\n",
      "Epoch 172/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5656 - accuracy: 0.2000 - val_loss: 0.5652 - val_accuracy: 0.2000\n",
      "Epoch 173/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5653 - accuracy: 0.2000 - val_loss: 0.5650 - val_accuracy: 0.2000\n",
      "Epoch 174/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5651 - accuracy: 0.2000 - val_loss: 0.5648 - val_accuracy: 0.2000\n",
      "Epoch 175/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5648 - accuracy: 0.2000 - val_loss: 0.5645 - val_accuracy: 0.2000\n",
      "Epoch 176/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5646 - accuracy: 0.2000 - val_loss: 0.5643 - val_accuracy: 0.2000\n",
      "Epoch 177/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5644 - accuracy: 0.2000 - val_loss: 0.5640 - val_accuracy: 0.2000\n",
      "Epoch 178/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5641 - accuracy: 0.2000 - val_loss: 0.5638 - val_accuracy: 0.2000\n",
      "Epoch 179/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5639 - accuracy: 0.2000 - val_loss: 0.5636 - val_accuracy: 0.2000\n",
      "Epoch 180/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5636 - accuracy: 0.2000 - val_loss: 0.5633 - val_accuracy: 0.2000\n",
      "Epoch 181/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5634 - accuracy: 0.2000 - val_loss: 0.5631 - val_accuracy: 0.2000\n",
      "Epoch 182/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5632 - accuracy: 0.2000 - val_loss: 0.5629 - val_accuracy: 0.2000\n",
      "Epoch 183/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5629 - accuracy: 0.2000 - val_loss: 0.5626 - val_accuracy: 0.2000\n",
      "Epoch 184/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5627 - accuracy: 0.2000 - val_loss: 0.5624 - val_accuracy: 0.2000\n",
      "Epoch 185/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5625 - accuracy: 0.2000 - val_loss: 0.5622 - val_accuracy: 0.2000\n",
      "Epoch 186/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5622 - accuracy: 0.2000 - val_loss: 0.5619 - val_accuracy: 0.2000\n",
      "Epoch 187/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5620 - accuracy: 0.2000 - val_loss: 0.5617 - val_accuracy: 0.2000\n",
      "Epoch 188/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5618 - accuracy: 0.2000 - val_loss: 0.5615 - val_accuracy: 0.2000\n",
      "Epoch 189/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5615 - accuracy: 0.2000 - val_loss: 0.5612 - val_accuracy: 0.2000\n",
      "Epoch 190/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5613 - accuracy: 0.2000 - val_loss: 0.5610 - val_accuracy: 0.2000\n",
      "Epoch 191/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5611 - accuracy: 0.2000 - val_loss: 0.5608 - val_accuracy: 0.2000\n",
      "Epoch 192/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5609 - accuracy: 0.2000 - val_loss: 0.5606 - val_accuracy: 0.2000\n",
      "Epoch 193/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5606 - accuracy: 0.2000 - val_loss: 0.5603 - val_accuracy: 0.2000\n",
      "Epoch 194/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5604 - accuracy: 0.2000 - val_loss: 0.5601 - val_accuracy: 0.2000\n",
      "Epoch 195/1000\n",
      "1/1 [==============================] - 0s 398ms/step - loss: 0.5602 - accuracy: 0.2000 - val_loss: 0.5599 - val_accuracy: 0.2000\n",
      "Epoch 196/1000\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 0.5600 - accuracy: 0.2000 - val_loss: 0.5597 - val_accuracy: 0.2000\n",
      "Epoch 197/1000\n",
      "1/1 [==============================] - 0s 403ms/step - loss: 0.5597 - accuracy: 0.2000 - val_loss: 0.5594 - val_accuracy: 0.2000\n",
      "Epoch 198/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5595 - accuracy: 0.2000 - val_loss: 0.5592 - val_accuracy: 0.2000\n",
      "Epoch 199/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5593 - accuracy: 0.2000 - val_loss: 0.5590 - val_accuracy: 0.2000\n",
      "Epoch 200/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5591 - accuracy: 0.2000 - val_loss: 0.5588 - val_accuracy: 0.2000\n",
      "Epoch 201/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5589 - accuracy: 0.2000 - val_loss: 0.5586 - val_accuracy: 0.2000\n",
      "Epoch 202/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5587 - accuracy: 0.2000 - val_loss: 0.5584 - val_accuracy: 0.2000\n",
      "Epoch 203/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5584 - accuracy: 0.2000 - val_loss: 0.5581 - val_accuracy: 0.2000\n",
      "Epoch 204/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5582 - accuracy: 0.2000 - val_loss: 0.5579 - val_accuracy: 0.2000\n",
      "Epoch 205/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5580 - accuracy: 0.2000 - val_loss: 0.5577 - val_accuracy: 0.2000\n",
      "Epoch 206/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5578 - accuracy: 0.2000 - val_loss: 0.5575 - val_accuracy: 0.2000\n",
      "Epoch 207/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5576 - accuracy: 0.2000 - val_loss: 0.5573 - val_accuracy: 0.2000\n",
      "Epoch 208/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5574 - accuracy: 0.2000 - val_loss: 0.5571 - val_accuracy: 0.2000\n",
      "Epoch 209/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5572 - accuracy: 0.2000 - val_loss: 0.5569 - val_accuracy: 0.2000\n",
      "Epoch 210/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5569 - accuracy: 0.2000 - val_loss: 0.5567 - val_accuracy: 0.2000\n",
      "Epoch 211/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5567 - accuracy: 0.2000 - val_loss: 0.5565 - val_accuracy: 0.2000\n",
      "Epoch 212/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5565 - accuracy: 0.2000 - val_loss: 0.5562 - val_accuracy: 0.2000\n",
      "Epoch 213/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5563 - accuracy: 0.2000 - val_loss: 0.5560 - val_accuracy: 0.2000\n",
      "Epoch 214/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5561 - accuracy: 0.2000 - val_loss: 0.5558 - val_accuracy: 0.2000\n",
      "Epoch 215/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5559 - accuracy: 0.2000 - val_loss: 0.5556 - val_accuracy: 0.2000\n",
      "Epoch 216/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5557 - accuracy: 0.2000 - val_loss: 0.5554 - val_accuracy: 0.2000\n",
      "Epoch 217/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5555 - accuracy: 0.2000 - val_loss: 0.5552 - val_accuracy: 0.2000\n",
      "Epoch 218/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5553 - accuracy: 0.2000 - val_loss: 0.5550 - val_accuracy: 0.2000\n",
      "Epoch 219/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5551 - accuracy: 0.2000 - val_loss: 0.5548 - val_accuracy: 0.2000\n",
      "Epoch 220/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5549 - accuracy: 0.2000 - val_loss: 0.5546 - val_accuracy: 0.2000\n",
      "Epoch 221/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5547 - accuracy: 0.2000 - val_loss: 0.5544 - val_accuracy: 0.2000\n",
      "Epoch 222/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5545 - accuracy: 0.2000 - val_loss: 0.5542 - val_accuracy: 0.2000\n",
      "Epoch 223/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5543 - accuracy: 0.2000 - val_loss: 0.5540 - val_accuracy: 0.2000\n",
      "Epoch 224/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5541 - accuracy: 0.2000 - val_loss: 0.5538 - val_accuracy: 0.2000\n",
      "Epoch 225/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5539 - accuracy: 0.2000 - val_loss: 0.5536 - val_accuracy: 0.2000\n",
      "Epoch 226/1000\n",
      "1/1 [==============================] - 0s 406ms/step - loss: 0.5537 - accuracy: 0.2000 - val_loss: 0.5534 - val_accuracy: 0.2000\n",
      "Epoch 227/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5535 - accuracy: 0.2000 - val_loss: 0.5532 - val_accuracy: 0.2000\n",
      "Epoch 228/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5533 - accuracy: 0.2000 - val_loss: 0.5530 - val_accuracy: 0.2000\n",
      "Epoch 229/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5531 - accuracy: 0.2000 - val_loss: 0.5528 - val_accuracy: 0.2000\n",
      "Epoch 230/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5529 - accuracy: 0.2000 - val_loss: 0.5526 - val_accuracy: 0.2000\n",
      "Epoch 231/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5527 - accuracy: 0.2000 - val_loss: 0.5525 - val_accuracy: 0.2000\n",
      "Epoch 232/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5525 - accuracy: 0.2000 - val_loss: 0.5523 - val_accuracy: 0.2000\n",
      "Epoch 233/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5523 - accuracy: 0.2000 - val_loss: 0.5521 - val_accuracy: 0.2000\n",
      "Epoch 234/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5521 - accuracy: 0.2000 - val_loss: 0.5519 - val_accuracy: 0.2000\n",
      "Epoch 235/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5520 - accuracy: 0.2000 - val_loss: 0.5517 - val_accuracy: 0.2000\n",
      "Epoch 236/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5518 - accuracy: 0.2000 - val_loss: 0.5515 - val_accuracy: 0.2000\n",
      "Epoch 237/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5516 - accuracy: 0.2000 - val_loss: 0.5513 - val_accuracy: 0.2000\n",
      "Epoch 238/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5514 - accuracy: 0.2000 - val_loss: 0.5511 - val_accuracy: 0.2000\n",
      "Epoch 239/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5512 - accuracy: 0.2000 - val_loss: 0.5509 - val_accuracy: 0.2000\n",
      "Epoch 240/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5510 - accuracy: 0.2000 - val_loss: 0.5508 - val_accuracy: 0.2000\n",
      "Epoch 241/1000\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 0.5508 - accuracy: 0.2000 - val_loss: 0.5506 - val_accuracy: 0.2000\n",
      "Epoch 242/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5506 - accuracy: 0.2000 - val_loss: 0.5504 - val_accuracy: 0.2000\n",
      "Epoch 243/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5505 - accuracy: 0.2000 - val_loss: 0.5502 - val_accuracy: 0.2000\n",
      "Epoch 244/1000\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 0.5503 - accuracy: 0.2000 - val_loss: 0.5500 - val_accuracy: 0.2000\n",
      "Epoch 245/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5501 - accuracy: 0.2000 - val_loss: 0.5498 - val_accuracy: 0.2000\n",
      "Epoch 246/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5499 - accuracy: 0.2000 - val_loss: 0.5497 - val_accuracy: 0.2000\n",
      "Epoch 247/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5497 - accuracy: 0.2000 - val_loss: 0.5495 - val_accuracy: 0.2000\n",
      "Epoch 248/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5496 - accuracy: 0.2000 - val_loss: 0.5493 - val_accuracy: 0.2000\n",
      "Epoch 249/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5494 - accuracy: 0.2000 - val_loss: 0.5491 - val_accuracy: 0.2000\n",
      "Epoch 250/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5492 - accuracy: 0.2000 - val_loss: 0.5489 - val_accuracy: 0.2000\n",
      "Epoch 251/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5490 - accuracy: 0.2000 - val_loss: 0.5488 - val_accuracy: 0.2000\n",
      "Epoch 252/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5488 - accuracy: 0.2000 - val_loss: 0.5486 - val_accuracy: 0.2000\n",
      "Epoch 253/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5487 - accuracy: 0.2000 - val_loss: 0.5484 - val_accuracy: 0.2000\n",
      "Epoch 254/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5485 - accuracy: 0.2000 - val_loss: 0.5482 - val_accuracy: 0.2000\n",
      "Epoch 255/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5483 - accuracy: 0.2000 - val_loss: 0.5481 - val_accuracy: 0.2000\n",
      "Epoch 256/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5481 - accuracy: 0.2000 - val_loss: 0.5479 - val_accuracy: 0.2000\n",
      "Epoch 257/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5480 - accuracy: 0.2000 - val_loss: 0.5477 - val_accuracy: 0.2000\n",
      "Epoch 258/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5478 - accuracy: 0.2000 - val_loss: 0.5475 - val_accuracy: 0.2000\n",
      "Epoch 259/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5476 - accuracy: 0.2000 - val_loss: 0.5474 - val_accuracy: 0.2000\n",
      "Epoch 260/1000\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 0.5474 - accuracy: 0.2000 - val_loss: 0.5472 - val_accuracy: 0.2000\n",
      "Epoch 261/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5473 - accuracy: 0.2000 - val_loss: 0.5470 - val_accuracy: 0.2000\n",
      "Epoch 262/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5471 - accuracy: 0.2000 - val_loss: 0.5469 - val_accuracy: 0.2000\n",
      "Epoch 263/1000\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 0.5469 - accuracy: 0.2000 - val_loss: 0.5467 - val_accuracy: 0.2000\n",
      "Epoch 264/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5468 - accuracy: 0.2000 - val_loss: 0.5465 - val_accuracy: 0.2000\n",
      "Epoch 265/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5466 - accuracy: 0.2000 - val_loss: 0.5463 - val_accuracy: 0.2000\n",
      "Epoch 266/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5464 - accuracy: 0.2000 - val_loss: 0.5462 - val_accuracy: 0.2000\n",
      "Epoch 267/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5463 - accuracy: 0.2000 - val_loss: 0.5460 - val_accuracy: 0.2000\n",
      "Epoch 268/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5461 - accuracy: 0.2000 - val_loss: 0.5458 - val_accuracy: 0.2000\n",
      "Epoch 269/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5459 - accuracy: 0.2000 - val_loss: 0.5457 - val_accuracy: 0.2000\n",
      "Epoch 270/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5458 - accuracy: 0.2000 - val_loss: 0.5455 - val_accuracy: 0.2000\n",
      "Epoch 271/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5456 - accuracy: 0.2000 - val_loss: 0.5454 - val_accuracy: 0.2000\n",
      "Epoch 272/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5454 - accuracy: 0.2000 - val_loss: 0.5452 - val_accuracy: 0.2000\n",
      "Epoch 273/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5453 - accuracy: 0.2000 - val_loss: 0.5450 - val_accuracy: 0.2000\n",
      "Epoch 274/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5451 - accuracy: 0.2000 - val_loss: 0.5449 - val_accuracy: 0.2000\n",
      "Epoch 275/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5449 - accuracy: 0.2000 - val_loss: 0.5447 - val_accuracy: 0.2000\n",
      "Epoch 276/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5448 - accuracy: 0.2000 - val_loss: 0.5445 - val_accuracy: 0.2000\n",
      "Epoch 277/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5446 - accuracy: 0.2000 - val_loss: 0.5444 - val_accuracy: 0.2000\n",
      "Epoch 278/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5445 - accuracy: 0.2000 - val_loss: 0.5442 - val_accuracy: 0.2000\n",
      "Epoch 279/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5443 - accuracy: 0.2000 - val_loss: 0.5441 - val_accuracy: 0.2000\n",
      "Epoch 280/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5441 - accuracy: 0.2000 - val_loss: 0.5439 - val_accuracy: 0.2000\n",
      "Epoch 281/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5440 - accuracy: 0.2000 - val_loss: 0.5437 - val_accuracy: 0.2000\n",
      "Epoch 282/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5438 - accuracy: 0.2000 - val_loss: 0.5436 - val_accuracy: 0.2000\n",
      "Epoch 283/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5437 - accuracy: 0.2000 - val_loss: 0.5434 - val_accuracy: 0.2000\n",
      "Epoch 284/1000\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 0.5435 - accuracy: 0.2000 - val_loss: 0.5433 - val_accuracy: 0.2000\n",
      "Epoch 285/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5434 - accuracy: 0.2000 - val_loss: 0.5431 - val_accuracy: 0.2000\n",
      "Epoch 286/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5432 - accuracy: 0.2000 - val_loss: 0.5430 - val_accuracy: 0.2000\n",
      "Epoch 287/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5430 - accuracy: 0.2000 - val_loss: 0.5428 - val_accuracy: 0.2000\n",
      "Epoch 288/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5429 - accuracy: 0.2000 - val_loss: 0.5427 - val_accuracy: 0.2000\n",
      "Epoch 289/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5427 - accuracy: 0.2000 - val_loss: 0.5425 - val_accuracy: 0.2000\n",
      "Epoch 290/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5426 - accuracy: 0.2000 - val_loss: 0.5424 - val_accuracy: 0.2000\n",
      "Epoch 291/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5424 - accuracy: 0.2000 - val_loss: 0.5422 - val_accuracy: 0.2000\n",
      "Epoch 292/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5423 - accuracy: 0.2000 - val_loss: 0.5421 - val_accuracy: 0.2000\n",
      "Epoch 293/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5421 - accuracy: 0.2000 - val_loss: 0.5419 - val_accuracy: 0.2000\n",
      "Epoch 294/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5420 - accuracy: 0.2000 - val_loss: 0.5418 - val_accuracy: 0.2000\n",
      "Epoch 295/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5418 - accuracy: 0.2000 - val_loss: 0.5416 - val_accuracy: 0.2000\n",
      "Epoch 296/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5417 - accuracy: 0.2000 - val_loss: 0.5415 - val_accuracy: 0.2000\n",
      "Epoch 297/1000\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 0.5415 - accuracy: 0.2000 - val_loss: 0.5413 - val_accuracy: 0.2000\n",
      "Epoch 298/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5414 - accuracy: 0.2000 - val_loss: 0.5412 - val_accuracy: 0.2000\n",
      "Epoch 299/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5412 - accuracy: 0.2000 - val_loss: 0.5410 - val_accuracy: 0.2000\n",
      "Epoch 300/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5411 - accuracy: 0.2000 - val_loss: 0.5409 - val_accuracy: 0.2000\n",
      "Epoch 301/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5409 - accuracy: 0.2000 - val_loss: 0.5407 - val_accuracy: 0.2000\n",
      "Epoch 302/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5408 - accuracy: 0.2000 - val_loss: 0.5406 - val_accuracy: 0.2000\n",
      "Epoch 303/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5407 - accuracy: 0.2000 - val_loss: 0.5404 - val_accuracy: 0.2000\n",
      "Epoch 304/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5405 - accuracy: 0.2000 - val_loss: 0.5403 - val_accuracy: 0.2000\n",
      "Epoch 305/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5404 - accuracy: 0.2000 - val_loss: 0.5401 - val_accuracy: 0.2000\n",
      "Epoch 306/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5402 - accuracy: 0.2000 - val_loss: 0.5400 - val_accuracy: 0.2000\n",
      "Epoch 307/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5401 - accuracy: 0.2000 - val_loss: 0.5399 - val_accuracy: 0.2000\n",
      "Epoch 308/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5399 - accuracy: 0.2000 - val_loss: 0.5397 - val_accuracy: 0.2000\n",
      "Epoch 309/1000\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 0.5398 - accuracy: 0.2000 - val_loss: 0.5396 - val_accuracy: 0.2000\n",
      "Epoch 310/1000\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 0.5397 - accuracy: 0.2000 - val_loss: 0.5394 - val_accuracy: 0.2000\n",
      "Epoch 311/1000\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 0.5395 - accuracy: 0.2000 - val_loss: 0.5393 - val_accuracy: 0.2000\n",
      "Epoch 312/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5394 - accuracy: 0.2000 - val_loss: 0.5392 - val_accuracy: 0.2000\n",
      "Epoch 313/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5392 - accuracy: 0.2000 - val_loss: 0.5390 - val_accuracy: 0.2000\n",
      "Epoch 314/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5391 - accuracy: 0.2000 - val_loss: 0.5389 - val_accuracy: 0.2000\n",
      "Epoch 315/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5390 - accuracy: 0.2000 - val_loss: 0.5387 - val_accuracy: 0.2000\n",
      "Epoch 316/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5388 - accuracy: 0.2000 - val_loss: 0.5386 - val_accuracy: 0.2000\n",
      "Epoch 317/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5387 - accuracy: 0.2000 - val_loss: 0.5385 - val_accuracy: 0.2000\n",
      "Epoch 318/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5385 - accuracy: 0.2000 - val_loss: 0.5383 - val_accuracy: 0.2000\n",
      "Epoch 319/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5384 - accuracy: 0.2000 - val_loss: 0.5382 - val_accuracy: 0.2000\n",
      "Epoch 320/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5383 - accuracy: 0.2000 - val_loss: 0.5381 - val_accuracy: 0.2000\n",
      "Epoch 321/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5381 - accuracy: 0.2000 - val_loss: 0.5379 - val_accuracy: 0.2000\n",
      "Epoch 322/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5380 - accuracy: 0.2000 - val_loss: 0.5378 - val_accuracy: 0.2000\n",
      "Epoch 323/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5379 - accuracy: 0.2000 - val_loss: 0.5377 - val_accuracy: 0.2000\n",
      "Epoch 324/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5377 - accuracy: 0.2000 - val_loss: 0.5375 - val_accuracy: 0.2000\n",
      "Epoch 325/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5376 - accuracy: 0.2000 - val_loss: 0.5374 - val_accuracy: 0.2000\n",
      "Epoch 326/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5375 - accuracy: 0.2000 - val_loss: 0.5373 - val_accuracy: 0.2000\n",
      "Epoch 327/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5373 - accuracy: 0.2000 - val_loss: 0.5371 - val_accuracy: 0.2000\n",
      "Epoch 328/1000\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 0.5372 - accuracy: 0.2000 - val_loss: 0.5370 - val_accuracy: 0.2000\n",
      "Epoch 329/1000\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 0.5371 - accuracy: 0.2000 - val_loss: 0.5369 - val_accuracy: 0.2000\n",
      "Epoch 330/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5369 - accuracy: 0.2000 - val_loss: 0.5367 - val_accuracy: 0.2000\n",
      "Epoch 331/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5368 - accuracy: 0.2000 - val_loss: 0.5366 - val_accuracy: 0.2000\n",
      "Epoch 332/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5367 - accuracy: 0.2000 - val_loss: 0.5365 - val_accuracy: 0.2000\n",
      "Epoch 333/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5366 - accuracy: 0.2000 - val_loss: 0.5363 - val_accuracy: 0.2000\n",
      "Epoch 334/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5364 - accuracy: 0.2000 - val_loss: 0.5362 - val_accuracy: 0.2000\n",
      "Epoch 335/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5363 - accuracy: 0.2000 - val_loss: 0.5361 - val_accuracy: 0.2000\n",
      "Epoch 336/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5362 - accuracy: 0.2000 - val_loss: 0.5360 - val_accuracy: 0.2000\n",
      "Epoch 337/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5360 - accuracy: 0.2000 - val_loss: 0.5358 - val_accuracy: 0.2000\n",
      "Epoch 338/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5359 - accuracy: 0.2000 - val_loss: 0.5357 - val_accuracy: 0.2000\n",
      "Epoch 339/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5358 - accuracy: 0.2000 - val_loss: 0.5356 - val_accuracy: 0.2000\n",
      "Epoch 340/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5357 - accuracy: 0.2000 - val_loss: 0.5355 - val_accuracy: 0.2000\n",
      "Epoch 341/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5355 - accuracy: 0.2000 - val_loss: 0.5353 - val_accuracy: 0.2000\n",
      "Epoch 342/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5354 - accuracy: 0.2000 - val_loss: 0.5352 - val_accuracy: 0.2000\n",
      "Epoch 343/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5353 - accuracy: 0.2000 - val_loss: 0.5351 - val_accuracy: 0.2000\n",
      "Epoch 344/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5352 - accuracy: 0.2000 - val_loss: 0.5350 - val_accuracy: 0.2000\n",
      "Epoch 345/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5350 - accuracy: 0.2000 - val_loss: 0.5348 - val_accuracy: 0.2000\n",
      "Epoch 346/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5349 - accuracy: 0.2000 - val_loss: 0.5347 - val_accuracy: 0.2000\n",
      "Epoch 347/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5348 - accuracy: 0.2000 - val_loss: 0.5346 - val_accuracy: 0.2000\n",
      "Epoch 348/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5347 - accuracy: 0.2000 - val_loss: 0.5345 - val_accuracy: 0.2000\n",
      "Epoch 349/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5345 - accuracy: 0.2000 - val_loss: 0.5344 - val_accuracy: 0.2000\n",
      "Epoch 350/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5344 - accuracy: 0.2000 - val_loss: 0.5342 - val_accuracy: 0.2000\n",
      "Epoch 351/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5343 - accuracy: 0.2000 - val_loss: 0.5341 - val_accuracy: 0.2000\n",
      "Epoch 352/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5342 - accuracy: 0.2000 - val_loss: 0.5340 - val_accuracy: 0.2000\n",
      "Epoch 353/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5341 - accuracy: 0.2000 - val_loss: 0.5339 - val_accuracy: 0.2000\n",
      "Epoch 354/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5339 - accuracy: 0.2000 - val_loss: 0.5338 - val_accuracy: 0.2000\n",
      "Epoch 355/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5338 - accuracy: 0.2000 - val_loss: 0.5336 - val_accuracy: 0.2000\n",
      "Epoch 356/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5337 - accuracy: 0.2000 - val_loss: 0.5335 - val_accuracy: 0.2000\n",
      "Epoch 357/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5336 - accuracy: 0.2000 - val_loss: 0.5334 - val_accuracy: 0.2000\n",
      "Epoch 358/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5335 - accuracy: 0.2000 - val_loss: 0.5333 - val_accuracy: 0.2000\n",
      "Epoch 359/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5334 - accuracy: 0.2000 - val_loss: 0.5332 - val_accuracy: 0.2000\n",
      "Epoch 360/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5332 - accuracy: 0.2000 - val_loss: 0.5331 - val_accuracy: 0.2000\n",
      "Epoch 361/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5331 - accuracy: 0.2000 - val_loss: 0.5329 - val_accuracy: 0.2000\n",
      "Epoch 362/1000\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 0.5330 - accuracy: 0.2000 - val_loss: 0.5328 - val_accuracy: 0.2000\n",
      "Epoch 363/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5329 - accuracy: 0.2000 - val_loss: 0.5327 - val_accuracy: 0.2000\n",
      "Epoch 364/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5328 - accuracy: 0.2000 - val_loss: 0.5326 - val_accuracy: 0.2000\n",
      "Epoch 365/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5327 - accuracy: 0.2000 - val_loss: 0.5325 - val_accuracy: 0.2000\n",
      "Epoch 366/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5326 - accuracy: 0.2000 - val_loss: 0.5324 - val_accuracy: 0.2000\n",
      "Epoch 367/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5324 - accuracy: 0.2000 - val_loss: 0.5323 - val_accuracy: 0.2000\n",
      "Epoch 368/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5323 - accuracy: 0.2000 - val_loss: 0.5321 - val_accuracy: 0.2000\n",
      "Epoch 369/1000\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 0.5322 - accuracy: 0.2000 - val_loss: 0.5320 - val_accuracy: 0.2000\n",
      "Epoch 370/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5321 - accuracy: 0.2000 - val_loss: 0.5319 - val_accuracy: 0.2000\n",
      "Epoch 371/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5320 - accuracy: 0.2000 - val_loss: 0.5318 - val_accuracy: 0.2000\n",
      "Epoch 372/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5319 - accuracy: 0.2000 - val_loss: 0.5317 - val_accuracy: 0.2000\n",
      "Epoch 373/1000\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 0.5318 - accuracy: 0.2000 - val_loss: 0.5316 - val_accuracy: 0.2000\n",
      "Epoch 374/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5317 - accuracy: 0.2000 - val_loss: 0.5315 - val_accuracy: 0.2000\n",
      "Epoch 375/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5316 - accuracy: 0.2000 - val_loss: 0.5314 - val_accuracy: 0.2000\n",
      "Epoch 376/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5314 - accuracy: 0.2000 - val_loss: 0.5313 - val_accuracy: 0.2000\n",
      "Epoch 377/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5313 - accuracy: 0.2000 - val_loss: 0.5311 - val_accuracy: 0.2000\n",
      "Epoch 378/1000\n",
      "1/1 [==============================] - 0s 395ms/step - loss: 0.5312 - accuracy: 0.2000 - val_loss: 0.5310 - val_accuracy: 0.2000\n",
      "Epoch 379/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5311 - accuracy: 0.2000 - val_loss: 0.5309 - val_accuracy: 0.2000\n",
      "Epoch 380/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5310 - accuracy: 0.2000 - val_loss: 0.5308 - val_accuracy: 0.2000\n",
      "Epoch 381/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5309 - accuracy: 0.2000 - val_loss: 0.5307 - val_accuracy: 0.2000\n",
      "Epoch 382/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5308 - accuracy: 0.2000 - val_loss: 0.5306 - val_accuracy: 0.2000\n",
      "Epoch 383/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5307 - accuracy: 0.2000 - val_loss: 0.5305 - val_accuracy: 0.2000\n",
      "Epoch 384/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5306 - accuracy: 0.2000 - val_loss: 0.5304 - val_accuracy: 0.2000\n",
      "Epoch 385/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5305 - accuracy: 0.2000 - val_loss: 0.5303 - val_accuracy: 0.2000\n",
      "Epoch 386/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5304 - accuracy: 0.2000 - val_loss: 0.5302 - val_accuracy: 0.2000\n",
      "Epoch 387/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5303 - accuracy: 0.2000 - val_loss: 0.5301 - val_accuracy: 0.2000\n",
      "Epoch 388/1000\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 0.5302 - accuracy: 0.2000 - val_loss: 0.5300 - val_accuracy: 0.2000\n",
      "Epoch 389/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5301 - accuracy: 0.2000 - val_loss: 0.5299 - val_accuracy: 0.2000\n",
      "Epoch 390/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5300 - accuracy: 0.2000 - val_loss: 0.5298 - val_accuracy: 0.2000\n",
      "Epoch 391/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5298 - accuracy: 0.2000 - val_loss: 0.5297 - val_accuracy: 0.2000\n",
      "Epoch 392/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5297 - accuracy: 0.2000 - val_loss: 0.5296 - val_accuracy: 0.2000\n",
      "Epoch 393/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5296 - accuracy: 0.2000 - val_loss: 0.5295 - val_accuracy: 0.2000\n",
      "Epoch 394/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5295 - accuracy: 0.2000 - val_loss: 0.5294 - val_accuracy: 0.2000\n",
      "Epoch 395/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5294 - accuracy: 0.2000 - val_loss: 0.5293 - val_accuracy: 0.2000\n",
      "Epoch 396/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5293 - accuracy: 0.2000 - val_loss: 0.5292 - val_accuracy: 0.2000\n",
      "Epoch 397/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5292 - accuracy: 0.2000 - val_loss: 0.5291 - val_accuracy: 0.2000\n",
      "Epoch 398/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5291 - accuracy: 0.2000 - val_loss: 0.5290 - val_accuracy: 0.2000\n",
      "Epoch 399/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5290 - accuracy: 0.2000 - val_loss: 0.5289 - val_accuracy: 0.2000\n",
      "Epoch 400/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5289 - accuracy: 0.2000 - val_loss: 0.5288 - val_accuracy: 0.2000\n",
      "Epoch 401/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5288 - accuracy: 0.2000 - val_loss: 0.5287 - val_accuracy: 0.2000\n",
      "Epoch 402/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5287 - accuracy: 0.2000 - val_loss: 0.5286 - val_accuracy: 0.2000\n",
      "Epoch 403/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5286 - accuracy: 0.2000 - val_loss: 0.5285 - val_accuracy: 0.2000\n",
      "Epoch 404/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5285 - accuracy: 0.2000 - val_loss: 0.5284 - val_accuracy: 0.2000\n",
      "Epoch 405/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5284 - accuracy: 0.2000 - val_loss: 0.5283 - val_accuracy: 0.2000\n",
      "Epoch 406/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5283 - accuracy: 0.2000 - val_loss: 0.5282 - val_accuracy: 0.2000\n",
      "Epoch 407/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5282 - accuracy: 0.2000 - val_loss: 0.5281 - val_accuracy: 0.2000\n",
      "Epoch 408/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5281 - accuracy: 0.2000 - val_loss: 0.5280 - val_accuracy: 0.2000\n",
      "Epoch 409/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5281 - accuracy: 0.2000 - val_loss: 0.5279 - val_accuracy: 0.2000\n",
      "Epoch 410/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5280 - accuracy: 0.2000 - val_loss: 0.5278 - val_accuracy: 0.2000\n",
      "Epoch 411/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5279 - accuracy: 0.2000 - val_loss: 0.5277 - val_accuracy: 0.2000\n",
      "Epoch 412/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5278 - accuracy: 0.2000 - val_loss: 0.5276 - val_accuracy: 0.2000\n",
      "Epoch 413/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5277 - accuracy: 0.2000 - val_loss: 0.5275 - val_accuracy: 0.2000\n",
      "Epoch 414/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5276 - accuracy: 0.2000 - val_loss: 0.5274 - val_accuracy: 0.2000\n",
      "Epoch 415/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5275 - accuracy: 0.2000 - val_loss: 0.5273 - val_accuracy: 0.2000\n",
      "Epoch 416/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5274 - accuracy: 0.2000 - val_loss: 0.5272 - val_accuracy: 0.2000\n",
      "Epoch 417/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5273 - accuracy: 0.2000 - val_loss: 0.5271 - val_accuracy: 0.2000\n",
      "Epoch 418/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5272 - accuracy: 0.2000 - val_loss: 0.5270 - val_accuracy: 0.2000\n",
      "Epoch 419/1000\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 0.5271 - accuracy: 0.2000 - val_loss: 0.5269 - val_accuracy: 0.2000\n",
      "Epoch 420/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5270 - accuracy: 0.2000 - val_loss: 0.5268 - val_accuracy: 0.2000\n",
      "Epoch 421/1000\n",
      "1/1 [==============================] - 0s 404ms/step - loss: 0.5269 - accuracy: 0.2000 - val_loss: 0.5267 - val_accuracy: 0.2000\n",
      "Epoch 422/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5268 - accuracy: 0.2000 - val_loss: 0.5267 - val_accuracy: 0.2000\n",
      "Epoch 423/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5267 - accuracy: 0.2000 - val_loss: 0.5266 - val_accuracy: 0.2000\n",
      "Epoch 424/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5266 - accuracy: 0.2000 - val_loss: 0.5265 - val_accuracy: 0.2000\n",
      "Epoch 425/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5266 - accuracy: 0.2000 - val_loss: 0.5264 - val_accuracy: 0.2000\n",
      "Epoch 426/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5265 - accuracy: 0.2000 - val_loss: 0.5263 - val_accuracy: 0.2000\n",
      "Epoch 427/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5264 - accuracy: 0.2000 - val_loss: 0.5262 - val_accuracy: 0.2000\n",
      "Epoch 428/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5263 - accuracy: 0.2000 - val_loss: 0.5261 - val_accuracy: 0.2000\n",
      "Epoch 429/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5262 - accuracy: 0.2000 - val_loss: 0.5260 - val_accuracy: 0.2000\n",
      "Epoch 430/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5261 - accuracy: 0.2000 - val_loss: 0.5259 - val_accuracy: 0.2000\n",
      "Epoch 431/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5260 - accuracy: 0.2000 - val_loss: 0.5258 - val_accuracy: 0.2000\n",
      "Epoch 432/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5259 - accuracy: 0.2000 - val_loss: 0.5258 - val_accuracy: 0.2000\n",
      "Epoch 433/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5258 - accuracy: 0.2000 - val_loss: 0.5257 - val_accuracy: 0.2000\n",
      "Epoch 434/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5257 - accuracy: 0.2000 - val_loss: 0.5256 - val_accuracy: 0.2000\n",
      "Epoch 435/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5257 - accuracy: 0.2000 - val_loss: 0.5255 - val_accuracy: 0.2000\n",
      "Epoch 436/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5256 - accuracy: 0.2000 - val_loss: 0.5254 - val_accuracy: 0.2000\n",
      "Epoch 437/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5255 - accuracy: 0.2000 - val_loss: 0.5253 - val_accuracy: 0.2000\n",
      "Epoch 438/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5254 - accuracy: 0.2000 - val_loss: 0.5252 - val_accuracy: 0.2000\n",
      "Epoch 439/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5253 - accuracy: 0.2000 - val_loss: 0.5252 - val_accuracy: 0.2000\n",
      "Epoch 440/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5252 - accuracy: 0.2000 - val_loss: 0.5251 - val_accuracy: 0.2000\n",
      "Epoch 441/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5251 - accuracy: 0.2000 - val_loss: 0.5250 - val_accuracy: 0.2000\n",
      "Epoch 442/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5251 - accuracy: 0.2000 - val_loss: 0.5249 - val_accuracy: 0.2000\n",
      "Epoch 443/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5250 - accuracy: 0.2000 - val_loss: 0.5248 - val_accuracy: 0.2000\n",
      "Epoch 444/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5249 - accuracy: 0.2000 - val_loss: 0.5247 - val_accuracy: 0.2000\n",
      "Epoch 445/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5248 - accuracy: 0.2000 - val_loss: 0.5246 - val_accuracy: 0.2000\n",
      "Epoch 446/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5247 - accuracy: 0.2000 - val_loss: 0.5246 - val_accuracy: 0.2000\n",
      "Epoch 447/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5246 - accuracy: 0.2000 - val_loss: 0.5245 - val_accuracy: 0.2000\n",
      "Epoch 448/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5246 - accuracy: 0.2000 - val_loss: 0.5244 - val_accuracy: 0.2000\n",
      "Epoch 449/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5245 - accuracy: 0.2000 - val_loss: 0.5243 - val_accuracy: 0.2000\n",
      "Epoch 450/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5244 - accuracy: 0.2000 - val_loss: 0.5242 - val_accuracy: 0.2000\n",
      "Epoch 451/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5243 - accuracy: 0.2000 - val_loss: 0.5241 - val_accuracy: 0.2000\n",
      "Epoch 452/1000\n",
      "1/1 [==============================] - 0s 409ms/step - loss: 0.5242 - accuracy: 0.2000 - val_loss: 0.5241 - val_accuracy: 0.2000\n",
      "Epoch 453/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5241 - accuracy: 0.2000 - val_loss: 0.5240 - val_accuracy: 0.2000\n",
      "Epoch 454/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5241 - accuracy: 0.2000 - val_loss: 0.5239 - val_accuracy: 0.2000\n",
      "Epoch 455/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5240 - accuracy: 0.2000 - val_loss: 0.5238 - val_accuracy: 0.2000\n",
      "Epoch 456/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5239 - accuracy: 0.2000 - val_loss: 0.5237 - val_accuracy: 0.2000\n",
      "Epoch 457/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5238 - accuracy: 0.2000 - val_loss: 0.5237 - val_accuracy: 0.2000\n",
      "Epoch 458/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5237 - accuracy: 0.2000 - val_loss: 0.5236 - val_accuracy: 0.2000\n",
      "Epoch 459/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5237 - accuracy: 0.2000 - val_loss: 0.5235 - val_accuracy: 0.2000\n",
      "Epoch 460/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5236 - accuracy: 0.2000 - val_loss: 0.5234 - val_accuracy: 0.2000\n",
      "Epoch 461/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5235 - accuracy: 0.2000 - val_loss: 0.5233 - val_accuracy: 0.2000\n",
      "Epoch 462/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5234 - accuracy: 0.2000 - val_loss: 0.5233 - val_accuracy: 0.2000\n",
      "Epoch 463/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5233 - accuracy: 0.2000 - val_loss: 0.5232 - val_accuracy: 0.2000\n",
      "Epoch 464/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5233 - accuracy: 0.2000 - val_loss: 0.5231 - val_accuracy: 0.2000\n",
      "Epoch 465/1000\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 0.5232 - accuracy: 0.2000 - val_loss: 0.5230 - val_accuracy: 0.2000\n",
      "Epoch 466/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5231 - accuracy: 0.2000 - val_loss: 0.5229 - val_accuracy: 0.2000\n",
      "Epoch 467/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5230 - accuracy: 0.2000 - val_loss: 0.5229 - val_accuracy: 0.2000\n",
      "Epoch 468/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5229 - accuracy: 0.2000 - val_loss: 0.5228 - val_accuracy: 0.2000\n",
      "Epoch 469/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5229 - accuracy: 0.2000 - val_loss: 0.5227 - val_accuracy: 0.2000\n",
      "Epoch 470/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5228 - accuracy: 0.2000 - val_loss: 0.5226 - val_accuracy: 0.2000\n",
      "Epoch 471/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5227 - accuracy: 0.2000 - val_loss: 0.5226 - val_accuracy: 0.2000\n",
      "Epoch 472/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5226 - accuracy: 0.2000 - val_loss: 0.5225 - val_accuracy: 0.2000\n",
      "Epoch 473/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5226 - accuracy: 0.2000 - val_loss: 0.5224 - val_accuracy: 0.2000\n",
      "Epoch 474/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5225 - accuracy: 0.2000 - val_loss: 0.5223 - val_accuracy: 0.2000\n",
      "Epoch 475/1000\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 0.5224 - accuracy: 0.2000 - val_loss: 0.5223 - val_accuracy: 0.2000\n",
      "Epoch 476/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5223 - accuracy: 0.2000 - val_loss: 0.5222 - val_accuracy: 0.2000\n",
      "Epoch 477/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5223 - accuracy: 0.2000 - val_loss: 0.5221 - val_accuracy: 0.2000\n",
      "Epoch 478/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5222 - accuracy: 0.2000 - val_loss: 0.5220 - val_accuracy: 0.2000\n",
      "Epoch 479/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5221 - accuracy: 0.2000 - val_loss: 0.5220 - val_accuracy: 0.2000\n",
      "Epoch 480/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5220 - accuracy: 0.2000 - val_loss: 0.5219 - val_accuracy: 0.2000\n",
      "Epoch 481/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5220 - accuracy: 0.2000 - val_loss: 0.5218 - val_accuracy: 0.2000\n",
      "Epoch 482/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5219 - accuracy: 0.2000 - val_loss: 0.5217 - val_accuracy: 0.2000\n",
      "Epoch 483/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5218 - accuracy: 0.2000 - val_loss: 0.5217 - val_accuracy: 0.2000\n",
      "Epoch 484/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5217 - accuracy: 0.2000 - val_loss: 0.5216 - val_accuracy: 0.2000\n",
      "Epoch 485/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5217 - accuracy: 0.2000 - val_loss: 0.5215 - val_accuracy: 0.2000\n",
      "Epoch 486/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5216 - accuracy: 0.2000 - val_loss: 0.5214 - val_accuracy: 0.2000\n",
      "Epoch 487/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5215 - accuracy: 0.2000 - val_loss: 0.5214 - val_accuracy: 0.2000\n",
      "Epoch 488/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5215 - accuracy: 0.2000 - val_loss: 0.5213 - val_accuracy: 0.2000\n",
      "Epoch 489/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5214 - accuracy: 0.2000 - val_loss: 0.5212 - val_accuracy: 0.2000\n",
      "Epoch 490/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5213 - accuracy: 0.2000 - val_loss: 0.5212 - val_accuracy: 0.2000\n",
      "Epoch 491/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5212 - accuracy: 0.2000 - val_loss: 0.5211 - val_accuracy: 0.2000\n",
      "Epoch 492/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5212 - accuracy: 0.2000 - val_loss: 0.5210 - val_accuracy: 0.2000\n",
      "Epoch 493/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5211 - accuracy: 0.2000 - val_loss: 0.5209 - val_accuracy: 0.2000\n",
      "Epoch 494/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5210 - accuracy: 0.2000 - val_loss: 0.5209 - val_accuracy: 0.2000\n",
      "Epoch 495/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5210 - accuracy: 0.2000 - val_loss: 0.5208 - val_accuracy: 0.2000\n",
      "Epoch 496/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5209 - accuracy: 0.2000 - val_loss: 0.5207 - val_accuracy: 0.2000\n",
      "Epoch 497/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5208 - accuracy: 0.2000 - val_loss: 0.5207 - val_accuracy: 0.2000\n",
      "Epoch 498/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5207 - accuracy: 0.2000 - val_loss: 0.5206 - val_accuracy: 0.2000\n",
      "Epoch 499/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5207 - accuracy: 0.2000 - val_loss: 0.5205 - val_accuracy: 0.2000\n",
      "Epoch 500/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5206 - accuracy: 0.2000 - val_loss: 0.5205 - val_accuracy: 0.2000\n",
      "Epoch 501/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5205 - accuracy: 0.2000 - val_loss: 0.5204 - val_accuracy: 0.2000\n",
      "Epoch 502/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5205 - accuracy: 0.2000 - val_loss: 0.5203 - val_accuracy: 0.2000\n",
      "Epoch 503/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5204 - accuracy: 0.2000 - val_loss: 0.5203 - val_accuracy: 0.2000\n",
      "Epoch 504/1000\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 0.5203 - accuracy: 0.2000 - val_loss: 0.5202 - val_accuracy: 0.2000\n",
      "Epoch 505/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5203 - accuracy: 0.2000 - val_loss: 0.5201 - val_accuracy: 0.2000\n",
      "Epoch 506/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5202 - accuracy: 0.2000 - val_loss: 0.5201 - val_accuracy: 0.2000\n",
      "Epoch 507/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5201 - accuracy: 0.2000 - val_loss: 0.5200 - val_accuracy: 0.2000\n",
      "Epoch 508/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5201 - accuracy: 0.2000 - val_loss: 0.5199 - val_accuracy: 0.2000\n",
      "Epoch 509/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5200 - accuracy: 0.2000 - val_loss: 0.5199 - val_accuracy: 0.2000\n",
      "Epoch 510/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5199 - accuracy: 0.2000 - val_loss: 0.5198 - val_accuracy: 0.2000\n",
      "Epoch 511/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5199 - accuracy: 0.2000 - val_loss: 0.5197 - val_accuracy: 0.2000\n",
      "Epoch 512/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5198 - accuracy: 0.2000 - val_loss: 0.5197 - val_accuracy: 0.2000\n",
      "Epoch 513/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5197 - accuracy: 0.2000 - val_loss: 0.5196 - val_accuracy: 0.2000\n",
      "Epoch 514/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5197 - accuracy: 0.2000 - val_loss: 0.5195 - val_accuracy: 0.2000\n",
      "Epoch 515/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5196 - accuracy: 0.2000 - val_loss: 0.5195 - val_accuracy: 0.2000\n",
      "Epoch 516/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5195 - accuracy: 0.2000 - val_loss: 0.5194 - val_accuracy: 0.2000\n",
      "Epoch 517/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5195 - accuracy: 0.2000 - val_loss: 0.5193 - val_accuracy: 0.2000\n",
      "Epoch 518/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5194 - accuracy: 0.2000 - val_loss: 0.5193 - val_accuracy: 0.2000\n",
      "Epoch 519/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5193 - accuracy: 0.2000 - val_loss: 0.5192 - val_accuracy: 0.2000\n",
      "Epoch 520/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5193 - accuracy: 0.2000 - val_loss: 0.5191 - val_accuracy: 0.2000\n",
      "Epoch 521/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5192 - accuracy: 0.2000 - val_loss: 0.5191 - val_accuracy: 0.2000\n",
      "Epoch 522/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5192 - accuracy: 0.2000 - val_loss: 0.5190 - val_accuracy: 0.2000\n",
      "Epoch 523/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5191 - accuracy: 0.2000 - val_loss: 0.5189 - val_accuracy: 0.2000\n",
      "Epoch 524/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5190 - accuracy: 0.2000 - val_loss: 0.5189 - val_accuracy: 0.2000\n",
      "Epoch 525/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5190 - accuracy: 0.2000 - val_loss: 0.5188 - val_accuracy: 0.2000\n",
      "Epoch 526/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5189 - accuracy: 0.2000 - val_loss: 0.5188 - val_accuracy: 0.2000\n",
      "Epoch 527/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5188 - accuracy: 0.2000 - val_loss: 0.5187 - val_accuracy: 0.2000\n",
      "Epoch 528/1000\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 0.5188 - accuracy: 0.2000 - val_loss: 0.5186 - val_accuracy: 0.2000\n",
      "Epoch 529/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5187 - accuracy: 0.2000 - val_loss: 0.5186 - val_accuracy: 0.2000\n",
      "Epoch 530/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5187 - accuracy: 0.2000 - val_loss: 0.5185 - val_accuracy: 0.2000\n",
      "Epoch 531/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5186 - accuracy: 0.2000 - val_loss: 0.5184 - val_accuracy: 0.2000\n",
      "Epoch 532/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5185 - accuracy: 0.2000 - val_loss: 0.5184 - val_accuracy: 0.2000\n",
      "Epoch 533/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5185 - accuracy: 0.2000 - val_loss: 0.5183 - val_accuracy: 0.2000\n",
      "Epoch 534/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5184 - accuracy: 0.2000 - val_loss: 0.5183 - val_accuracy: 0.2000\n",
      "Epoch 535/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5183 - accuracy: 0.2000 - val_loss: 0.5182 - val_accuracy: 0.2000\n",
      "Epoch 536/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5183 - accuracy: 0.2000 - val_loss: 0.5181 - val_accuracy: 0.2000\n",
      "Epoch 537/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5182 - accuracy: 0.2000 - val_loss: 0.5181 - val_accuracy: 0.2000\n",
      "Epoch 538/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5182 - accuracy: 0.2000 - val_loss: 0.5180 - val_accuracy: 0.2000\n",
      "Epoch 539/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5181 - accuracy: 0.2000 - val_loss: 0.5180 - val_accuracy: 0.2000\n",
      "Epoch 540/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5180 - accuracy: 0.2000 - val_loss: 0.5179 - val_accuracy: 0.2000\n",
      "Epoch 541/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5180 - accuracy: 0.2000 - val_loss: 0.5178 - val_accuracy: 0.2000\n",
      "Epoch 542/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5179 - accuracy: 0.2000 - val_loss: 0.5178 - val_accuracy: 0.2000\n",
      "Epoch 543/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5179 - accuracy: 0.2000 - val_loss: 0.5177 - val_accuracy: 0.2000\n",
      "Epoch 544/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5178 - accuracy: 0.2000 - val_loss: 0.5177 - val_accuracy: 0.2000\n",
      "Epoch 545/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5177 - accuracy: 0.2000 - val_loss: 0.5176 - val_accuracy: 0.2000\n",
      "Epoch 546/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5177 - accuracy: 0.2000 - val_loss: 0.5176 - val_accuracy: 0.2000\n",
      "Epoch 547/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5176 - accuracy: 0.2000 - val_loss: 0.5175 - val_accuracy: 0.2000\n",
      "Epoch 548/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5176 - accuracy: 0.2000 - val_loss: 0.5174 - val_accuracy: 0.2000\n",
      "Epoch 549/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5175 - accuracy: 0.2000 - val_loss: 0.5174 - val_accuracy: 0.2000\n",
      "Epoch 550/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5175 - accuracy: 0.2000 - val_loss: 0.5173 - val_accuracy: 0.2000\n",
      "Epoch 551/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5174 - accuracy: 0.2000 - val_loss: 0.5173 - val_accuracy: 0.2000\n",
      "Epoch 552/1000\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 0.5173 - accuracy: 0.2000 - val_loss: 0.5172 - val_accuracy: 0.2000\n",
      "Epoch 553/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5173 - accuracy: 0.2000 - val_loss: 0.5172 - val_accuracy: 0.2000\n",
      "Epoch 554/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5172 - accuracy: 0.2000 - val_loss: 0.5171 - val_accuracy: 0.2000\n",
      "Epoch 555/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5172 - accuracy: 0.2000 - val_loss: 0.5170 - val_accuracy: 0.2000\n",
      "Epoch 556/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5171 - accuracy: 0.2000 - val_loss: 0.5170 - val_accuracy: 0.2000\n",
      "Epoch 557/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5171 - accuracy: 0.2000 - val_loss: 0.5169 - val_accuracy: 0.2000\n",
      "Epoch 558/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5170 - accuracy: 0.2000 - val_loss: 0.5169 - val_accuracy: 0.2000\n",
      "Epoch 559/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5170 - accuracy: 0.2000 - val_loss: 0.5168 - val_accuracy: 0.2000\n",
      "Epoch 560/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5169 - accuracy: 0.2000 - val_loss: 0.5168 - val_accuracy: 0.2000\n",
      "Epoch 561/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5168 - accuracy: 0.2000 - val_loss: 0.5167 - val_accuracy: 0.2000\n",
      "Epoch 562/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5168 - accuracy: 0.2000 - val_loss: 0.5167 - val_accuracy: 0.2000\n",
      "Epoch 563/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5167 - accuracy: 0.2000 - val_loss: 0.5166 - val_accuracy: 0.2000\n",
      "Epoch 564/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5167 - accuracy: 0.2000 - val_loss: 0.5165 - val_accuracy: 0.2000\n",
      "Epoch 565/1000\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 0.5166 - accuracy: 0.2000 - val_loss: 0.5165 - val_accuracy: 0.2000\n",
      "Epoch 566/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5166 - accuracy: 0.2000 - val_loss: 0.5164 - val_accuracy: 0.2000\n",
      "Epoch 567/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5165 - accuracy: 0.2000 - val_loss: 0.5164 - val_accuracy: 0.2000\n",
      "Epoch 568/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5165 - accuracy: 0.2000 - val_loss: 0.5163 - val_accuracy: 0.2000\n",
      "Epoch 569/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5164 - accuracy: 0.2000 - val_loss: 0.5163 - val_accuracy: 0.2000\n",
      "Epoch 570/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5163 - accuracy: 0.2000 - val_loss: 0.5162 - val_accuracy: 0.2000\n",
      "Epoch 571/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5163 - accuracy: 0.2000 - val_loss: 0.5162 - val_accuracy: 0.2000\n",
      "Epoch 572/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5162 - accuracy: 0.2000 - val_loss: 0.5161 - val_accuracy: 0.2000\n",
      "Epoch 573/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5162 - accuracy: 0.2000 - val_loss: 0.5161 - val_accuracy: 0.2000\n",
      "Epoch 574/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5161 - accuracy: 0.2000 - val_loss: 0.5160 - val_accuracy: 0.2000\n",
      "Epoch 575/1000\n",
      "1/1 [==============================] - 0s 404ms/step - loss: 0.5161 - accuracy: 0.2000 - val_loss: 0.5160 - val_accuracy: 0.2000\n",
      "Epoch 576/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5160 - accuracy: 0.2000 - val_loss: 0.5159 - val_accuracy: 0.2000\n",
      "Epoch 577/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5160 - accuracy: 0.2000 - val_loss: 0.5159 - val_accuracy: 0.2000\n",
      "Epoch 578/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5159 - accuracy: 0.2000 - val_loss: 0.5158 - val_accuracy: 0.2000\n",
      "Epoch 579/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5159 - accuracy: 0.2000 - val_loss: 0.5157 - val_accuracy: 0.2000\n",
      "Epoch 580/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5158 - accuracy: 0.2000 - val_loss: 0.5157 - val_accuracy: 0.2000\n",
      "Epoch 581/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5158 - accuracy: 0.2000 - val_loss: 0.5156 - val_accuracy: 0.2000\n",
      "Epoch 582/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5157 - accuracy: 0.2000 - val_loss: 0.5156 - val_accuracy: 0.2000\n",
      "Epoch 583/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5157 - accuracy: 0.2000 - val_loss: 0.5155 - val_accuracy: 0.2000\n",
      "Epoch 584/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5156 - accuracy: 0.2000 - val_loss: 0.5155 - val_accuracy: 0.2000\n",
      "Epoch 585/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5156 - accuracy: 0.2000 - val_loss: 0.5154 - val_accuracy: 0.2000\n",
      "Epoch 586/1000\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 0.5155 - accuracy: 0.2000 - val_loss: 0.5154 - val_accuracy: 0.2000\n",
      "Epoch 587/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5155 - accuracy: 0.2000 - val_loss: 0.5153 - val_accuracy: 0.2000\n",
      "Epoch 588/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5154 - accuracy: 0.2000 - val_loss: 0.5153 - val_accuracy: 0.2000\n",
      "Epoch 589/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5154 - accuracy: 0.2000 - val_loss: 0.5152 - val_accuracy: 0.2000\n",
      "Epoch 590/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5153 - accuracy: 0.2000 - val_loss: 0.5152 - val_accuracy: 0.2000\n",
      "Epoch 591/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5153 - accuracy: 0.2000 - val_loss: 0.5151 - val_accuracy: 0.2000\n",
      "Epoch 592/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5152 - accuracy: 0.2000 - val_loss: 0.5151 - val_accuracy: 0.2000\n",
      "Epoch 593/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5152 - accuracy: 0.2000 - val_loss: 0.5150 - val_accuracy: 0.2000\n",
      "Epoch 594/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5151 - accuracy: 0.2000 - val_loss: 0.5150 - val_accuracy: 0.2000\n",
      "Epoch 595/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5151 - accuracy: 0.2000 - val_loss: 0.5149 - val_accuracy: 0.2000\n",
      "Epoch 596/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5150 - accuracy: 0.2000 - val_loss: 0.5149 - val_accuracy: 0.2000\n",
      "Epoch 597/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5150 - accuracy: 0.2000 - val_loss: 0.5148 - val_accuracy: 0.2000\n",
      "Epoch 598/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5149 - accuracy: 0.2000 - val_loss: 0.5148 - val_accuracy: 0.2000\n",
      "Epoch 599/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5149 - accuracy: 0.2000 - val_loss: 0.5148 - val_accuracy: 0.2000\n",
      "Epoch 600/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5148 - accuracy: 0.2000 - val_loss: 0.5147 - val_accuracy: 0.2000\n",
      "Epoch 601/1000\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 0.5148 - accuracy: 0.2000 - val_loss: 0.5147 - val_accuracy: 0.2000\n",
      "Epoch 602/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5147 - accuracy: 0.2000 - val_loss: 0.5146 - val_accuracy: 0.2000\n",
      "Epoch 603/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5147 - accuracy: 0.2000 - val_loss: 0.5146 - val_accuracy: 0.2000\n",
      "Epoch 604/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5146 - accuracy: 0.2000 - val_loss: 0.5145 - val_accuracy: 0.2000\n",
      "Epoch 605/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5146 - accuracy: 0.2000 - val_loss: 0.5145 - val_accuracy: 0.2000\n",
      "Epoch 606/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5145 - accuracy: 0.2000 - val_loss: 0.5144 - val_accuracy: 0.2000\n",
      "Epoch 607/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5145 - accuracy: 0.2000 - val_loss: 0.5144 - val_accuracy: 0.2000\n",
      "Epoch 608/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5144 - accuracy: 0.2000 - val_loss: 0.5143 - val_accuracy: 0.2000\n",
      "Epoch 609/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5144 - accuracy: 0.2000 - val_loss: 0.5143 - val_accuracy: 0.2000\n",
      "Epoch 610/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5144 - accuracy: 0.2000 - val_loss: 0.5142 - val_accuracy: 0.2000\n",
      "Epoch 611/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5143 - accuracy: 0.2000 - val_loss: 0.5142 - val_accuracy: 0.2000\n",
      "Epoch 612/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5143 - accuracy: 0.2000 - val_loss: 0.5141 - val_accuracy: 0.2000\n",
      "Epoch 613/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5142 - accuracy: 0.2000 - val_loss: 0.5141 - val_accuracy: 0.2000\n",
      "Epoch 614/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5142 - accuracy: 0.2000 - val_loss: 0.5140 - val_accuracy: 0.2000\n",
      "Epoch 615/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5141 - accuracy: 0.2000 - val_loss: 0.5140 - val_accuracy: 0.2000\n",
      "Epoch 616/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5141 - accuracy: 0.2000 - val_loss: 0.5140 - val_accuracy: 0.2000\n",
      "Epoch 617/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5140 - accuracy: 0.2000 - val_loss: 0.5139 - val_accuracy: 0.2000\n",
      "Epoch 618/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5140 - accuracy: 0.2000 - val_loss: 0.5139 - val_accuracy: 0.2000\n",
      "Epoch 619/1000\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.5139 - accuracy: 0.2000 - val_loss: 0.5138 - val_accuracy: 0.2000\n",
      "Epoch 620/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5139 - accuracy: 0.2000 - val_loss: 0.5138 - val_accuracy: 0.2000\n",
      "Epoch 621/1000\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 0.5139 - accuracy: 0.2000 - val_loss: 0.5137 - val_accuracy: 0.2000\n",
      "Epoch 622/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5138 - accuracy: 0.2000 - val_loss: 0.5137 - val_accuracy: 0.2000\n",
      "Epoch 623/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5138 - accuracy: 0.2000 - val_loss: 0.5136 - val_accuracy: 0.2000\n",
      "Epoch 624/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5137 - accuracy: 0.2000 - val_loss: 0.5136 - val_accuracy: 0.2000\n",
      "Epoch 625/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5137 - accuracy: 0.2000 - val_loss: 0.5136 - val_accuracy: 0.2000\n",
      "Epoch 626/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5136 - accuracy: 0.2000 - val_loss: 0.5135 - val_accuracy: 0.2000\n",
      "Epoch 627/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5136 - accuracy: 0.2000 - val_loss: 0.5135 - val_accuracy: 0.2000\n",
      "Epoch 628/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5135 - accuracy: 0.2000 - val_loss: 0.5134 - val_accuracy: 0.2000\n",
      "Epoch 629/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5135 - accuracy: 0.2000 - val_loss: 0.5134 - val_accuracy: 0.2000\n",
      "Epoch 630/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5135 - accuracy: 0.2000 - val_loss: 0.5133 - val_accuracy: 0.2000\n",
      "Epoch 631/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5134 - accuracy: 0.2000 - val_loss: 0.5133 - val_accuracy: 0.2000\n",
      "Epoch 632/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5134 - accuracy: 0.2000 - val_loss: 0.5133 - val_accuracy: 0.2000\n",
      "Epoch 633/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5133 - accuracy: 0.2000 - val_loss: 0.5132 - val_accuracy: 0.2000\n",
      "Epoch 634/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5133 - accuracy: 0.2000 - val_loss: 0.5132 - val_accuracy: 0.2000\n",
      "Epoch 635/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5132 - accuracy: 0.2000 - val_loss: 0.5131 - val_accuracy: 0.2000\n",
      "Epoch 636/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5132 - accuracy: 0.2000 - val_loss: 0.5131 - val_accuracy: 0.2000\n",
      "Epoch 637/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5132 - accuracy: 0.2000 - val_loss: 0.5130 - val_accuracy: 0.2000\n",
      "Epoch 638/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5131 - accuracy: 0.2000 - val_loss: 0.5130 - val_accuracy: 0.2000\n",
      "Epoch 639/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5131 - accuracy: 0.2000 - val_loss: 0.5130 - val_accuracy: 0.2000\n",
      "Epoch 640/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5130 - accuracy: 0.2000 - val_loss: 0.5129 - val_accuracy: 0.2000\n",
      "Epoch 641/1000\n",
      "1/1 [==============================] - 0s 422ms/step - loss: 0.5130 - accuracy: 0.2000 - val_loss: 0.5129 - val_accuracy: 0.2000\n",
      "Epoch 642/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5129 - accuracy: 0.2000 - val_loss: 0.5128 - val_accuracy: 0.2000\n",
      "Epoch 643/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5129 - accuracy: 0.2000 - val_loss: 0.5128 - val_accuracy: 0.2000\n",
      "Epoch 644/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5129 - accuracy: 0.2000 - val_loss: 0.5127 - val_accuracy: 0.2000\n",
      "Epoch 645/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5128 - accuracy: 0.2000 - val_loss: 0.5127 - val_accuracy: 0.2000\n",
      "Epoch 646/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5128 - accuracy: 0.2000 - val_loss: 0.5127 - val_accuracy: 0.2000\n",
      "Epoch 647/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5127 - accuracy: 0.2000 - val_loss: 0.5126 - val_accuracy: 0.2000\n",
      "Epoch 648/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5127 - accuracy: 0.2000 - val_loss: 0.5126 - val_accuracy: 0.2000\n",
      "Epoch 649/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5127 - accuracy: 0.2000 - val_loss: 0.5125 - val_accuracy: 0.2000\n",
      "Epoch 650/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5126 - accuracy: 0.2000 - val_loss: 0.5125 - val_accuracy: 0.2000\n",
      "Epoch 651/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5126 - accuracy: 0.2000 - val_loss: 0.5125 - val_accuracy: 0.2000\n",
      "Epoch 652/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5125 - accuracy: 0.2000 - val_loss: 0.5124 - val_accuracy: 0.2000\n",
      "Epoch 653/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5125 - accuracy: 0.2000 - val_loss: 0.5124 - val_accuracy: 0.2000\n",
      "Epoch 654/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5125 - accuracy: 0.2000 - val_loss: 0.5123 - val_accuracy: 0.2000\n",
      "Epoch 655/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5124 - accuracy: 0.2000 - val_loss: 0.5123 - val_accuracy: 0.2000\n",
      "Epoch 656/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5124 - accuracy: 0.2000 - val_loss: 0.5123 - val_accuracy: 0.2000\n",
      "Epoch 657/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5123 - accuracy: 0.2000 - val_loss: 0.5122 - val_accuracy: 0.2000\n",
      "Epoch 658/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5123 - accuracy: 0.2000 - val_loss: 0.5122 - val_accuracy: 0.2000\n",
      "Epoch 659/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5123 - accuracy: 0.2000 - val_loss: 0.5121 - val_accuracy: 0.2000\n",
      "Epoch 660/1000\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 0.5122 - accuracy: 0.2000 - val_loss: 0.5121 - val_accuracy: 0.2000\n",
      "Epoch 661/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5122 - accuracy: 0.2000 - val_loss: 0.5121 - val_accuracy: 0.2000\n",
      "Epoch 662/1000\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 0.5121 - accuracy: 0.2000 - val_loss: 0.5120 - val_accuracy: 0.2000\n",
      "Epoch 663/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5121 - accuracy: 0.2000 - val_loss: 0.5120 - val_accuracy: 0.2000\n",
      "Epoch 664/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5121 - accuracy: 0.2000 - val_loss: 0.5120 - val_accuracy: 0.2000\n",
      "Epoch 665/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5120 - accuracy: 0.2000 - val_loss: 0.5119 - val_accuracy: 0.2000\n",
      "Epoch 666/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5120 - accuracy: 0.2000 - val_loss: 0.5119 - val_accuracy: 0.2000\n",
      "Epoch 667/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5120 - accuracy: 0.2000 - val_loss: 0.5118 - val_accuracy: 0.2000\n",
      "Epoch 668/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5119 - accuracy: 0.2000 - val_loss: 0.5118 - val_accuracy: 0.2000\n",
      "Epoch 669/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5119 - accuracy: 0.2000 - val_loss: 0.5118 - val_accuracy: 0.2000\n",
      "Epoch 670/1000\n",
      "1/1 [==============================] - 0s 412ms/step - loss: 0.5118 - accuracy: 0.2000 - val_loss: 0.5117 - val_accuracy: 0.2000\n",
      "Epoch 671/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5118 - accuracy: 0.2000 - val_loss: 0.5117 - val_accuracy: 0.2000\n",
      "Epoch 672/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5118 - accuracy: 0.2000 - val_loss: 0.5117 - val_accuracy: 0.2000\n",
      "Epoch 673/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5117 - accuracy: 0.2000 - val_loss: 0.5116 - val_accuracy: 0.2000\n",
      "Epoch 674/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5117 - accuracy: 0.2000 - val_loss: 0.5116 - val_accuracy: 0.2000\n",
      "Epoch 675/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5117 - accuracy: 0.2000 - val_loss: 0.5115 - val_accuracy: 0.2000\n",
      "Epoch 676/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5116 - accuracy: 0.2000 - val_loss: 0.5115 - val_accuracy: 0.2000\n",
      "Epoch 677/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5116 - accuracy: 0.2000 - val_loss: 0.5115 - val_accuracy: 0.2000\n",
      "Epoch 678/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5115 - accuracy: 0.2000 - val_loss: 0.5114 - val_accuracy: 0.2000\n",
      "Epoch 679/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5115 - accuracy: 0.2000 - val_loss: 0.5114 - val_accuracy: 0.2000\n",
      "Epoch 680/1000\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 0.5115 - accuracy: 0.2000 - val_loss: 0.5114 - val_accuracy: 0.2000\n",
      "Epoch 681/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5114 - accuracy: 0.2000 - val_loss: 0.5113 - val_accuracy: 0.2000\n",
      "Epoch 682/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5114 - accuracy: 0.2000 - val_loss: 0.5113 - val_accuracy: 0.2000\n",
      "Epoch 683/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5114 - accuracy: 0.2000 - val_loss: 0.5113 - val_accuracy: 0.2000\n",
      "Epoch 684/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5113 - accuracy: 0.2000 - val_loss: 0.5112 - val_accuracy: 0.2000\n",
      "Epoch 685/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5113 - accuracy: 0.2000 - val_loss: 0.5112 - val_accuracy: 0.2000\n",
      "Epoch 686/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5113 - accuracy: 0.2000 - val_loss: 0.5111 - val_accuracy: 0.2000\n",
      "Epoch 687/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5112 - accuracy: 0.2000 - val_loss: 0.5111 - val_accuracy: 0.2000\n",
      "Epoch 688/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5112 - accuracy: 0.2000 - val_loss: 0.5111 - val_accuracy: 0.2000\n",
      "Epoch 689/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5111 - accuracy: 0.2000 - val_loss: 0.5110 - val_accuracy: 0.2000\n",
      "Epoch 690/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5111 - accuracy: 0.2000 - val_loss: 0.5110 - val_accuracy: 0.2000\n",
      "Epoch 691/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5111 - accuracy: 0.2000 - val_loss: 0.5110 - val_accuracy: 0.2000\n",
      "Epoch 692/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5110 - accuracy: 0.2000 - val_loss: 0.5109 - val_accuracy: 0.2000\n",
      "Epoch 693/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5110 - accuracy: 0.2000 - val_loss: 0.5109 - val_accuracy: 0.2000\n",
      "Epoch 694/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5110 - accuracy: 0.2000 - val_loss: 0.5109 - val_accuracy: 0.2000\n",
      "Epoch 695/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5109 - accuracy: 0.2000 - val_loss: 0.5108 - val_accuracy: 0.2000\n",
      "Epoch 696/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5109 - accuracy: 0.2000 - val_loss: 0.5108 - val_accuracy: 0.2000\n",
      "Epoch 697/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5109 - accuracy: 0.2000 - val_loss: 0.5108 - val_accuracy: 0.2000\n",
      "Epoch 698/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5108 - accuracy: 0.2000 - val_loss: 0.5107 - val_accuracy: 0.2000\n",
      "Epoch 699/1000\n",
      "1/1 [==============================] - 0s 407ms/step - loss: 0.5108 - accuracy: 0.2000 - val_loss: 0.5107 - val_accuracy: 0.2000\n",
      "Epoch 700/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5108 - accuracy: 0.2000 - val_loss: 0.5107 - val_accuracy: 0.2000\n",
      "Epoch 701/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5107 - accuracy: 0.2000 - val_loss: 0.5106 - val_accuracy: 0.2000\n",
      "Epoch 702/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5107 - accuracy: 0.2000 - val_loss: 0.5106 - val_accuracy: 0.2000\n",
      "Epoch 703/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5107 - accuracy: 0.2000 - val_loss: 0.5106 - val_accuracy: 0.2000\n",
      "Epoch 704/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5106 - accuracy: 0.2000 - val_loss: 0.5105 - val_accuracy: 0.2000\n",
      "Epoch 705/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5106 - accuracy: 0.2000 - val_loss: 0.5105 - val_accuracy: 0.2000\n",
      "Epoch 706/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5106 - accuracy: 0.2000 - val_loss: 0.5105 - val_accuracy: 0.2000\n",
      "Epoch 707/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5105 - accuracy: 0.2000 - val_loss: 0.5104 - val_accuracy: 0.2000\n",
      "Epoch 708/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5105 - accuracy: 0.2000 - val_loss: 0.5104 - val_accuracy: 0.2000\n",
      "Epoch 709/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5105 - accuracy: 0.2000 - val_loss: 0.5104 - val_accuracy: 0.2000\n",
      "Epoch 710/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5104 - accuracy: 0.2000 - val_loss: 0.5103 - val_accuracy: 0.2000\n",
      "Epoch 711/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5104 - accuracy: 0.2000 - val_loss: 0.5103 - val_accuracy: 0.2000\n",
      "Epoch 712/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5104 - accuracy: 0.2000 - val_loss: 0.5103 - val_accuracy: 0.2000\n",
      "Epoch 713/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5103 - accuracy: 0.2000 - val_loss: 0.5102 - val_accuracy: 0.2000\n",
      "Epoch 714/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5103 - accuracy: 0.2000 - val_loss: 0.5102 - val_accuracy: 0.2000\n",
      "Epoch 715/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5103 - accuracy: 0.2000 - val_loss: 0.5102 - val_accuracy: 0.2000\n",
      "Epoch 716/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5102 - accuracy: 0.2000 - val_loss: 0.5101 - val_accuracy: 0.2000\n",
      "Epoch 717/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5102 - accuracy: 0.2000 - val_loss: 0.5101 - val_accuracy: 0.2000\n",
      "Epoch 718/1000\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 0.5102 - accuracy: 0.2000 - val_loss: 0.5101 - val_accuracy: 0.2000\n",
      "Epoch 719/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5101 - accuracy: 0.2000 - val_loss: 0.5100 - val_accuracy: 0.2000\n",
      "Epoch 720/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5101 - accuracy: 0.2000 - val_loss: 0.5100 - val_accuracy: 0.2000\n",
      "Epoch 721/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5101 - accuracy: 0.2000 - val_loss: 0.5100 - val_accuracy: 0.2000\n",
      "Epoch 722/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5100 - accuracy: 0.2000 - val_loss: 0.5099 - val_accuracy: 0.2000\n",
      "Epoch 723/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5100 - accuracy: 0.2000 - val_loss: 0.5099 - val_accuracy: 0.2000\n",
      "Epoch 724/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5100 - accuracy: 0.2000 - val_loss: 0.5099 - val_accuracy: 0.2000\n",
      "Epoch 725/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5100 - accuracy: 0.2000 - val_loss: 0.5098 - val_accuracy: 0.2000\n",
      "Epoch 726/1000\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 0.5099 - accuracy: 0.2000 - val_loss: 0.5098 - val_accuracy: 0.2000\n",
      "Epoch 727/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5099 - accuracy: 0.2000 - val_loss: 0.5098 - val_accuracy: 0.2000\n",
      "Epoch 728/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5099 - accuracy: 0.2000 - val_loss: 0.5098 - val_accuracy: 0.2000\n",
      "Epoch 729/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5098 - accuracy: 0.2000 - val_loss: 0.5097 - val_accuracy: 0.2000\n",
      "Epoch 730/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5098 - accuracy: 0.2000 - val_loss: 0.5097 - val_accuracy: 0.2000\n",
      "Epoch 731/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5098 - accuracy: 0.2000 - val_loss: 0.5097 - val_accuracy: 0.2000\n",
      "Epoch 732/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5097 - accuracy: 0.2000 - val_loss: 0.5096 - val_accuracy: 0.2000\n",
      "Epoch 733/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5097 - accuracy: 0.2000 - val_loss: 0.5096 - val_accuracy: 0.2000\n",
      "Epoch 734/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5097 - accuracy: 0.2000 - val_loss: 0.5096 - val_accuracy: 0.2000\n",
      "Epoch 735/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5096 - accuracy: 0.2000 - val_loss: 0.5095 - val_accuracy: 0.2000\n",
      "Epoch 736/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5096 - accuracy: 0.2000 - val_loss: 0.5095 - val_accuracy: 0.2000\n",
      "Epoch 737/1000\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 0.5096 - accuracy: 0.2000 - val_loss: 0.5095 - val_accuracy: 0.2000\n",
      "Epoch 738/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5096 - accuracy: 0.2000 - val_loss: 0.5095 - val_accuracy: 0.2000\n",
      "Epoch 739/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5095 - accuracy: 0.2000 - val_loss: 0.5094 - val_accuracy: 0.2000\n",
      "Epoch 740/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5095 - accuracy: 0.2000 - val_loss: 0.5094 - val_accuracy: 0.2000\n",
      "Epoch 741/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5095 - accuracy: 0.2000 - val_loss: 0.5094 - val_accuracy: 0.2000\n",
      "Epoch 742/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5094 - accuracy: 0.2000 - val_loss: 0.5093 - val_accuracy: 0.2000\n",
      "Epoch 743/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5094 - accuracy: 0.2000 - val_loss: 0.5093 - val_accuracy: 0.2000\n",
      "Epoch 744/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5094 - accuracy: 0.2000 - val_loss: 0.5093 - val_accuracy: 0.2000\n",
      "Epoch 745/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5094 - accuracy: 0.2000 - val_loss: 0.5092 - val_accuracy: 0.2000\n",
      "Epoch 746/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5093 - accuracy: 0.2000 - val_loss: 0.5092 - val_accuracy: 0.2000\n",
      "Epoch 747/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5093 - accuracy: 0.2000 - val_loss: 0.5092 - val_accuracy: 0.2000\n",
      "Epoch 748/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5093 - accuracy: 0.2000 - val_loss: 0.5092 - val_accuracy: 0.2000\n",
      "Epoch 749/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5092 - accuracy: 0.2000 - val_loss: 0.5091 - val_accuracy: 0.2000\n",
      "Epoch 750/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5092 - accuracy: 0.2000 - val_loss: 0.5091 - val_accuracy: 0.2000\n",
      "Epoch 751/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5092 - accuracy: 0.2000 - val_loss: 0.5091 - val_accuracy: 0.2000\n",
      "Epoch 752/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5091 - accuracy: 0.2000 - val_loss: 0.5090 - val_accuracy: 0.2000\n",
      "Epoch 753/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5091 - accuracy: 0.2000 - val_loss: 0.5090 - val_accuracy: 0.2000\n",
      "Epoch 754/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5091 - accuracy: 0.2000 - val_loss: 0.5090 - val_accuracy: 0.2000\n",
      "Epoch 755/1000\n",
      "1/1 [==============================] - 0s 406ms/step - loss: 0.5091 - accuracy: 0.2000 - val_loss: 0.5090 - val_accuracy: 0.2000\n",
      "Epoch 756/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5090 - accuracy: 0.2000 - val_loss: 0.5089 - val_accuracy: 0.2000\n",
      "Epoch 757/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5090 - accuracy: 0.2000 - val_loss: 0.5089 - val_accuracy: 0.2000\n",
      "Epoch 758/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5090 - accuracy: 0.2000 - val_loss: 0.5089 - val_accuracy: 0.2000\n",
      "Epoch 759/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5090 - accuracy: 0.2000 - val_loss: 0.5088 - val_accuracy: 0.2000\n",
      "Epoch 760/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5089 - accuracy: 0.2000 - val_loss: 0.5088 - val_accuracy: 0.2000\n",
      "Epoch 761/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5089 - accuracy: 0.2000 - val_loss: 0.5088 - val_accuracy: 0.2000\n",
      "Epoch 762/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5089 - accuracy: 0.2000 - val_loss: 0.5088 - val_accuracy: 0.2000\n",
      "Epoch 763/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5088 - accuracy: 0.2000 - val_loss: 0.5087 - val_accuracy: 0.2000\n",
      "Epoch 764/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5088 - accuracy: 0.2000 - val_loss: 0.5087 - val_accuracy: 0.2000\n",
      "Epoch 765/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5088 - accuracy: 0.2000 - val_loss: 0.5087 - val_accuracy: 0.2000\n",
      "Epoch 766/1000\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 0.5088 - accuracy: 0.2000 - val_loss: 0.5087 - val_accuracy: 0.2000\n",
      "Epoch 767/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5087 - accuracy: 0.2000 - val_loss: 0.5086 - val_accuracy: 0.2000\n",
      "Epoch 768/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5087 - accuracy: 0.2000 - val_loss: 0.5086 - val_accuracy: 0.2000\n",
      "Epoch 769/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5087 - accuracy: 0.2000 - val_loss: 0.5086 - val_accuracy: 0.2000\n",
      "Epoch 770/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5087 - accuracy: 0.2000 - val_loss: 0.5085 - val_accuracy: 0.2000\n",
      "Epoch 771/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5086 - accuracy: 0.2000 - val_loss: 0.5085 - val_accuracy: 0.2000\n",
      "Epoch 772/1000\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 0.5086 - accuracy: 0.2000 - val_loss: 0.5085 - val_accuracy: 0.2000\n",
      "Epoch 773/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5086 - accuracy: 0.2000 - val_loss: 0.5085 - val_accuracy: 0.2000\n",
      "Epoch 774/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5085 - accuracy: 0.2000 - val_loss: 0.5084 - val_accuracy: 0.2000\n",
      "Epoch 775/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5085 - accuracy: 0.2000 - val_loss: 0.5084 - val_accuracy: 0.2000\n",
      "Epoch 776/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5085 - accuracy: 0.2000 - val_loss: 0.5084 - val_accuracy: 0.2000\n",
      "Epoch 777/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5085 - accuracy: 0.2000 - val_loss: 0.5084 - val_accuracy: 0.2000\n",
      "Epoch 778/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5084 - accuracy: 0.2000 - val_loss: 0.5083 - val_accuracy: 0.2000\n",
      "Epoch 779/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5084 - accuracy: 0.2000 - val_loss: 0.5083 - val_accuracy: 0.2000\n",
      "Epoch 780/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5084 - accuracy: 0.2000 - val_loss: 0.5083 - val_accuracy: 0.2000\n",
      "Epoch 781/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5084 - accuracy: 0.2000 - val_loss: 0.5083 - val_accuracy: 0.2000\n",
      "Epoch 782/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5083 - accuracy: 0.2000 - val_loss: 0.5082 - val_accuracy: 0.2000\n",
      "Epoch 783/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5083 - accuracy: 0.2000 - val_loss: 0.5082 - val_accuracy: 0.2000\n",
      "Epoch 784/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5083 - accuracy: 0.2000 - val_loss: 0.5082 - val_accuracy: 0.2000\n",
      "Epoch 785/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5083 - accuracy: 0.2000 - val_loss: 0.5082 - val_accuracy: 0.2000\n",
      "Epoch 786/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5082 - accuracy: 0.2000 - val_loss: 0.5081 - val_accuracy: 0.2000\n",
      "Epoch 787/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5082 - accuracy: 0.2000 - val_loss: 0.5081 - val_accuracy: 0.2000\n",
      "Epoch 788/1000\n",
      "1/1 [==============================] - 0s 406ms/step - loss: 0.5082 - accuracy: 0.2000 - val_loss: 0.5081 - val_accuracy: 0.2000\n",
      "Epoch 789/1000\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 0.5082 - accuracy: 0.2000 - val_loss: 0.5081 - val_accuracy: 0.2000\n",
      "Epoch 790/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5081 - accuracy: 0.2000 - val_loss: 0.5080 - val_accuracy: 0.2000\n",
      "Epoch 791/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5081 - accuracy: 0.2000 - val_loss: 0.5080 - val_accuracy: 0.2000\n",
      "Epoch 792/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5081 - accuracy: 0.2000 - val_loss: 0.5080 - val_accuracy: 0.2000\n",
      "Epoch 793/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5081 - accuracy: 0.2000 - val_loss: 0.5080 - val_accuracy: 0.2000\n",
      "Epoch 794/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5080 - accuracy: 0.2000 - val_loss: 0.5079 - val_accuracy: 0.2000\n",
      "Epoch 795/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5080 - accuracy: 0.2000 - val_loss: 0.5079 - val_accuracy: 0.2000\n",
      "Epoch 796/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5080 - accuracy: 0.2000 - val_loss: 0.5079 - val_accuracy: 0.2000\n",
      "Epoch 797/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5080 - accuracy: 0.2000 - val_loss: 0.5079 - val_accuracy: 0.2000\n",
      "Epoch 798/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5079 - accuracy: 0.2000 - val_loss: 0.5078 - val_accuracy: 0.2000\n",
      "Epoch 799/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5079 - accuracy: 0.2000 - val_loss: 0.5078 - val_accuracy: 0.2000\n",
      "Epoch 800/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5079 - accuracy: 0.2000 - val_loss: 0.5078 - val_accuracy: 0.2000\n",
      "Epoch 801/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5079 - accuracy: 0.2000 - val_loss: 0.5078 - val_accuracy: 0.2000\n",
      "Epoch 802/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5078 - accuracy: 0.2000 - val_loss: 0.5077 - val_accuracy: 0.2000\n",
      "Epoch 803/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5078 - accuracy: 0.2000 - val_loss: 0.5077 - val_accuracy: 0.2000\n",
      "Epoch 804/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5078 - accuracy: 0.2000 - val_loss: 0.5077 - val_accuracy: 0.2000\n",
      "Epoch 805/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5078 - accuracy: 0.2000 - val_loss: 0.5077 - val_accuracy: 0.2000\n",
      "Epoch 806/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5077 - accuracy: 0.2000 - val_loss: 0.5076 - val_accuracy: 0.2000\n",
      "Epoch 807/1000\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 0.5077 - accuracy: 0.2000 - val_loss: 0.5076 - val_accuracy: 0.2000\n",
      "Epoch 808/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5077 - accuracy: 0.2000 - val_loss: 0.5076 - val_accuracy: 0.2000\n",
      "Epoch 809/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5077 - accuracy: 0.2000 - val_loss: 0.5076 - val_accuracy: 0.2000\n",
      "Epoch 810/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5076 - accuracy: 0.2000 - val_loss: 0.5075 - val_accuracy: 0.2000\n",
      "Epoch 811/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5076 - accuracy: 0.2000 - val_loss: 0.5075 - val_accuracy: 0.2000\n",
      "Epoch 812/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5076 - accuracy: 0.2000 - val_loss: 0.5075 - val_accuracy: 0.2000\n",
      "Epoch 813/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5076 - accuracy: 0.2000 - val_loss: 0.5075 - val_accuracy: 0.2000\n",
      "Epoch 814/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5076 - accuracy: 0.2000 - val_loss: 0.5075 - val_accuracy: 0.2000\n",
      "Epoch 815/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5075 - accuracy: 0.2000 - val_loss: 0.5074 - val_accuracy: 0.2000\n",
      "Epoch 816/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5075 - accuracy: 0.2000 - val_loss: 0.5074 - val_accuracy: 0.2000\n",
      "Epoch 817/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5075 - accuracy: 0.2000 - val_loss: 0.5074 - val_accuracy: 0.2000\n",
      "Epoch 818/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5075 - accuracy: 0.2000 - val_loss: 0.5074 - val_accuracy: 0.2000\n",
      "Epoch 819/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5074 - accuracy: 0.2000 - val_loss: 0.5073 - val_accuracy: 0.2000\n",
      "Epoch 820/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5074 - accuracy: 0.2000 - val_loss: 0.5073 - val_accuracy: 0.2000\n",
      "Epoch 821/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5074 - accuracy: 0.2000 - val_loss: 0.5073 - val_accuracy: 0.2000\n",
      "Epoch 822/1000\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 0.5074 - accuracy: 0.2000 - val_loss: 0.5073 - val_accuracy: 0.2000\n",
      "Epoch 823/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5073 - accuracy: 0.2000 - val_loss: 0.5073 - val_accuracy: 0.2000\n",
      "Epoch 824/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5073 - accuracy: 0.2000 - val_loss: 0.5072 - val_accuracy: 0.2000\n",
      "Epoch 825/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5073 - accuracy: 0.2000 - val_loss: 0.5072 - val_accuracy: 0.2000\n",
      "Epoch 826/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5073 - accuracy: 0.2000 - val_loss: 0.5072 - val_accuracy: 0.2000\n",
      "Epoch 827/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5073 - accuracy: 0.2000 - val_loss: 0.5072 - val_accuracy: 0.2000\n",
      "Epoch 828/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5072 - accuracy: 0.2000 - val_loss: 0.5071 - val_accuracy: 0.2000\n",
      "Epoch 829/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5072 - accuracy: 0.2000 - val_loss: 0.5071 - val_accuracy: 0.2000\n",
      "Epoch 830/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5072 - accuracy: 0.2000 - val_loss: 0.5071 - val_accuracy: 0.2000\n",
      "Epoch 831/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5072 - accuracy: 0.2000 - val_loss: 0.5071 - val_accuracy: 0.2000\n",
      "Epoch 832/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5072 - accuracy: 0.2000 - val_loss: 0.5071 - val_accuracy: 0.2000\n",
      "Epoch 833/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5071 - accuracy: 0.2000 - val_loss: 0.5070 - val_accuracy: 0.2000\n",
      "Epoch 834/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5071 - accuracy: 0.2000 - val_loss: 0.5070 - val_accuracy: 0.2000\n",
      "Epoch 835/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5071 - accuracy: 0.2000 - val_loss: 0.5070 - val_accuracy: 0.2000\n",
      "Epoch 836/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5071 - accuracy: 0.2000 - val_loss: 0.5070 - val_accuracy: 0.2000\n",
      "Epoch 837/1000\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 0.5070 - accuracy: 0.2000 - val_loss: 0.5069 - val_accuracy: 0.2000\n",
      "Epoch 838/1000\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 0.5070 - accuracy: 0.2000 - val_loss: 0.5069 - val_accuracy: 0.2000\n",
      "Epoch 839/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5070 - accuracy: 0.2000 - val_loss: 0.5069 - val_accuracy: 0.2000\n",
      "Epoch 840/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5070 - accuracy: 0.2000 - val_loss: 0.5069 - val_accuracy: 0.2000\n",
      "Epoch 841/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5070 - accuracy: 0.2000 - val_loss: 0.5069 - val_accuracy: 0.2000\n",
      "Epoch 842/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5069 - accuracy: 0.2000 - val_loss: 0.5068 - val_accuracy: 0.2000\n",
      "Epoch 843/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5069 - accuracy: 0.2000 - val_loss: 0.5068 - val_accuracy: 0.2000\n",
      "Epoch 844/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5069 - accuracy: 0.2000 - val_loss: 0.5068 - val_accuracy: 0.2000\n",
      "Epoch 845/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5069 - accuracy: 0.2000 - val_loss: 0.5068 - val_accuracy: 0.2000\n",
      "Epoch 846/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5069 - accuracy: 0.2000 - val_loss: 0.5068 - val_accuracy: 0.2000\n",
      "Epoch 847/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5068 - accuracy: 0.2000 - val_loss: 0.5067 - val_accuracy: 0.2000\n",
      "Epoch 848/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5068 - accuracy: 0.2000 - val_loss: 0.5067 - val_accuracy: 0.2000\n",
      "Epoch 849/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5068 - accuracy: 0.2000 - val_loss: 0.5067 - val_accuracy: 0.2000\n",
      "Epoch 850/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5068 - accuracy: 0.2000 - val_loss: 0.5067 - val_accuracy: 0.2000\n",
      "Epoch 851/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5067 - accuracy: 0.2000 - val_loss: 0.5067 - val_accuracy: 0.2000\n",
      "Epoch 852/1000\n",
      "1/1 [==============================] - 0s 413ms/step - loss: 0.5067 - accuracy: 0.2000 - val_loss: 0.5066 - val_accuracy: 0.2000\n",
      "Epoch 853/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5067 - accuracy: 0.2000 - val_loss: 0.5066 - val_accuracy: 0.2000\n",
      "Epoch 854/1000\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5067 - accuracy: 0.2000 - val_loss: 0.5066 - val_accuracy: 0.2000\n",
      "Epoch 855/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5067 - accuracy: 0.2000 - val_loss: 0.5066 - val_accuracy: 0.2000\n",
      "Epoch 856/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5066 - accuracy: 0.2000 - val_loss: 0.5066 - val_accuracy: 0.2000\n",
      "Epoch 857/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5066 - accuracy: 0.2000 - val_loss: 0.5065 - val_accuracy: 0.2000\n",
      "Epoch 858/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5066 - accuracy: 0.2000 - val_loss: 0.5065 - val_accuracy: 0.2000\n",
      "Epoch 859/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5066 - accuracy: 0.2000 - val_loss: 0.5065 - val_accuracy: 0.2000\n",
      "Epoch 860/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5066 - accuracy: 0.2000 - val_loss: 0.5065 - val_accuracy: 0.2000\n",
      "Epoch 861/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5065 - accuracy: 0.2000 - val_loss: 0.5065 - val_accuracy: 0.2000\n",
      "Epoch 862/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5065 - accuracy: 0.2000 - val_loss: 0.5064 - val_accuracy: 0.2000\n",
      "Epoch 863/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5065 - accuracy: 0.2000 - val_loss: 0.5064 - val_accuracy: 0.2000\n",
      "Epoch 864/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5065 - accuracy: 0.2000 - val_loss: 0.5064 - val_accuracy: 0.2000\n",
      "Epoch 865/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5065 - accuracy: 0.2000 - val_loss: 0.5064 - val_accuracy: 0.2000\n",
      "Epoch 866/1000\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 0.5064 - accuracy: 0.2000 - val_loss: 0.5064 - val_accuracy: 0.2000\n",
      "Epoch 867/1000\n",
      "1/1 [==============================] - 0s 409ms/step - loss: 0.5064 - accuracy: 0.2000 - val_loss: 0.5063 - val_accuracy: 0.2000\n",
      "Epoch 868/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5064 - accuracy: 0.2000 - val_loss: 0.5063 - val_accuracy: 0.2000\n",
      "Epoch 869/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5064 - accuracy: 0.2000 - val_loss: 0.5063 - val_accuracy: 0.2000\n",
      "Epoch 870/1000\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 0.5064 - accuracy: 0.2000 - val_loss: 0.5063 - val_accuracy: 0.2000\n",
      "Epoch 871/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5064 - accuracy: 0.2000 - val_loss: 0.5063 - val_accuracy: 0.2000\n",
      "Epoch 872/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5063 - accuracy: 0.2000 - val_loss: 0.5062 - val_accuracy: 0.2000\n",
      "Epoch 873/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5063 - accuracy: 0.2000 - val_loss: 0.5062 - val_accuracy: 0.2000\n",
      "Epoch 874/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5063 - accuracy: 0.2000 - val_loss: 0.5062 - val_accuracy: 0.2000\n",
      "Epoch 875/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5063 - accuracy: 0.2000 - val_loss: 0.5062 - val_accuracy: 0.2000\n",
      "Epoch 876/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5063 - accuracy: 0.2000 - val_loss: 0.5062 - val_accuracy: 0.2000\n",
      "Epoch 877/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5062 - accuracy: 0.2000 - val_loss: 0.5061 - val_accuracy: 0.2000\n",
      "Epoch 878/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5062 - accuracy: 0.2000 - val_loss: 0.5061 - val_accuracy: 0.2000\n",
      "Epoch 879/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5062 - accuracy: 0.2000 - val_loss: 0.5061 - val_accuracy: 0.2000\n",
      "Epoch 880/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5062 - accuracy: 0.2000 - val_loss: 0.5061 - val_accuracy: 0.2000\n",
      "Epoch 881/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5062 - accuracy: 0.2000 - val_loss: 0.5061 - val_accuracy: 0.2000\n",
      "Epoch 882/1000\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 0.5061 - accuracy: 0.2000 - val_loss: 0.5060 - val_accuracy: 0.2000\n",
      "Epoch 883/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5061 - accuracy: 0.2000 - val_loss: 0.5060 - val_accuracy: 0.2000\n",
      "Epoch 884/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5061 - accuracy: 0.2000 - val_loss: 0.5060 - val_accuracy: 0.2000\n",
      "Epoch 885/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5061 - accuracy: 0.2000 - val_loss: 0.5060 - val_accuracy: 0.2000\n",
      "Epoch 886/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5061 - accuracy: 0.2000 - val_loss: 0.5060 - val_accuracy: 0.2000\n",
      "Epoch 887/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5061 - accuracy: 0.2000 - val_loss: 0.5060 - val_accuracy: 0.2000\n",
      "Epoch 888/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5060 - accuracy: 0.2000 - val_loss: 0.5059 - val_accuracy: 0.2000\n",
      "Epoch 889/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5060 - accuracy: 0.2000 - val_loss: 0.5059 - val_accuracy: 0.2000\n",
      "Epoch 890/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5060 - accuracy: 0.2000 - val_loss: 0.5059 - val_accuracy: 0.2000\n",
      "Epoch 891/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5060 - accuracy: 0.2000 - val_loss: 0.5059 - val_accuracy: 0.2000\n",
      "Epoch 892/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5060 - accuracy: 0.2000 - val_loss: 0.5059 - val_accuracy: 0.2000\n",
      "Epoch 893/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5059 - accuracy: 0.2000 - val_loss: 0.5059 - val_accuracy: 0.2000\n",
      "Epoch 894/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5059 - accuracy: 0.2000 - val_loss: 0.5058 - val_accuracy: 0.2000\n",
      "Epoch 895/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5059 - accuracy: 0.2000 - val_loss: 0.5058 - val_accuracy: 0.2000\n",
      "Epoch 896/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5059 - accuracy: 0.2000 - val_loss: 0.5058 - val_accuracy: 0.2000\n",
      "Epoch 897/1000\n",
      "1/1 [==============================] - 0s 425ms/step - loss: 0.5059 - accuracy: 0.2000 - val_loss: 0.5058 - val_accuracy: 0.2000\n",
      "Epoch 898/1000\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 0.5059 - accuracy: 0.2000 - val_loss: 0.5058 - val_accuracy: 0.2000\n",
      "Epoch 899/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5058 - accuracy: 0.2000 - val_loss: 0.5057 - val_accuracy: 0.2000\n",
      "Epoch 900/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5058 - accuracy: 0.2000 - val_loss: 0.5057 - val_accuracy: 0.2000\n",
      "Epoch 901/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5058 - accuracy: 0.2000 - val_loss: 0.5057 - val_accuracy: 0.2000\n",
      "Epoch 902/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5058 - accuracy: 0.2000 - val_loss: 0.5057 - val_accuracy: 0.2000\n",
      "Epoch 903/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5058 - accuracy: 0.2000 - val_loss: 0.5057 - val_accuracy: 0.2000\n",
      "Epoch 904/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5058 - accuracy: 0.2000 - val_loss: 0.5057 - val_accuracy: 0.2000\n",
      "Epoch 905/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5057 - accuracy: 0.2000 - val_loss: 0.5056 - val_accuracy: 0.2000\n",
      "Epoch 906/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5057 - accuracy: 0.2000 - val_loss: 0.5056 - val_accuracy: 0.2000\n",
      "Epoch 907/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5057 - accuracy: 0.2000 - val_loss: 0.5056 - val_accuracy: 0.2000\n",
      "Epoch 908/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5057 - accuracy: 0.2000 - val_loss: 0.5056 - val_accuracy: 0.2000\n",
      "Epoch 909/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5057 - accuracy: 0.2000 - val_loss: 0.5056 - val_accuracy: 0.2000\n",
      "Epoch 910/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5056 - accuracy: 0.2000 - val_loss: 0.5056 - val_accuracy: 0.2000\n",
      "Epoch 911/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5056 - accuracy: 0.2000 - val_loss: 0.5055 - val_accuracy: 0.2000\n",
      "Epoch 912/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5056 - accuracy: 0.2000 - val_loss: 0.5055 - val_accuracy: 0.2000\n",
      "Epoch 913/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5056 - accuracy: 0.2000 - val_loss: 0.5055 - val_accuracy: 0.2000\n",
      "Epoch 914/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5056 - accuracy: 0.2000 - val_loss: 0.5055 - val_accuracy: 0.2000\n",
      "Epoch 915/1000\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 0.5056 - accuracy: 0.2000 - val_loss: 0.5055 - val_accuracy: 0.2000\n",
      "Epoch 916/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5055 - accuracy: 0.2000 - val_loss: 0.5055 - val_accuracy: 0.2000\n",
      "Epoch 917/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5055 - accuracy: 0.2000 - val_loss: 0.5054 - val_accuracy: 0.2000\n",
      "Epoch 918/1000\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 0.5055 - accuracy: 0.2000 - val_loss: 0.5054 - val_accuracy: 0.2000\n",
      "Epoch 919/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5055 - accuracy: 0.2000 - val_loss: 0.5054 - val_accuracy: 0.2000\n",
      "Epoch 920/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5055 - accuracy: 0.2000 - val_loss: 0.5054 - val_accuracy: 0.2000\n",
      "Epoch 921/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5055 - accuracy: 0.2000 - val_loss: 0.5054 - val_accuracy: 0.2000\n",
      "Epoch 922/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5055 - accuracy: 0.2000 - val_loss: 0.5054 - val_accuracy: 0.2000\n",
      "Epoch 923/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5054 - accuracy: 0.2000 - val_loss: 0.5053 - val_accuracy: 0.2000\n",
      "Epoch 924/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5054 - accuracy: 0.2000 - val_loss: 0.5053 - val_accuracy: 0.2000\n",
      "Epoch 925/1000\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 0.5054 - accuracy: 0.2000 - val_loss: 0.5053 - val_accuracy: 0.2000\n",
      "Epoch 926/1000\n",
      "1/1 [==============================] - 0s 398ms/step - loss: 0.5054 - accuracy: 0.2000 - val_loss: 0.5053 - val_accuracy: 0.2000\n",
      "Epoch 927/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5054 - accuracy: 0.2000 - val_loss: 0.5053 - val_accuracy: 0.2000\n",
      "Epoch 928/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5054 - accuracy: 0.2000 - val_loss: 0.5053 - val_accuracy: 0.2000\n",
      "Epoch 929/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5053 - accuracy: 0.2000 - val_loss: 0.5052 - val_accuracy: 0.2000\n",
      "Epoch 930/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5053 - accuracy: 0.2000 - val_loss: 0.5052 - val_accuracy: 0.2000\n",
      "Epoch 931/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5053 - accuracy: 0.2000 - val_loss: 0.5052 - val_accuracy: 0.2000\n",
      "Epoch 932/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5053 - accuracy: 0.2000 - val_loss: 0.5052 - val_accuracy: 0.2000\n",
      "Epoch 933/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5053 - accuracy: 0.2000 - val_loss: 0.5052 - val_accuracy: 0.2000\n",
      "Epoch 934/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5053 - accuracy: 0.2000 - val_loss: 0.5052 - val_accuracy: 0.2000\n",
      "Epoch 935/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5052 - accuracy: 0.2000 - val_loss: 0.5052 - val_accuracy: 0.2000\n",
      "Epoch 936/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5052 - accuracy: 0.2000 - val_loss: 0.5051 - val_accuracy: 0.2000\n",
      "Epoch 937/1000\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 0.5052 - accuracy: 0.2000 - val_loss: 0.5051 - val_accuracy: 0.2000\n",
      "Epoch 938/1000\n",
      "1/1 [==============================] - 0s 406ms/step - loss: 0.5052 - accuracy: 0.2000 - val_loss: 0.5051 - val_accuracy: 0.2000\n",
      "Epoch 939/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5052 - accuracy: 0.2000 - val_loss: 0.5051 - val_accuracy: 0.2000\n",
      "Epoch 940/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5052 - accuracy: 0.2000 - val_loss: 0.5051 - val_accuracy: 0.2000\n",
      "Epoch 941/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5052 - accuracy: 0.2000 - val_loss: 0.5051 - val_accuracy: 0.2000\n",
      "Epoch 942/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5051 - accuracy: 0.2000 - val_loss: 0.5050 - val_accuracy: 0.2000\n",
      "Epoch 943/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5051 - accuracy: 0.2000 - val_loss: 0.5050 - val_accuracy: 0.2000\n",
      "Epoch 944/1000\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 0.5051 - accuracy: 0.2000 - val_loss: 0.5050 - val_accuracy: 0.2000\n",
      "Epoch 945/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5051 - accuracy: 0.2000 - val_loss: 0.5050 - val_accuracy: 0.2000\n",
      "Epoch 946/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5051 - accuracy: 0.2000 - val_loss: 0.5050 - val_accuracy: 0.2000\n",
      "Epoch 947/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5051 - accuracy: 0.2000 - val_loss: 0.5050 - val_accuracy: 0.2000\n",
      "Epoch 948/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5050 - accuracy: 0.2000 - val_loss: 0.5050 - val_accuracy: 0.2000\n",
      "Epoch 949/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5050 - accuracy: 0.2000 - val_loss: 0.5049 - val_accuracy: 0.2000\n",
      "Epoch 950/1000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5050 - accuracy: 0.2000 - val_loss: 0.5049 - val_accuracy: 0.2000\n",
      "Epoch 951/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5050 - accuracy: 0.2000 - val_loss: 0.5049 - val_accuracy: 0.2000\n",
      "Epoch 952/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5050 - accuracy: 0.2000 - val_loss: 0.5049 - val_accuracy: 0.2000\n",
      "Epoch 953/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5050 - accuracy: 0.2000 - val_loss: 0.5049 - val_accuracy: 0.2000\n",
      "Epoch 954/1000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.5050 - accuracy: 0.2000 - val_loss: 0.5049 - val_accuracy: 0.2000\n",
      "Epoch 955/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5049 - accuracy: 0.2000 - val_loss: 0.5049 - val_accuracy: 0.2000\n",
      "Epoch 956/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5049 - accuracy: 0.2000 - val_loss: 0.5048 - val_accuracy: 0.2000\n",
      "Epoch 957/1000\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 0.5049 - accuracy: 0.2000 - val_loss: 0.5048 - val_accuracy: 0.2000\n",
      "Epoch 958/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5049 - accuracy: 0.2000 - val_loss: 0.5048 - val_accuracy: 0.2000\n",
      "Epoch 959/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5049 - accuracy: 0.2000 - val_loss: 0.5048 - val_accuracy: 0.2000\n",
      "Epoch 960/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5049 - accuracy: 0.2000 - val_loss: 0.5048 - val_accuracy: 0.2000\n",
      "Epoch 961/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5049 - accuracy: 0.2000 - val_loss: 0.5048 - val_accuracy: 0.2000\n",
      "Epoch 962/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5048 - accuracy: 0.2000 - val_loss: 0.5047 - val_accuracy: 0.2000\n",
      "Epoch 963/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5048 - accuracy: 0.2000 - val_loss: 0.5047 - val_accuracy: 0.2000\n",
      "Epoch 964/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5048 - accuracy: 0.2000 - val_loss: 0.5047 - val_accuracy: 0.2000\n",
      "Epoch 965/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5048 - accuracy: 0.2000 - val_loss: 0.5047 - val_accuracy: 0.2000\n",
      "Epoch 966/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5048 - accuracy: 0.2000 - val_loss: 0.5047 - val_accuracy: 0.2000\n",
      "Epoch 967/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5048 - accuracy: 0.2000 - val_loss: 0.5047 - val_accuracy: 0.2000\n",
      "Epoch 968/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5048 - accuracy: 0.2000 - val_loss: 0.5047 - val_accuracy: 0.2000\n",
      "Epoch 969/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5047 - accuracy: 0.2000 - val_loss: 0.5047 - val_accuracy: 0.2000\n",
      "Epoch 970/1000\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 0.5047 - accuracy: 0.2000 - val_loss: 0.5046 - val_accuracy: 0.2000\n",
      "Epoch 971/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5047 - accuracy: 0.2000 - val_loss: 0.5046 - val_accuracy: 0.2000\n",
      "Epoch 972/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5047 - accuracy: 0.2000 - val_loss: 0.5046 - val_accuracy: 0.2000\n",
      "Epoch 973/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5047 - accuracy: 0.2000 - val_loss: 0.5046 - val_accuracy: 0.2000\n",
      "Epoch 974/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5047 - accuracy: 0.2000 - val_loss: 0.5046 - val_accuracy: 0.2000\n",
      "Epoch 975/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5047 - accuracy: 0.2000 - val_loss: 0.5046 - val_accuracy: 0.2000\n",
      "Epoch 976/1000\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.5046 - accuracy: 0.2000 - val_loss: 0.5046 - val_accuracy: 0.2000\n",
      "Epoch 977/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5046 - accuracy: 0.2000 - val_loss: 0.5045 - val_accuracy: 0.2000\n",
      "Epoch 978/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5046 - accuracy: 0.2000 - val_loss: 0.5045 - val_accuracy: 0.2000\n",
      "Epoch 979/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5046 - accuracy: 0.2000 - val_loss: 0.5045 - val_accuracy: 0.2000\n",
      "Epoch 980/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5046 - accuracy: 0.2000 - val_loss: 0.5045 - val_accuracy: 0.2000\n",
      "Epoch 981/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5046 - accuracy: 0.2000 - val_loss: 0.5045 - val_accuracy: 0.2000\n",
      "Epoch 982/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5046 - accuracy: 0.2000 - val_loss: 0.5045 - val_accuracy: 0.2000\n",
      "Epoch 983/1000\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 0.5046 - accuracy: 0.2000 - val_loss: 0.5045 - val_accuracy: 0.2000\n",
      "Epoch 984/1000\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 0.5045 - accuracy: 0.2000 - val_loss: 0.5044 - val_accuracy: 0.2000\n",
      "Epoch 985/1000\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.5045 - accuracy: 0.2000 - val_loss: 0.5044 - val_accuracy: 0.2000\n",
      "Epoch 986/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5045 - accuracy: 0.2000 - val_loss: 0.5044 - val_accuracy: 0.2000\n",
      "Epoch 987/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5045 - accuracy: 0.2000 - val_loss: 0.5044 - val_accuracy: 0.2000\n",
      "Epoch 988/1000\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.5045 - accuracy: 0.2000 - val_loss: 0.5044 - val_accuracy: 0.2000\n",
      "Epoch 989/1000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5045 - accuracy: 0.2000 - val_loss: 0.5044 - val_accuracy: 0.2000\n",
      "Epoch 990/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5045 - accuracy: 0.2000 - val_loss: 0.5044 - val_accuracy: 0.2000\n",
      "Epoch 991/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5044 - accuracy: 0.2000 - val_loss: 0.5044 - val_accuracy: 0.2000\n",
      "Epoch 992/1000\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 0.5044 - accuracy: 0.2000 - val_loss: 0.5043 - val_accuracy: 0.2000\n",
      "Epoch 993/1000\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 0.5044 - accuracy: 0.2000 - val_loss: 0.5043 - val_accuracy: 0.2000\n",
      "Epoch 994/1000\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.5044 - accuracy: 0.2000 - val_loss: 0.5043 - val_accuracy: 0.2000\n",
      "Epoch 995/1000\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 0.5044 - accuracy: 0.2000 - val_loss: 0.5043 - val_accuracy: 0.2000\n",
      "Epoch 996/1000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.5044 - accuracy: 0.2000 - val_loss: 0.5043 - val_accuracy: 0.2000\n",
      "Epoch 997/1000\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.5044 - accuracy: 0.2000 - val_loss: 0.5043 - val_accuracy: 0.2000\n",
      "Epoch 998/1000\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.5044 - accuracy: 0.2000 - val_loss: 0.5043 - val_accuracy: 0.2000\n",
      "Epoch 999/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5043 - accuracy: 0.2000 - val_loss: 0.5043 - val_accuracy: 0.2000\n",
      "Epoch 1000/1000\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.5043 - accuracy: 0.2000 - val_loss: 0.5042 - val_accuracy: 0.2000\n"
     ]
    }
   ],
   "source": [
    "# Vanilla Optimizer\n",
    "# model.set_weights(initial_weights)\n",
    "# model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd_optimizer)\n",
    "# model_fit = model.fit(x=X_train, y=y_train, batch_size=len(y_train), epochs=1000, verbose=\"auto\", callbacks=None,\n",
    "#     validation_split=0.0, validation_data=(X_val, y_val), shuffle=True, validation_batch_size=None)\n",
    "\n",
    "# train_loss_list = model_fit.history['loss']\n",
    "\n",
    "# num_epochs = len(model_fit.epoch)\n",
    "# num_epochs_dict['VGD'] = num_epochs\n",
    "# loss_dict['VGD'] = train_loss_list\n",
    "\n",
    "# train_accuracy, val_accuracy = model_fit.history['accuracy'][-1], model_fit.history['val_accuracy'][-1]\n",
    "\n",
    "# eval_dict['VGD'] = (train_accuracy, val_accuracy) \n",
    "\n",
    "# with open('model_history_vgd.pkl', 'wb') as f:\n",
    "#     pickle.dump(model_fit.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "11385/11385 [==============================] - 56s 5ms/step - loss: 0.4966 - accuracy: 0.2522 - val_loss: 0.4881 - val_accuracy: 0.3281\n",
      "Epoch 2/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.4325 - accuracy: 0.5110 - val_loss: 0.3638 - val_accuracy: 0.6756\n",
      "Epoch 3/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.2861 - accuracy: 0.7331 - val_loss: 0.2227 - val_accuracy: 0.8182\n",
      "Epoch 4/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.1835 - accuracy: 0.8458 - val_loss: 0.1563 - val_accuracy: 0.8669\n",
      "Epoch 5/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.1317 - accuracy: 0.8986 - val_loss: 0.1170 - val_accuracy: 0.9020\n",
      "Epoch 6/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.1016 - accuracy: 0.9224 - val_loss: 0.0966 - val_accuracy: 0.9231\n",
      "Epoch 7/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0854 - accuracy: 0.9330 - val_loss: 0.0861 - val_accuracy: 0.9265\n",
      "Epoch 8/1000\n",
      "11385/11385 [==============================] - 54s 5ms/step - loss: 0.0758 - accuracy: 0.9423 - val_loss: 0.0821 - val_accuracy: 0.9328\n",
      "Epoch 9/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0693 - accuracy: 0.9448 - val_loss: 0.0761 - val_accuracy: 0.9339\n",
      "Epoch 10/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0643 - accuracy: 0.9494 - val_loss: 0.0747 - val_accuracy: 0.9352\n",
      "Epoch 11/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0594 - accuracy: 0.9536 - val_loss: 0.0789 - val_accuracy: 0.9291\n",
      "Epoch 12/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0564 - accuracy: 0.9548 - val_loss: 0.0754 - val_accuracy: 0.9291\n",
      "Epoch 13/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0530 - accuracy: 0.9592 - val_loss: 0.0722 - val_accuracy: 0.9352\n",
      "Epoch 14/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0506 - accuracy: 0.9611 - val_loss: 0.0745 - val_accuracy: 0.9357\n",
      "Epoch 15/1000\n",
      "11385/11385 [==============================] - 54s 5ms/step - loss: 0.0481 - accuracy: 0.9625 - val_loss: 0.0748 - val_accuracy: 0.9349\n",
      "Epoch 16/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0459 - accuracy: 0.9622 - val_loss: 0.0777 - val_accuracy: 0.9349\n",
      "Epoch 17/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0435 - accuracy: 0.9665 - val_loss: 0.0723 - val_accuracy: 0.9349\n",
      "Epoch 18/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0426 - accuracy: 0.9662 - val_loss: 0.0765 - val_accuracy: 0.9315\n",
      "Epoch 19/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0406 - accuracy: 0.9674 - val_loss: 0.0741 - val_accuracy: 0.9328\n",
      "Epoch 20/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0389 - accuracy: 0.9697 - val_loss: 0.0770 - val_accuracy: 0.9312\n",
      "Epoch 21/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0371 - accuracy: 0.9722 - val_loss: 0.0735 - val_accuracy: 0.9360\n",
      "Epoch 22/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0371 - accuracy: 0.9701 - val_loss: 0.0758 - val_accuracy: 0.9362\n",
      "Epoch 23/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0347 - accuracy: 0.9745 - val_loss: 0.0807 - val_accuracy: 0.9260\n",
      "Epoch 24/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0338 - accuracy: 0.9728 - val_loss: 0.0778 - val_accuracy: 0.9325\n",
      "Epoch 25/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0327 - accuracy: 0.9737 - val_loss: 0.0792 - val_accuracy: 0.9310\n",
      "Epoch 26/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0318 - accuracy: 0.9746 - val_loss: 0.0829 - val_accuracy: 0.9281\n",
      "Epoch 27/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0300 - accuracy: 0.9765 - val_loss: 0.0827 - val_accuracy: 0.9289\n",
      "Epoch 28/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0292 - accuracy: 0.9797 - val_loss: 0.0811 - val_accuracy: 0.9312\n",
      "Epoch 29/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0284 - accuracy: 0.9790 - val_loss: 0.0808 - val_accuracy: 0.9307\n",
      "Epoch 30/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0269 - accuracy: 0.9808 - val_loss: 0.0882 - val_accuracy: 0.9262\n",
      "Epoch 31/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0262 - accuracy: 0.9807 - val_loss: 0.0839 - val_accuracy: 0.9315\n",
      "Epoch 32/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0253 - accuracy: 0.9814 - val_loss: 0.0871 - val_accuracy: 0.9318\n",
      "Epoch 33/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0241 - accuracy: 0.9820 - val_loss: 0.0848 - val_accuracy: 0.9312\n",
      "Epoch 34/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0238 - accuracy: 0.9829 - val_loss: 0.0887 - val_accuracy: 0.9315\n",
      "Epoch 35/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0233 - accuracy: 0.9823 - val_loss: 0.0928 - val_accuracy: 0.9257\n",
      "Epoch 36/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0220 - accuracy: 0.9845 - val_loss: 0.0884 - val_accuracy: 0.9318\n",
      "Epoch 37/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0213 - accuracy: 0.9852 - val_loss: 0.0932 - val_accuracy: 0.9275\n",
      "Epoch 38/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0206 - accuracy: 0.9845 - val_loss: 0.0941 - val_accuracy: 0.9310\n",
      "Epoch 39/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0202 - accuracy: 0.9859 - val_loss: 0.0927 - val_accuracy: 0.9291\n",
      "Epoch 40/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0188 - accuracy: 0.9870 - val_loss: 0.0928 - val_accuracy: 0.9302\n",
      "Epoch 41/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0187 - accuracy: 0.9870 - val_loss: 0.0926 - val_accuracy: 0.9302\n",
      "Epoch 42/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0179 - accuracy: 0.9872 - val_loss: 0.0963 - val_accuracy: 0.9312\n",
      "Epoch 43/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0173 - accuracy: 0.9881 - val_loss: 0.0966 - val_accuracy: 0.9270\n",
      "Epoch 44/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0165 - accuracy: 0.9899 - val_loss: 0.0957 - val_accuracy: 0.9296\n",
      "Epoch 45/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0157 - accuracy: 0.9899 - val_loss: 0.0992 - val_accuracy: 0.9294\n",
      "Epoch 46/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0151 - accuracy: 0.9912 - val_loss: 0.1006 - val_accuracy: 0.9286\n",
      "Epoch 47/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0140 - accuracy: 0.9923 - val_loss: 0.1002 - val_accuracy: 0.9302\n",
      "Epoch 48/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0141 - accuracy: 0.9920 - val_loss: 0.1034 - val_accuracy: 0.9283\n",
      "Epoch 49/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0132 - accuracy: 0.9921 - val_loss: 0.1012 - val_accuracy: 0.9299\n"
     ]
    }
   ],
   "source": [
    "# SGD with momentum (generalized delta rule)\n",
    "model.set_weights(initial_weights)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=momentum_optimizer)\n",
    "model_fit = model.fit(x=X_train, y=y_train, batch_size=1, epochs=1000, verbose=\"auto\", callbacks=[\n",
    "   average_loss_callback,\n",
    "   ModelCheckpoint(filepath=f\"model_checkpoint_{model.optimizer.get_config()['name']}.h5\",\n",
    "                   save_best_only=True,  # Only save the best performing model\n",
    "                    monitor='loss',  # Monitor the validation loss\n",
    "                    mode='min',          # Minimize the validation loss\n",
    "                    save_weights_only=False, # Save the entire model (including optimizer state)\n",
    "                    append_history=True,),\n",
    "   TensorBoard(log_dir='./logs')],\n",
    "    validation_split=0.0, validation_data=(X_val, y_val), shuffle=True, validation_batch_size=None)\n",
    "\n",
    "train_loss_list = model_fit.history['loss']\n",
    "\n",
    "num_epochs = len(model_fit.epoch)\n",
    "num_epochs_dict['momentum'] = num_epochs\n",
    "loss_dict['momentum'] = train_loss_list\n",
    "\n",
    "train_accuracy, val_accuracy = model_fit.history['accuracy'][-1], model_fit.history['val_accuracy'][-1]\n",
    "\n",
    "eval_dict['momentum'] = (train_accuracy, val_accuracy) \n",
    "\n",
    "with open('model_history_momentum.pkl', 'wb') as f:\n",
    "    pickle.dump(model_fit.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "11385/11385 [==============================] - 54s 5ms/step - loss: 0.4966 - accuracy: 0.2473 - val_loss: 0.4821 - val_accuracy: 0.2448\n",
      "Epoch 2/1000\n",
      "11385/11385 [==============================] - 50s 4ms/step - loss: 0.4323 - accuracy: 0.5047 - val_loss: 0.3594 - val_accuracy: 0.6838\n",
      "Epoch 3/1000\n",
      "11385/11385 [==============================] - 53s 5ms/step - loss: 0.2843 - accuracy: 0.7369 - val_loss: 0.2231 - val_accuracy: 0.7942\n",
      "Epoch 4/1000\n",
      "11385/11385 [==============================] - 52s 5ms/step - loss: 0.1819 - accuracy: 0.8518 - val_loss: 0.1540 - val_accuracy: 0.8709\n",
      "Epoch 5/1000\n",
      "11385/11385 [==============================] - 54s 5ms/step - loss: 0.1305 - accuracy: 0.8978 - val_loss: 0.1150 - val_accuracy: 0.9078\n",
      "Epoch 6/1000\n",
      "11385/11385 [==============================] - 65s 6ms/step - loss: 0.1012 - accuracy: 0.9231 - val_loss: 0.1096 - val_accuracy: 0.8996\n",
      "Epoch 7/1000\n",
      "11385/11385 [==============================] - 61s 5ms/step - loss: 0.0852 - accuracy: 0.9346 - val_loss: 0.0911 - val_accuracy: 0.9202\n",
      "Epoch 8/1000\n",
      "11385/11385 [==============================] - 62s 5ms/step - loss: 0.0754 - accuracy: 0.9410 - val_loss: 0.0846 - val_accuracy: 0.9267\n",
      "Epoch 9/1000\n",
      "11385/11385 [==============================] - 61s 5ms/step - loss: 0.0689 - accuracy: 0.9464 - val_loss: 0.0812 - val_accuracy: 0.9278\n",
      "Epoch 10/1000\n",
      "11385/11385 [==============================] - 64s 6ms/step - loss: 0.0643 - accuracy: 0.9491 - val_loss: 0.0782 - val_accuracy: 0.9302\n",
      "Epoch 11/1000\n",
      "11385/11385 [==============================] - 62s 5ms/step - loss: 0.0599 - accuracy: 0.9527 - val_loss: 0.0749 - val_accuracy: 0.9336\n",
      "Epoch 12/1000\n",
      "11385/11385 [==============================] - 66s 6ms/step - loss: 0.0559 - accuracy: 0.9558 - val_loss: 0.0820 - val_accuracy: 0.9331\n",
      "Epoch 13/1000\n",
      "11385/11385 [==============================] - 65s 6ms/step - loss: 0.0534 - accuracy: 0.9576 - val_loss: 0.0744 - val_accuracy: 0.9339\n",
      "Epoch 14/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.0502 - accuracy: 0.9605 - val_loss: 0.0781 - val_accuracy: 0.9323\n",
      "Epoch 15/1000\n",
      "11385/11385 [==============================] - 58s 5ms/step - loss: 0.0481 - accuracy: 0.9612 - val_loss: 0.0742 - val_accuracy: 0.9323\n",
      "Epoch 16/1000\n",
      "11385/11385 [==============================] - 61s 5ms/step - loss: 0.0461 - accuracy: 0.9638 - val_loss: 0.0747 - val_accuracy: 0.9318\n",
      "Epoch 17/1000\n",
      "11385/11385 [==============================] - 70s 6ms/step - loss: 0.0440 - accuracy: 0.9647 - val_loss: 0.0782 - val_accuracy: 0.9291\n",
      "Epoch 18/1000\n",
      "11385/11385 [==============================] - 66s 6ms/step - loss: 0.0421 - accuracy: 0.9689 - val_loss: 0.0768 - val_accuracy: 0.9344\n",
      "Epoch 19/1000\n",
      "11385/11385 [==============================] - 57s 5ms/step - loss: 0.0409 - accuracy: 0.9674 - val_loss: 0.0746 - val_accuracy: 0.9383\n",
      "Epoch 20/1000\n",
      "11385/11385 [==============================] - 61s 5ms/step - loss: 0.0394 - accuracy: 0.9681 - val_loss: 0.0756 - val_accuracy: 0.9291\n",
      "Epoch 21/1000\n",
      "11385/11385 [==============================] - 63s 6ms/step - loss: 0.0375 - accuracy: 0.9715 - val_loss: 0.0884 - val_accuracy: 0.9238\n",
      "Epoch 22/1000\n",
      "11385/11385 [==============================] - 58s 5ms/step - loss: 0.0362 - accuracy: 0.9727 - val_loss: 0.0796 - val_accuracy: 0.9310\n",
      "Epoch 23/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.0350 - accuracy: 0.9734 - val_loss: 0.0834 - val_accuracy: 0.9281\n",
      "Epoch 24/1000\n",
      "11385/11385 [==============================] - 58s 5ms/step - loss: 0.0334 - accuracy: 0.9741 - val_loss: 0.0791 - val_accuracy: 0.9310\n",
      "Epoch 25/1000\n",
      "11385/11385 [==============================] - 62s 5ms/step - loss: 0.0322 - accuracy: 0.9750 - val_loss: 0.0785 - val_accuracy: 0.9365\n",
      "Epoch 26/1000\n",
      "11385/11385 [==============================] - 69s 6ms/step - loss: 0.0318 - accuracy: 0.9741 - val_loss: 0.0836 - val_accuracy: 0.9265\n",
      "Epoch 27/1000\n",
      "11385/11385 [==============================] - 69s 6ms/step - loss: 0.0299 - accuracy: 0.9774 - val_loss: 0.0833 - val_accuracy: 0.9336\n",
      "Epoch 28/1000\n",
      "11385/11385 [==============================] - 56s 5ms/step - loss: 0.0291 - accuracy: 0.9789 - val_loss: 0.0804 - val_accuracy: 0.9365\n",
      "Epoch 29/1000\n",
      "11385/11385 [==============================] - 56s 5ms/step - loss: 0.0284 - accuracy: 0.9794 - val_loss: 0.0839 - val_accuracy: 0.9315\n",
      "Epoch 30/1000\n",
      "11385/11385 [==============================] - 56s 5ms/step - loss: 0.0270 - accuracy: 0.9804 - val_loss: 0.0816 - val_accuracy: 0.9312\n",
      "Epoch 31/1000\n",
      "11385/11385 [==============================] - 56s 5ms/step - loss: 0.0263 - accuracy: 0.9820 - val_loss: 0.0862 - val_accuracy: 0.9299\n",
      "Epoch 32/1000\n",
      "11385/11385 [==============================] - 110s 10ms/step - loss: 0.0253 - accuracy: 0.9815 - val_loss: 0.0855 - val_accuracy: 0.9307\n",
      "Epoch 33/1000\n",
      "11385/11385 [==============================] - 82s 7ms/step - loss: 0.0244 - accuracy: 0.9818 - val_loss: 0.1016 - val_accuracy: 0.9199\n",
      "Epoch 34/1000\n",
      "11385/11385 [==============================] - 81s 7ms/step - loss: 0.0236 - accuracy: 0.9826 - val_loss: 0.0911 - val_accuracy: 0.9254\n",
      "Epoch 35/1000\n",
      "11385/11385 [==============================] - 84s 7ms/step - loss: 0.0229 - accuracy: 0.9838 - val_loss: 0.0872 - val_accuracy: 0.9310\n",
      "Epoch 36/1000\n",
      "11385/11385 [==============================] - 105s 9ms/step - loss: 0.0223 - accuracy: 0.9845 - val_loss: 0.0898 - val_accuracy: 0.9307\n",
      "Epoch 37/1000\n",
      "11385/11385 [==============================] - 82s 7ms/step - loss: 0.0210 - accuracy: 0.9856 - val_loss: 0.0887 - val_accuracy: 0.9299\n",
      "Epoch 38/1000\n",
      "11385/11385 [==============================] - 81s 7ms/step - loss: 0.0207 - accuracy: 0.9855 - val_loss: 0.0919 - val_accuracy: 0.9315\n",
      "Epoch 39/1000\n",
      "11385/11385 [==============================] - 83s 7ms/step - loss: 0.0193 - accuracy: 0.9875 - val_loss: 0.0908 - val_accuracy: 0.9289\n",
      "Epoch 40/1000\n",
      "11385/11385 [==============================] - 84s 7ms/step - loss: 0.0196 - accuracy: 0.9857 - val_loss: 0.0925 - val_accuracy: 0.9325\n",
      "Epoch 41/1000\n",
      "11385/11385 [==============================] - 79s 7ms/step - loss: 0.0184 - accuracy: 0.9882 - val_loss: 0.0926 - val_accuracy: 0.9291\n",
      "Epoch 42/1000\n",
      "11385/11385 [==============================] - 82s 7ms/step - loss: 0.0176 - accuracy: 0.9890 - val_loss: 0.0988 - val_accuracy: 0.9252\n",
      "Epoch 43/1000\n",
      "11385/11385 [==============================] - 109s 10ms/step - loss: 0.0174 - accuracy: 0.9886 - val_loss: 0.0937 - val_accuracy: 0.9312\n",
      "Epoch 44/1000\n",
      "11385/11385 [==============================] - 61s 5ms/step - loss: 0.0163 - accuracy: 0.9891 - val_loss: 0.0993 - val_accuracy: 0.9291\n",
      "Epoch 45/1000\n",
      "11385/11385 [==============================] - 61s 5ms/step - loss: 0.0154 - accuracy: 0.9903 - val_loss: 0.0975 - val_accuracy: 0.9286\n",
      "Epoch 46/1000\n",
      "11385/11385 [==============================] - 61s 5ms/step - loss: 0.0150 - accuracy: 0.9905 - val_loss: 0.1024 - val_accuracy: 0.9291\n",
      "Epoch 47/1000\n",
      "11385/11385 [==============================] - 61s 5ms/step - loss: 0.0145 - accuracy: 0.9920 - val_loss: 0.0985 - val_accuracy: 0.9315\n",
      "Epoch 48/1000\n",
      "11385/11385 [==============================] - 61s 5ms/step - loss: 0.0134 - accuracy: 0.9925 - val_loss: 0.1007 - val_accuracy: 0.9299\n",
      "Epoch 49/1000\n",
      "11385/11385 [==============================] - 61s 5ms/step - loss: 0.0132 - accuracy: 0.9928 - val_loss: 0.1015 - val_accuracy: 0.9283\n",
      "Epoch 50/1000\n",
      "11385/11385 [==============================] - 61s 5ms/step - loss: 0.0127 - accuracy: 0.9938 - val_loss: 0.1030 - val_accuracy: 0.9323\n",
      "Epoch 51/1000\n",
      "11385/11385 [==============================] - 61s 5ms/step - loss: 0.0123 - accuracy: 0.9931 - val_loss: 0.1029 - val_accuracy: 0.9294\n"
     ]
    }
   ],
   "source": [
    "# SGD with momentum (NAG)\n",
    "model.set_weights(initial_weights)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=nag_optimizer)\n",
    "model_fit = model.fit(x=X_train, y=y_train, batch_size=1, epochs=1000, verbose=\"auto\", callbacks=[\n",
    "   average_loss_callback,\n",
    "   ModelCheckpoint(filepath=f\"model_checkpoint_{model.optimizer.get_config()['name']}.h5\",\n",
    "                   save_best_only=True,  # Only save the best performing model\n",
    "                    monitor='loss',  # Monitor the validation loss\n",
    "                    mode='min',          # Minimize the validation loss\n",
    "                    save_weights_only=False, # Save the entire model (including optimizer state)\n",
    "                    append_history=True,),\n",
    "   TensorBoard(log_dir='./logs')],\n",
    "    validation_split=0.0, validation_data=(X_val, y_val), shuffle=True, validation_batch_size=None)\n",
    "\n",
    "train_loss_list = model_fit.history['loss']\n",
    "\n",
    "num_epochs = len(model_fit.epoch)\n",
    "num_epochs_dict['NAG'] = num_epochs\n",
    "loss_dict['NAG'] = train_loss_list\n",
    "\n",
    "train_accuracy, val_accuracy = model_fit.history['accuracy'][-1], model_fit.history['val_accuracy'][-1]\n",
    "\n",
    "eval_dict['NAG'] = (train_accuracy, val_accuracy) \n",
    "\n",
    "with open('model_history_nag.pkl', 'wb') as f:\n",
    "    pickle.dump(model_fit.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.5005 - accuracy: 0.2462 - val_loss: 0.4969 - val_accuracy: 0.2846\n",
      "Epoch 2/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.4953 - accuracy: 0.3722 - val_loss: 0.4930 - val_accuracy: 0.4105\n",
      "Epoch 3/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.4907 - accuracy: 0.4094 - val_loss: 0.4880 - val_accuracy: 0.6427\n",
      "Epoch 4/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.4849 - accuracy: 0.6039 - val_loss: 0.4811 - val_accuracy: 0.5686\n",
      "Epoch 5/1000\n",
      "11385/11385 [==============================] - 66s 6ms/step - loss: 0.4766 - accuracy: 0.6082 - val_loss: 0.4715 - val_accuracy: 0.6735\n",
      "Epoch 6/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.4650 - accuracy: 0.6376 - val_loss: 0.4582 - val_accuracy: 0.6904\n",
      "Epoch 7/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.4496 - accuracy: 0.6814 - val_loss: 0.4408 - val_accuracy: 0.6825\n",
      "Epoch 8/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.4307 - accuracy: 0.6829 - val_loss: 0.4212 - val_accuracy: 0.6585\n",
      "Epoch 9/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.4104 - accuracy: 0.6848 - val_loss: 0.4009 - val_accuracy: 0.6379\n",
      "Epoch 10/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.3907 - accuracy: 0.6983 - val_loss: 0.3817 - val_accuracy: 0.7080\n",
      "Epoch 11/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.3716 - accuracy: 0.7246 - val_loss: 0.3633 - val_accuracy: 0.7296\n",
      "Epoch 12/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.3530 - accuracy: 0.7480 - val_loss: 0.3448 - val_accuracy: 0.7449\n",
      "Epoch 13/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.3346 - accuracy: 0.7643 - val_loss: 0.3272 - val_accuracy: 0.7639\n",
      "Epoch 14/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.3170 - accuracy: 0.7788 - val_loss: 0.3103 - val_accuracy: 0.7631\n",
      "Epoch 15/1000\n",
      "11385/11385 [==============================] - 61s 5ms/step - loss: 0.3004 - accuracy: 0.7911 - val_loss: 0.2950 - val_accuracy: 0.7842\n",
      "Epoch 16/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.2853 - accuracy: 0.8000 - val_loss: 0.2806 - val_accuracy: 0.7810\n",
      "Epoch 17/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.2715 - accuracy: 0.8061 - val_loss: 0.2675 - val_accuracy: 0.8032\n",
      "Epoch 18/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.2589 - accuracy: 0.8187 - val_loss: 0.2559 - val_accuracy: 0.8058\n",
      "Epoch 19/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.2476 - accuracy: 0.8245 - val_loss: 0.2450 - val_accuracy: 0.8179\n",
      "Epoch 20/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.2372 - accuracy: 0.8336 - val_loss: 0.2353 - val_accuracy: 0.8224\n",
      "Epoch 21/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.2277 - accuracy: 0.8390 - val_loss: 0.2261 - val_accuracy: 0.8327\n",
      "Epoch 22/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.2189 - accuracy: 0.8489 - val_loss: 0.2178 - val_accuracy: 0.8393\n",
      "Epoch 23/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.2107 - accuracy: 0.8547 - val_loss: 0.2098 - val_accuracy: 0.8490\n",
      "Epoch 24/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.2030 - accuracy: 0.8605 - val_loss: 0.2023 - val_accuracy: 0.8540\n",
      "Epoch 25/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.1958 - accuracy: 0.8679 - val_loss: 0.1956 - val_accuracy: 0.8596\n",
      "Epoch 26/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.1891 - accuracy: 0.8732 - val_loss: 0.1893 - val_accuracy: 0.8675\n",
      "Epoch 27/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.1827 - accuracy: 0.8798 - val_loss: 0.1832 - val_accuracy: 0.8677\n",
      "Epoch 28/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.1769 - accuracy: 0.8844 - val_loss: 0.1774 - val_accuracy: 0.8777\n",
      "Epoch 29/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.1713 - accuracy: 0.8889 - val_loss: 0.1720 - val_accuracy: 0.8835\n",
      "Epoch 30/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.1661 - accuracy: 0.8935 - val_loss: 0.1671 - val_accuracy: 0.8862\n",
      "Epoch 31/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.1612 - accuracy: 0.8972 - val_loss: 0.1625 - val_accuracy: 0.8917\n",
      "Epoch 32/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.1565 - accuracy: 0.9000 - val_loss: 0.1581 - val_accuracy: 0.8928\n",
      "Epoch 33/1000\n",
      "11385/11385 [==============================] - 61s 5ms/step - loss: 0.1522 - accuracy: 0.9051 - val_loss: 0.1542 - val_accuracy: 0.8938\n",
      "Epoch 34/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.1481 - accuracy: 0.9061 - val_loss: 0.1501 - val_accuracy: 0.8983\n",
      "Epoch 35/1000\n",
      "11385/11385 [==============================] - 62s 5ms/step - loss: 0.1443 - accuracy: 0.9099 - val_loss: 0.1466 - val_accuracy: 0.8993\n",
      "Epoch 36/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.1406 - accuracy: 0.9122 - val_loss: 0.1434 - val_accuracy: 0.8988\n",
      "Epoch 37/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.1372 - accuracy: 0.9137 - val_loss: 0.1401 - val_accuracy: 0.9020\n",
      "Epoch 38/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.1340 - accuracy: 0.9166 - val_loss: 0.1372 - val_accuracy: 0.9033\n",
      "Epoch 39/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.1310 - accuracy: 0.9194 - val_loss: 0.1345 - val_accuracy: 0.9049\n",
      "Epoch 40/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.1282 - accuracy: 0.9208 - val_loss: 0.1317 - val_accuracy: 0.9078\n",
      "Epoch 41/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.1255 - accuracy: 0.9220 - val_loss: 0.1294 - val_accuracy: 0.9101\n",
      "Epoch 42/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.1229 - accuracy: 0.9248 - val_loss: 0.1271 - val_accuracy: 0.9096\n",
      "Epoch 43/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.1205 - accuracy: 0.9256 - val_loss: 0.1249 - val_accuracy: 0.9123\n",
      "Epoch 44/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.1181 - accuracy: 0.9275 - val_loss: 0.1231 - val_accuracy: 0.9123\n",
      "Epoch 45/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.1161 - accuracy: 0.9286 - val_loss: 0.1211 - val_accuracy: 0.9125\n",
      "Epoch 46/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.1140 - accuracy: 0.9291 - val_loss: 0.1193 - val_accuracy: 0.9133\n",
      "Epoch 47/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.1121 - accuracy: 0.9303 - val_loss: 0.1176 - val_accuracy: 0.9133\n",
      "Epoch 48/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.1103 - accuracy: 0.9316 - val_loss: 0.1159 - val_accuracy: 0.9162\n",
      "Epoch 49/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.1084 - accuracy: 0.9332 - val_loss: 0.1143 - val_accuracy: 0.9186\n",
      "Epoch 50/1000\n",
      "11385/11385 [==============================] - 60s 5ms/step - loss: 0.1068 - accuracy: 0.9344 - val_loss: 0.1127 - val_accuracy: 0.9183\n",
      "Epoch 51/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.1051 - accuracy: 0.9341 - val_loss: 0.1114 - val_accuracy: 0.9207\n",
      "Epoch 52/1000\n",
      "11385/11385 [==============================] - 58s 5ms/step - loss: 0.1037 - accuracy: 0.9348 - val_loss: 0.1100 - val_accuracy: 0.9220\n",
      "Epoch 53/1000\n",
      "11385/11385 [==============================] - 58s 5ms/step - loss: 0.1022 - accuracy: 0.9348 - val_loss: 0.1086 - val_accuracy: 0.9207\n",
      "Epoch 54/1000\n",
      "11385/11385 [==============================] - 58s 5ms/step - loss: 0.1007 - accuracy: 0.9359 - val_loss: 0.1074 - val_accuracy: 0.9207\n",
      "Epoch 55/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.0994 - accuracy: 0.9373 - val_loss: 0.1063 - val_accuracy: 0.9225\n",
      "Epoch 56/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.0981 - accuracy: 0.9368 - val_loss: 0.1052 - val_accuracy: 0.9231\n",
      "Epoch 57/1000\n",
      "11385/11385 [==============================] - 56s 5ms/step - loss: 0.0969 - accuracy: 0.9390 - val_loss: 0.1041 - val_accuracy: 0.9220\n",
      "Epoch 58/1000\n",
      "11385/11385 [==============================] - 57s 5ms/step - loss: 0.0957 - accuracy: 0.9389 - val_loss: 0.1030 - val_accuracy: 0.9238\n",
      "Epoch 59/1000\n",
      "11385/11385 [==============================] - 63s 6ms/step - loss: 0.0945 - accuracy: 0.9392 - val_loss: 0.1021 - val_accuracy: 0.9233\n",
      "Epoch 60/1000\n",
      "11385/11385 [==============================] - 58s 5ms/step - loss: 0.0933 - accuracy: 0.9396 - val_loss: 0.1011 - val_accuracy: 0.9254\n",
      "Epoch 61/1000\n",
      "11385/11385 [==============================] - 58s 5ms/step - loss: 0.0923 - accuracy: 0.9398 - val_loss: 0.1006 - val_accuracy: 0.9249\n",
      "Epoch 62/1000\n",
      "11385/11385 [==============================] - 58s 5ms/step - loss: 0.0913 - accuracy: 0.9416 - val_loss: 0.0996 - val_accuracy: 0.9238\n",
      "Epoch 63/1000\n",
      "11385/11385 [==============================] - 58s 5ms/step - loss: 0.0903 - accuracy: 0.9417 - val_loss: 0.0986 - val_accuracy: 0.9244\n",
      "Epoch 64/1000\n",
      "11385/11385 [==============================] - 58s 5ms/step - loss: 0.0893 - accuracy: 0.9416 - val_loss: 0.0979 - val_accuracy: 0.9281\n",
      "Epoch 65/1000\n",
      "11385/11385 [==============================] - 58s 5ms/step - loss: 0.0884 - accuracy: 0.9427 - val_loss: 0.0970 - val_accuracy: 0.9260\n",
      "Epoch 66/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.0874 - accuracy: 0.9436 - val_loss: 0.0965 - val_accuracy: 0.9252\n",
      "Epoch 67/1000\n",
      "11385/11385 [==============================] - 58s 5ms/step - loss: 0.0866 - accuracy: 0.9446 - val_loss: 0.0958 - val_accuracy: 0.9249\n",
      "Epoch 68/1000\n",
      "11385/11385 [==============================] - 58s 5ms/step - loss: 0.0858 - accuracy: 0.9442 - val_loss: 0.0947 - val_accuracy: 0.9281\n",
      "Epoch 69/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.0849 - accuracy: 0.9444 - val_loss: 0.0942 - val_accuracy: 0.9283\n",
      "Epoch 70/1000\n",
      "11385/11385 [==============================] - 58s 5ms/step - loss: 0.0841 - accuracy: 0.9458 - val_loss: 0.0936 - val_accuracy: 0.9283\n",
      "Epoch 71/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.0834 - accuracy: 0.9450 - val_loss: 0.0930 - val_accuracy: 0.9275\n",
      "Epoch 72/1000\n",
      "11385/11385 [==============================] - 58s 5ms/step - loss: 0.0826 - accuracy: 0.9461 - val_loss: 0.0925 - val_accuracy: 0.9273\n",
      "Epoch 73/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.0820 - accuracy: 0.9468 - val_loss: 0.0917 - val_accuracy: 0.9283\n",
      "Epoch 74/1000\n",
      "11385/11385 [==============================] - 66s 6ms/step - loss: 0.0812 - accuracy: 0.9469 - val_loss: 0.0911 - val_accuracy: 0.9289\n",
      "Epoch 75/1000\n",
      "11385/11385 [==============================] - 62s 5ms/step - loss: 0.0805 - accuracy: 0.9469 - val_loss: 0.0906 - val_accuracy: 0.9289\n",
      "Epoch 76/1000\n",
      "11385/11385 [==============================] - 57s 5ms/step - loss: 0.0798 - accuracy: 0.9474 - val_loss: 0.0903 - val_accuracy: 0.9291\n",
      "Epoch 77/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0792 - accuracy: 0.9479 - val_loss: 0.0896 - val_accuracy: 0.9299\n",
      "Epoch 78/1000\n",
      "11385/11385 [==============================] - 53s 5ms/step - loss: 0.0786 - accuracy: 0.9481 - val_loss: 0.0890 - val_accuracy: 0.9302\n",
      "Epoch 79/1000\n",
      "11385/11385 [==============================] - 53s 5ms/step - loss: 0.0779 - accuracy: 0.9497 - val_loss: 0.0886 - val_accuracy: 0.9294\n",
      "Epoch 80/1000\n",
      "11385/11385 [==============================] - 63s 6ms/step - loss: 0.0773 - accuracy: 0.9491 - val_loss: 0.0884 - val_accuracy: 0.9294\n",
      "Epoch 81/1000\n",
      "11385/11385 [==============================] - 53s 5ms/step - loss: 0.0767 - accuracy: 0.9499 - val_loss: 0.0878 - val_accuracy: 0.9289\n",
      "Epoch 82/1000\n",
      "11385/11385 [==============================] - 54s 5ms/step - loss: 0.0762 - accuracy: 0.9492 - val_loss: 0.0872 - val_accuracy: 0.9299\n",
      "Epoch 83/1000\n",
      "11385/11385 [==============================] - 55s 5ms/step - loss: 0.0756 - accuracy: 0.9497 - val_loss: 0.0868 - val_accuracy: 0.9310\n",
      "Epoch 84/1000\n",
      "11385/11385 [==============================] - 53s 5ms/step - loss: 0.0750 - accuracy: 0.9498 - val_loss: 0.0863 - val_accuracy: 0.9307\n",
      "Epoch 85/1000\n",
      "11385/11385 [==============================] - 52s 5ms/step - loss: 0.0745 - accuracy: 0.9507 - val_loss: 0.0861 - val_accuracy: 0.9310\n",
      "Epoch 86/1000\n",
      "11385/11385 [==============================] - 54s 5ms/step - loss: 0.0740 - accuracy: 0.9503 - val_loss: 0.0858 - val_accuracy: 0.9302\n",
      "Epoch 87/1000\n",
      "11385/11385 [==============================] - 54s 5ms/step - loss: 0.0735 - accuracy: 0.9513 - val_loss: 0.0852 - val_accuracy: 0.9310\n",
      "Epoch 88/1000\n",
      "11385/11385 [==============================] - 53s 5ms/step - loss: 0.0730 - accuracy: 0.9514 - val_loss: 0.0850 - val_accuracy: 0.9323\n",
      "Epoch 89/1000\n",
      "11385/11385 [==============================] - 59s 5ms/step - loss: 0.0725 - accuracy: 0.9514 - val_loss: 0.0846 - val_accuracy: 0.9296\n",
      "Epoch 90/1000\n",
      "11385/11385 [==============================] - 63s 6ms/step - loss: 0.0721 - accuracy: 0.9514 - val_loss: 0.0842 - val_accuracy: 0.9312\n",
      "Epoch 91/1000\n",
      "11385/11385 [==============================] - 54s 5ms/step - loss: 0.0716 - accuracy: 0.9523 - val_loss: 0.0839 - val_accuracy: 0.9307\n"
     ]
    }
   ],
   "source": [
    "# AdaGrad\n",
    "model.set_weights(initial_weights)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=adagrad_optimizer)\n",
    "model_fit = model.fit(x=X_train, y=y_train, batch_size=1, epochs=1000, verbose=\"auto\", callbacks=[\n",
    "   average_loss_callback,\n",
    "   ModelCheckpoint(filepath=f\"model_checkpoint_{model.optimizer.get_config()['name']}.h5\",\n",
    "                   save_best_only=True,  # Only save the best performing model\n",
    "                    monitor='loss',  # Monitor the validation loss\n",
    "                    mode='min',          # Minimize the validation loss\n",
    "                    save_weights_only=False, # Save the entire model (including optimizer state)\n",
    "                    append_history=True,),\n",
    "   TensorBoard(log_dir='./logs')],\n",
    "    validation_split=0.0, validation_data=(X_val, y_val), shuffle=True, validation_batch_size=None)\n",
    "\n",
    "train_loss_list = model_fit.history['loss']\n",
    "\n",
    "num_epochs = len(model_fit.epoch)\n",
    "num_epochs_dict['adaGrad'] = num_epochs\n",
    "loss_dict['adaGrad'] = train_loss_list\n",
    "\n",
    "train_accuracy, val_accuracy = model_fit.history['accuracy'][-1], model_fit.history['val_accuracy'][-1]\n",
    "\n",
    "eval_dict['adaGrad'] = (train_accuracy, val_accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "11385/11385 [==============================] - 152s 13ms/step - loss: 0.1621 - accuracy: 0.8401 - val_loss: 0.0807 - val_accuracy: 0.9291\n",
      "Epoch 2/1000\n",
      "11385/11385 [==============================] - 159s 14ms/step - loss: 0.0889 - accuracy: 0.9276 - val_loss: 0.0872 - val_accuracy: 0.9273\n",
      "Epoch 3/1000\n",
      "11385/11385 [==============================] - 149s 13ms/step - loss: 0.0719 - accuracy: 0.9463 - val_loss: 0.1798 - val_accuracy: 0.9080\n",
      "Epoch 4/1000\n",
      "11385/11385 [==============================] - 152s 13ms/step - loss: 0.0605 - accuracy: 0.9561 - val_loss: 0.1135 - val_accuracy: 0.9394\n",
      "Epoch 5/1000\n",
      "11385/11385 [==============================] - 135s 12ms/step - loss: 0.0476 - accuracy: 0.9669 - val_loss: 0.1378 - val_accuracy: 0.9004\n",
      "Epoch 6/1000\n",
      "11385/11385 [==============================] - 136s 12ms/step - loss: 0.0455 - accuracy: 0.9701 - val_loss: 0.0907 - val_accuracy: 0.9465\n",
      "Epoch 7/1000\n",
      "11385/11385 [==============================] - 147s 13ms/step - loss: 0.0371 - accuracy: 0.9758 - val_loss: 0.1183 - val_accuracy: 0.9433\n",
      "Epoch 8/1000\n",
      "11385/11385 [==============================] - 139s 12ms/step - loss: 0.0332 - accuracy: 0.9805 - val_loss: 0.1424 - val_accuracy: 0.9478\n",
      "Epoch 9/1000\n",
      "11385/11385 [==============================] - 139s 12ms/step - loss: 0.0281 - accuracy: 0.9845 - val_loss: 0.1453 - val_accuracy: 0.9470\n",
      "Epoch 10/1000\n",
      "11385/11385 [==============================] - 141s 12ms/step - loss: 0.0279 - accuracy: 0.9850 - val_loss: 0.1347 - val_accuracy: 0.9444\n",
      "Epoch 11/1000\n",
      "11385/11385 [==============================] - 145s 13ms/step - loss: 0.0212 - accuracy: 0.9887 - val_loss: 0.1289 - val_accuracy: 0.9447\n",
      "Epoch 12/1000\n",
      "11385/11385 [==============================] - 138s 12ms/step - loss: 0.0221 - accuracy: 0.9888 - val_loss: 0.1850 - val_accuracy: 0.9441\n",
      "Epoch 13/1000\n",
      "11385/11385 [==============================] - 132s 12ms/step - loss: 0.0170 - accuracy: 0.9918 - val_loss: 0.2035 - val_accuracy: 0.9381\n",
      "Epoch 14/1000\n",
      "11385/11385 [==============================] - 128s 11ms/step - loss: 0.0159 - accuracy: 0.9935 - val_loss: 0.1757 - val_accuracy: 0.9494\n",
      "Epoch 15/1000\n",
      "11385/11385 [==============================] - 135s 12ms/step - loss: 0.0154 - accuracy: 0.9924 - val_loss: 0.2257 - val_accuracy: 0.9497\n",
      "Epoch 16/1000\n",
      "11385/11385 [==============================] - 132s 12ms/step - loss: 0.0154 - accuracy: 0.9940 - val_loss: 0.2195 - val_accuracy: 0.9473\n",
      "Epoch 17/1000\n",
      "11385/11385 [==============================] - 132s 12ms/step - loss: 0.0133 - accuracy: 0.9952 - val_loss: 0.3238 - val_accuracy: 0.9418\n",
      "Epoch 18/1000\n",
      "11385/11385 [==============================] - 127s 11ms/step - loss: 0.0123 - accuracy: 0.9954 - val_loss: 0.2089 - val_accuracy: 0.9473\n",
      "Epoch 19/1000\n",
      "11385/11385 [==============================] - 137s 12ms/step - loss: 0.0119 - accuracy: 0.9961 - val_loss: 0.2870 - val_accuracy: 0.9381\n",
      "Epoch 20/1000\n",
      "11385/11385 [==============================] - 134s 12ms/step - loss: 0.0116 - accuracy: 0.9958 - val_loss: 0.2317 - val_accuracy: 0.9499\n",
      "Epoch 21/1000\n",
      "11385/11385 [==============================] - 133s 12ms/step - loss: 0.0120 - accuracy: 0.9963 - val_loss: 0.2830 - val_accuracy: 0.9476\n"
     ]
    }
   ],
   "source": [
    "# RMSProp\n",
    "model.set_weights(initial_weights)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=rms_optimizer)\n",
    "model_fit = model.fit(x=X_train, y=y_train, batch_size=1, epochs=1000, verbose=\"auto\", callbacks=[\n",
    "   average_loss_callback,\n",
    "   ModelCheckpoint(filepath=f\"model_checkpoint_{model.optimizer.get_config()['name']}.h5\",\n",
    "                   save_best_only=True,  # Only save the best performing model\n",
    "                    monitor='loss',  # Monitor the validation loss\n",
    "                    mode='min',          # Minimize the validation loss\n",
    "                    save_weights_only=False, # Save the entire model (including optimizer state)\n",
    "                    append_history=True,),\n",
    "   TensorBoard(log_dir='./logs')],\n",
    "    validation_split=0.0, validation_data=(X_val, y_val), shuffle=True, validation_batch_size=None)\n",
    "\n",
    "train_loss_list = model_fit.history['loss']\n",
    "\n",
    "num_epochs = len(model_fit.epoch)\n",
    "num_epochs_dict['rms'] = num_epochs\n",
    "loss_dict['rms'] = train_loss_list\n",
    "\n",
    "train_accuracy, val_accuracy = model_fit.history['accuracy'][-1], model_fit.history['val_accuracy'][-1]\n",
    "\n",
    "eval_dict['rms'] = (train_accuracy, val_accuracy)  \n",
    "\n",
    "with open('model_history_rmsprop.pkl', 'wb') as f:\n",
    "    pickle.dump(model_fit.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "11385/11385 [==============================] - 66s 6ms/step - loss: 0.1513 - accuracy: 0.8523 - val_loss: 0.1648 - val_accuracy: 0.8580\n",
      "Epoch 2/1000\n",
      "11385/11385 [==============================] - 69s 6ms/step - loss: 0.0847 - accuracy: 0.9256 - val_loss: 0.0708 - val_accuracy: 0.9299\n",
      "Epoch 3/1000\n",
      "11385/11385 [==============================] - 68s 6ms/step - loss: 0.0646 - accuracy: 0.9419 - val_loss: 0.0820 - val_accuracy: 0.9339\n",
      "Epoch 4/1000\n",
      "11385/11385 [==============================] - 70s 6ms/step - loss: 0.0548 - accuracy: 0.9520 - val_loss: 0.0736 - val_accuracy: 0.9404\n",
      "Epoch 5/1000\n",
      "11385/11385 [==============================] - 69s 6ms/step - loss: 0.0470 - accuracy: 0.9595 - val_loss: 0.0831 - val_accuracy: 0.9088\n",
      "Epoch 6/1000\n",
      "11385/11385 [==============================] - 69s 6ms/step - loss: 0.0392 - accuracy: 0.9664 - val_loss: 0.0679 - val_accuracy: 0.9389\n",
      "Epoch 7/1000\n",
      "11385/11385 [==============================] - 69s 6ms/step - loss: 0.0357 - accuracy: 0.9695 - val_loss: 0.0795 - val_accuracy: 0.9378\n",
      "Epoch 8/1000\n",
      "11385/11385 [==============================] - 69s 6ms/step - loss: 0.0324 - accuracy: 0.9722 - val_loss: 0.0909 - val_accuracy: 0.9270\n",
      "Epoch 9/1000\n",
      "11385/11385 [==============================] - 69s 6ms/step - loss: 0.0299 - accuracy: 0.9745 - val_loss: 0.1281 - val_accuracy: 0.9212\n",
      "Epoch 10/1000\n",
      "11385/11385 [==============================] - 67s 6ms/step - loss: 0.0287 - accuracy: 0.9765 - val_loss: 0.0681 - val_accuracy: 0.9484\n",
      "Epoch 11/1000\n",
      "11385/11385 [==============================] - 67s 6ms/step - loss: 0.0237 - accuracy: 0.9809 - val_loss: 0.0945 - val_accuracy: 0.9365\n",
      "Epoch 12/1000\n",
      "11385/11385 [==============================] - 68s 6ms/step - loss: 0.0237 - accuracy: 0.9786 - val_loss: 0.0752 - val_accuracy: 0.9373\n",
      "Epoch 13/1000\n",
      "11385/11385 [==============================] - 68s 6ms/step - loss: 0.0219 - accuracy: 0.9823 - val_loss: 0.1246 - val_accuracy: 0.9165\n",
      "Epoch 14/1000\n",
      "11385/11385 [==============================] - 68s 6ms/step - loss: 0.0183 - accuracy: 0.9859 - val_loss: 0.0730 - val_accuracy: 0.9468\n",
      "Epoch 15/1000\n",
      "11385/11385 [==============================] - 69s 6ms/step - loss: 0.0185 - accuracy: 0.9852 - val_loss: 0.0868 - val_accuracy: 0.9486\n",
      "Epoch 16/1000\n",
      "11385/11385 [==============================] - 69s 6ms/step - loss: 0.0184 - accuracy: 0.9853 - val_loss: 0.0790 - val_accuracy: 0.9502\n",
      "Epoch 17/1000\n",
      "11385/11385 [==============================] - 69s 6ms/step - loss: 0.0183 - accuracy: 0.9858 - val_loss: 0.1002 - val_accuracy: 0.9336\n"
     ]
    }
   ],
   "source": [
    "# Adam\n",
    "model.set_weights(initial_weights)\n",
    "# model = keras.models.load_model('model.14-0.10.h5')\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=adam_optimizer)\n",
    "model_fit = model.fit(x=X_train, y=y_train, batch_size=1, epochs=1000, verbose=\"auto\", callbacks=[\n",
    "   average_loss_callback,\n",
    "   ModelCheckpoint(filepath=f\"model_checkpoint_{model.optimizer.get_config()['name']}.h5\",\n",
    "                   save_best_only=True,  # Only save the best performing model\n",
    "                    monitor='loss',  # Monitor the validation loss\n",
    "                    mode='min',          # Minimize the validation loss\n",
    "                    save_weights_only=False, # Save the entire model (including optimizer state)\n",
    "                    append_history=True,),\n",
    "   TensorBoard(log_dir='./logs')],\n",
    "    validation_split=0.0, validation_data=(X_val, y_val), shuffle=True, validation_batch_size=None)\n",
    "\n",
    "train_loss_list = model_fit.history['loss']\n",
    "\n",
    "num_epochs = len(model_fit.epoch)\n",
    "num_epochs_dict['adam'] = num_epochs\n",
    "loss_dict['adam'] = train_loss_list\n",
    "\n",
    "train_accuracy, val_accuracy = model_fit.history['accuracy'][-1], model_fit.history['val_accuracy'][-1]\n",
    "\n",
    "eval_dict['adam'] = (train_accuracy, val_accuracy) \n",
    "\n",
    "with open('model_history_adam.pkl', 'wb') as f:\n",
    "    pickle.dump(model_fit.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot of number of epochs it took to train each optimizer\n",
    "\n",
    "nums = []\n",
    "names = []\n",
    "for ele in num_epochs_dict:\n",
    "    if(ele == 'VGD'): continue\n",
    "    names.append(ele)\n",
    "    nums.append(num_epochs_dict[ele])\n",
    "data = pd.DataFrame({'num': nums, 'names': names})\n",
    "ax = sns.barplot(x = names, y = nums)\n",
    "\n",
    "# Set y-axis limits to integers\n",
    "ax.set_yticks(range(int(min(nums)), int(max(nums))+1))\n",
    "\n",
    "# Show the plot\n",
    "ax.set_title('number of epochs it took to train each optimizer')\n",
    "ax.set_xlabel('optimizer')\n",
    "ax.set_ylabel('number of epochs to train')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC8yklEQVR4nOzdd3hT5dvA8W/SJt17UaB00MqmlL1k7w0iS2WI40UREHGADEFAtggupiKogEzZeygge8mm7FEKhU66c94/YvMzttCmNE1b7s915YKcnHOeO6cZd56pUhRFQQghhBCiiFBbOgAhhBBCiLwkyY0QQgghihRJboQQQghRpEhyI4QQQogiRZIbIYQQQhQpktwIIYQQokiR5EYIIYQQRYokN0IIIYQoUiS5EUIIIUSRIsnNMwoICKBv376WDsNkmzdvpkqVKtja2qJSqYiOjrZ0SOIJ+vbtS0BAQK6O/eyzz1CpVHkb0HPg5s2b2Nrasm/fPkuHkqVr166hUqn48ccfTTqudu3afPTRR+YJ6jmhUqkYOHCg2cuZMmUKZcuWRafTmb2s3Ni9ezcqlYrdu3fn+JjU1FT8/Pz49ttvzRfYPyS5sZA7d+7w2WefceLEiXwvOyoqim7dumFnZ8c333zD4sWLcXBwyPc4igpL/i2FeYwbN45atWpRr149S4eSpz7++GO++eYbIiIiLB2KeIrY2FgmT57Mxx9/jFpddL6mNRoNQ4cOZcKECSQlJZm3MEU8E39/f6VPnz4mH3f48GEFUH744Yc8jyk7mzZtUgBl27Zt+V52UWTuv2VKSoqSlJSUq2NTU1OVxMTEPI6oaIuMjFQ0Go3yyy+/WDqUJ9LpdEpiYqKSlpZm0nHp6elKsWLFlFGjRpkpsqIPUN59912zlvHll18qzs7OBfq9m56eriQmJirp6ekmHffo0SNFq9UqCxYsMFNkekUnJRQ5FhkZCYCrq6tlAzFRUlJSga2iNcXjx49N2l+j0WBjY5OrsqytrbG1tc3VsZaUkJCQ5XZFUUhMTHymc2f3OlqyZAnW1ta0b9/+mcoxJ5VKha2tLVZWViYdp1ar6dq1Kz/99BOKrJlcYP3www906NChQL931Wo1tra2Jtcsubq60qJFC5ObVE0lyU0WMvopnD9/nm7duuHs7IyHhweDBw/OUVXalStXePnll3F3d8fe3p7atWuzYcMGw+O7d++mRo0aAPTr1w+VSmXUfn7p0iVeeuklihUrhq2tLSVLlqRHjx7ExMRkW/Zvv/1GtWrVsLOzw9PTk1dffZXbt28bHm/UqBF9+vQBoEaNGqhUqqf2Gbp+/TrvvPMOZcqUwc7ODg8PD15++WWuXbtm2OfIkSOoVCoWLVqU6fgtW7agUqlYv369Ydvt27d5/fXX8fHxwcbGhgoVKrBw4UKj4zLac5cuXcrIkSMpUaIE9vb2xMbG8vDhQ4YNG0alSpVwdHTE2dmZ1q1bc/LkySzj79ChAw4ODnh7e/P+++8bYvpvW/HBgwdp1aoVLi4u2Nvb07Bhw2z7XGT3t2zUqBEVK1bk6NGjNGjQAHt7e0aMGAHA2rVradu2LcWLF8fGxobSpUvz+eefk56eblTGf/vcZPS3mDZtGnPnzqV06dLY2NhQo0YNDh8+bHRsVn1uMvoMrFmzhooVKxr+Bps3b87y+VWvXh1bW1tKly7NnDlzTOrHk5NrmnG+s2fP0qtXL9zc3Khfvz6g79PWrl07tmzZQvXq1bGzs2POnDlA9u+zjPif9Dp6kjVr1lCrVi0cHR0N28aMGYNGo+H+/fuZ9n/rrbdwdXV96mdDSkoKo0ePplq1ari4uODg4MCLL77Irl27jPYbM2YMarWaHTt2ZCpDq9UaXuNZ9bmJiIigX79+lCxZEhsbG3x9fenYsaPRexWgefPmXL9+/anNqKmpqbi7u9OvX79Mj8XGxmJra8uwYcMM22bPnk2FChWwt7fHzc2N6tWr88svvzzx/NlZsmSJ4XPM3d2dHj16cPPmTaN9/v3eqlu3LnZ2dgQGBvL9999nOl9kZCT9+/fHx8cHW1tbQkNDs/y80ul0fPXVV1SqVAlbW1u8vLxo1aoVR44cybRvdu+fuLg4hgwZQkBAADY2Nnh7e9O8eXOOHTv21Od+9epVTp06RbNmzYy25/S18SQ5/dzs06cPtra2nDt3zmh7y5YtcXNz486dO0DWfW5y+t3VvHlz/vzzTx4+fPjUmJ+JWeuFCqkxY8YogFKpUiWlffv2ytdff628+uqrCqC89tprRvv+t1kqIiJC8fHxUZycnJRPP/1UmTFjhhIaGqqo1Wpl1apVhn3GjRunAMpbb72lLF68WFm8eLESHh6uJCcnK4GBgUrx4sWV8ePHK/Pnz1fGjh2r1KhRQ7l27dpT4/7hhx8UQKlRo4by5ZdfKp988oliZ2enBAQEKI8ePVIURVG2bt2qvPXWWwqgjBs3Tlm8eLGyf//+J57zt99+U0JDQ5XRo0crc+fOVUaMGKG4ubkp/v7+SkJCgmG/oKAgpU2bNpmO79evn+Lm5qakpKQYnnvJkiUVPz8/Zdy4ccp3332ndOjQQQGUL7/80nDcrl27FEApX768UqVKFWXGjBnKF198oSQkJCiHDx9WSpcurXzyySfKnDlzlHHjxiklSpRQXFxclNu3bxvOER8frwQFBSl2dnbKJ598osycOVOpWbOmEhoaqgDKrl27DPvu2LFD0Wq1Sp06dZTp06crX375pVK5cmVFq9UqBw8efOL1edrfUlEUpWHDhkqxYsUULy8v5b333lPmzJmjrFmzRlEURenUqZPSrVs3ZerUqcp3332nvPzyywqgDBs2zKiMPn36KP7+/ob7V69eVQAlLCxMCQ4OViZPnqxMmTJF8fT0VEqWLGm41oryv9fyvwFKaGio4uvrq3z++efKzJkzlaCgIMXe3l558OCBYb9jx44pNjY2SkBAgDJp0iRlwoQJSvHixQ3XLzs5vaYZMZYvX17p2LGj8u233yrffPONoij691dwcLDi5uamfPLJJ8r333+v7Nq1K0fvM0V5+usoKykpKYqdnZ0ydOhQo+2XLl1SAGX27NlG25OTkxU3Nzfl9ddff+q1uH//vuLr66sMHTpU+e6775QpU6YoZcqUUTQajXL8+HGj8sPCwhR/f38lNjZWURRF2bx5swIon3/+uWG/jNfAv5tC69atq7i4uCgjR45U5s+fr0ycOFFp3LixsmfPHqNYbt26leVz+a/XX39dcXV1VZKTk422L1q0SAGUw4cPK4qiKHPnzlUApWvXrsqcOXOUr776Sunfv78yaNCgp57/ScaPH6+oVCqle/fuyrfffquMHTtW8fT0NPocUxT9e6t48eKKt7e3MnDgQGXWrFlK/fr1FcCoyePx48dKuXLlFI1Go7z//vvKrFmzlBdffFEBlJkzZxqV3bdvXwVQWrdurcycOVOZNm2a0rFjR6NrldP3T69evRStVqsMHTpUmT9/vjJ58mSlffv2ypIlS576/JcsWaIAyqlTp4y25/S18SQ5/dx89OiRUrJkSaVGjRqGZs/vv/9eAZTFixcb9st4b2V8jpry3fXnn38qgLJu3bps484tSW6ykPFh26FDB6Pt77zzjgIoJ0+eNGz7b3IzZMgQBVD++OMPw7a4uDglMDBQCQgIMLRPPqmfxvHjxxVA+e2330yKOSUlRfH29lYqVqxo1E67fv16BVBGjx5t2JaRBGV8OD3N48ePM207cOCAAig//fSTYdvw4cMVjUajPHz40LAtOTlZcXV1Nfrg79+/v+Lr62v0IaAoitKjRw/FxcXFUF7GGycoKChTDElJSZnaea9evarY2Ngo48aNM2ybPn26AhiSCUVRlMTERKVs2bJGb0qdTqeEhIQoLVu2VHQ6ndFzDwwMVJo3b/7Ua/S0PjcNGzZUAOX777/P9FhW1/btt99W7O3tjfrYPCm58fDwMLrea9euzfSB8aTkRqvVKpcvXzZsO3nyZKYvvPbt2yv29vZGH3yXLl1SrK2ts01uTLmmGTH27Nkz03n8/f0VQNm8ebPR9py+z572OsrK5cuXn/jFX6dOHaVWrVpG21atWpUpUc5KWlpapiTh0aNHio+PT6bE6PTp04pWq1XeeOMN5dGjR0qJEiWU6tWrK6mpqYZ9/pvcPHr0SAGUqVOnZvscFUVRtFqtMmDAgKfus2XLliy/gNq0aaMEBQUZ7nfs2FGpUKFCjsrNzrVr1xQrKytlwoQJRttPnz6tWFtbG23PeG9Nnz7dsC05OVmpUqWK4u3tbUjyZ86cqQBGSUVKSopSp04dxdHR0ZAo7Ny5UwGyTMr+/RrO6fvHxcUlV31zRo4cqQBKXFxcpsdy8tp4kpx+birK//7248ePV65cuaI4OjoqnTp1Mtrnv8mNKd9dd+7cUQBl8uTJ2e6bW9Is9RTvvvuu0f333nsPgI0bNz7xmI0bN1KzZk1DtTqAo6Mjb731FteuXePs2bNPLdPFxQXQN+eY0jfjyJEjREZG8s477xi107Zt25ayZctmqq7PKTs7O8P/U1NTiYqKIjg4GFdXV6Pq1e7du5OamsqqVasM27Zu3Up0dDTdu3cH9P0lVq5cSfv27VEUhQcPHhhuLVu2JCYmJlOVbZ8+fYxiALCxsTG086anpxMVFYWjoyNlypQxOn7z5s2UKFGCDh06GLbZ2try5ptvGp3vxIkTXLp0iV69ehEVFWWIKSEhgaZNm7J3795n6utjY2OTZfX+v59XXFwcDx484MUXX+Tx48ecP38+2/N2794dNzc3w/0XX3wR0DfXZKdZs2aULl3acL9y5co4Ozsbjk1PT2f79u106tSJ4sWLG/YLDg6mdevW2Z4/N9f0//7v/7I8V2BgIC1btjTaZur7LKvXUVaioqIAjK5rht69e3Pw4EHCw8MN237++Wf8/Pxo2LDhU89rZWWFVqsF9E0fDx8+JC0tjerVq2d6zVesWJGxY8cyf/58WrZsyYMHD1i0aBHW1tZPPL+dnR1arZbdu3fz6NGjbJ+nm5sbDx48eOo+TZo0wdPTk2XLlhm2PXr0iG3bthne06DvQ3Hr1q1MTaK5sWrVKnQ6Hd26dTP6fChWrBghISGZmvGsra15++23Dfe1Wi1vv/02kZGRHD16FNC/VooVK0bPnj0N+2k0GgYNGkR8fDx79uwBYOXKlahUKsaMGZMprv82w2b3/gH9dTl48KChGSenoqKisLa2NmoWzZCb10aGnH5uArRo0YK3336bcePG0aVLF2xtbQ3NwU9iyndXxvsru9fgs5Dk5ilCQkKM7pcuXRq1Wp2pDfvfrl+/TpkyZTJtL1eunOHxpwkMDGTo0KHMnz8fT09PWrZsyTfffJNtf5uM82ZVdtmyZbMt90kSExMZPXo0fn5+2NjY4OnpiZeXF9HR0UYxhYaGUrZsWaMPwmXLluHp6UmTJk0AuH//PtHR0cydOxcvLy+jW8aXf0Zn539fj//S6XR8+eWXhISEGMV06tQpo5iuX79O6dKlM30wBQcHG92/dOkSoP8C/G9c8+fPJzk5OUf9nZ6kRIkShi+2fztz5gydO3fGxcUFZ2dnvLy8ePXVVwFyVF6pUqWM7md8YOTky+2/x2Ycn3FsZGQkiYmJma4VZL5+WcnNNc3qb/2k7aa+z5507idRsuhs2717d2xsbPj5558B/d9o/fr1vPLKK4bX2P3794mIiDDc4uPjDccvWrSIypUrY2tri4eHB15eXmzYsCHLv/WHH35IaGgohw4dYsyYMZQvX/6p8drY2DB58mQ2bdqEj48PDRo0YMqUKU8c8q0oSrb9pqytrXnppZdYu3YtycnJgD75SE1NNUpuPv74YxwdHalZsyYhISG8++67uZ4f6NKlSyiKQkhISKbXzblz5zJ9PhQvXjzTNBYvvPACgOFz+vr164SEhGTq+Prf10p4eDjFixfH3d092zize/+Afp6av//+Gz8/P2rWrMlnn32Wox8e2cnutfHv119ERIShA35OPzczTJs2DXd3d06cOMGsWbPw9vZ+alymfHdlvL/MOQeXJDcmyK/J0KZPn86pU6cYMWIEiYmJDBo0iAoVKnDr1q18Kf/f3nvvPSZMmEC3bt1Yvnw5W7duZdu2bXh4eGT65d29e3d27drFgwcPSE5O5vfff+ell14y/KrI2P/VV19l27ZtWd7+O69IVr+2J06cyNChQ2nQoAFLlixhy5YtbNu2jQoVKuSqhiXjmKlTpz4xrqx+ReVUVs8hOjqahg0bcvLkScaNG8e6devYtm0bkydPNorpaZ40UiarL+a8PDYncnNNn1SzkpMal+zk9BweHh5A1gmim5sb7dq1MyQ3K1asIDk52ZCQgr6Tvq+vr+E2bdo0QN9Btm/fvpQuXZoFCxawefNmtm3bRpMmTbL8W1+5csWQIJ4+fTpHsQ8ZMoSLFy/yxRdfYGtry6hRoyhXrhzHjx/PtG90dDSenp7ZnrNHjx7ExcWxadMmAJYvX07ZsmUJDQ017FOuXDkuXLjA0qVLqV+/PitXrqR+/fpZ1oBkR6fToVKpDNfnv7fsag/yS07eP926dePKlSvMnj2b4sWLM3XqVCpUqGC4lk/i4eFBWloacXFxWT6e3Wvj368/X19fww9OUz83jx8/bkgmc/oazOl3V8b7KyevwdzKvi7rOXbp0iWjX3yXL19Gp9M9dbZYf39/Lly4kGl7RjODv78/kH2iVKlSJSpVqsTIkSPZv38/9erV4/vvv2f8+PFPLBfgwoULhpqSDBcuXDA8bqoVK1bQp08fpk+fbtiWlJSU5YzG3bt3Z+zYsaxcuRIfHx9iY2Pp0aOH4XEvLy+cnJxIT0/PNBLA1JgaN27MggULjLb/9wPb39+fs2fPZvqVevnyZaPjMqqXnZ2dcxVXbpLe3bt3ExUVxapVq2jQoIFh+9WrV00+lzl4e3tja2ub6VpB5uuXlWe9ptnJ6fvMVKVKlcLOzu6Jf4fevXvTsWNHDh8+zM8//0xYWBgVKlQwPP7zzz8bDVUPCgoC9K/ZoKAgVq1aZfR6ySoB0Ol09O3bF2dnZ4YMGcLEiRPp2rUrXbp0yTb+0qVL88EHH/DBBx9w6dIlqlSpwvTp01myZIlhn9u3b5OSkmKouXiaBg0aGL4g69evz86dO/n0008z7efg4ED37t3p3r07KSkpdOnShQkTJjB8+HCThjOXLl0aRVEIDAw01MA8zZ07d0hISDCqvbl48SKA4XPa39+fU6dOodPpjGpv/vtaKV26NFu2bOHhw4c5qr3JCV9fX9555x3eeecdIiMjqVq1KhMmTHhq027ZsmUB/WdB5cqVjR7LyWtj27ZtRsdkvD5z+rkJ+qkY+vXrR/ny5albty5Tpkyhc+fOhpGhT5OT766M91dOXoO5JTU3T/HNN98Y3Z89ezbAU1+Ybdq04dChQxw4cMCwLSEhgblz5xIQEGCoQsx4M/43SYiNjSUtLc1oW6VKlVCr1Yaq4axUr14db29vvv/+e6P9Nm3axLlz52jbtu1TnumTWVlZZfo1P3v27EzDlUH/Qq1UqRLLli1j2bJl+Pr6Gn1xW1lZ8dJLL7Fy5Ur+/vvvTMdnNcw2pzH99ttvRkPeQT908fbt2/z++++GbUlJScybN89ov2rVqlG6dGmmTZtm1IyQ07ie9LfM7jmA8S+9lJSUfJmWPCesrKxo1qwZa9asMeozcPny5Wx/ecKzX9Ps5PR9ZiqNRkP16tWzHPoL+ve+p6cnkydPZs+ePUa1NgD16tWjWbNmhltGcpPV3/vgwYNG8WeYMWMG+/fvZ+7cuXz++efUrVuXAQMGPLV/wuPHjzMNRS9dujROTk6ZPjcy+qLUrVv3iefLkDEvzrp161i8eDFpaWlGTVLwv35KGbRaLeXLl0dRFFJTUw3xnT9/Pts+Fl26dMHKyoqxY8dmeo8ripKprLS0NKPanJSUFObMmYOXlxfVqlUD9K+ViIgIoybztLQ0Zs+ejaOjo6G/1EsvvYSiKIwdOzZTXKbWaKanp2dqjvH29qZ48eJP/RwHqFOnDkCWr8GcvDb+/fpr1qwZvr6+QM4/N0Hf1Hjjxg0WLVrEjBkzCAgIoE+fPk+N3ZTvrqNHj6JSqQzP1Ryk5uYprl69SocOHWjVqhUHDhxgyZIl9OrVy6hK9r8++eQTfv31V1q3bs2gQYNwd3dn0aJFXL16lZUrVxp+OZQuXRpXV1e+//57nJyccHBwoFatWpw8eZKBAwfy8ssv88ILL5CWlsbixYsNicGTaDQaJk+eTL9+/WjYsCE9e/bk3r17fPXVVwQEBPD+++/n6hq0a9eOxYsX4+LiQvny5Tlw4ADbt283VN//V/fu3Rk9ejS2trb0798/Uzv3pEmT2LVrF7Vq1eLNN9+kfPnyPHz4kGPHjrF9+/YczXvQrl07xo0bR79+/ahbty6nT5/m559/NnyRZHj77bf5+uuv6dmzJ4MHD8bX15eff/7Z8Esy4xe0Wq1m/vz5tG7dmgoVKtCvXz9KlCjB7du32bVrF87Ozqxbt+6J8Tzpb/m0fh5169bFzc2NPn36MGjQIFQqFYsXLy5QE6t99tlnbN26lXr16jFgwADS09P5+uuvqVixYrZLTTzrNc1OTt9nudGxY0c+/fRTYmNjcXZ2NnpMo9HQo0cPvv76a6ysrIw6qT5Nu3btWLVqFZ07d6Zt27ZcvXqV77//nvLlyxslf+fOnWPUqFH07dvXMIngjz/+SJUqVXjnnXdYvnx5lue/ePEiTZs2pVu3bpQvXx5ra2tWr17NvXv3jGpPQf/LvlSpUoSFheUo9u7duzN79mzGjBlDpUqVMv3abtGiBcWKFaNevXr4+Phw7tw5vv76a9q2bYuTkxMAhw4donHjxowZM4bPPvvsiWWVLl2a8ePHM3z4cK5du0anTp1wcnLi6tWrrF69mrfeestofp3ixYszefJkrl27xgsvvMCyZcs4ceIEc+fORaPRAPp5YObMmUPfvn05evQoAQEBrFixgn379jFz5kxDjI0bN+a1115j1qxZXLp0iVatWqHT6fjjjz9o3LixSetJxcXFUbJkSbp27UpoaCiOjo5s376dw4cPG9WCZyUoKIiKFSuyfft2Xn/9dcP23L42MuT0c3Pnzp18++23jBkzhqpVqwL6SQUbNWrEqFGjmDJlSpbn37lzZ46/uzK6IDzpeyRPmG0cViGWMTT17NmzSteuXRUnJyfFzc1NGThwYKbpsLNafiE8PFzp2rWr4urqqtja2io1a9ZU1q9fn6mctWvXKuXLlzcMrf3hhx+UK1euKK+//rpSunRpxdbWVnF3d1caN26sbN++PUexL1u2TAkLC1NsbGwUd3d35ZVXXlFu3bpltI8pQ8EfPXqk9OvXT/H09FQcHR2Vli1bKufPn3/ishMZ84EAyp9//pnlOe/du6e8++67ip+fn6LRaJRixYopTZs2VebOnWvYJ2OYYVbDCpOSkpQPPvhA8fX1Vezs7JR69eopBw4cUBo2bKg0bNjQaN8rV64obdu2Vezs7BQvLy/lgw8+UFauXKkAyl9//WW07/Hjx5UuXbooHh4eio2NjeLv769069ZN2bFjR7bXKau/paLoh6s+aZjsvn37lNq1ayt2dnZK8eLFlY8++sgwBPPfQ4ufNBQ8q2G/gDJmzBjD/ScNBc9qiGpWf9MdO3YoYWFhilarVUqXLq3Mnz9f+eCDDxRbW9unX5B/5OSaZsR4//79LGNq27ZtlufOyfvsaa+jJ7l3755ibW1tNKfHvx06dEgBlBYtWuT4nDqdTpk4caLi7++v2NjYKGFhYcr69euN/rZpaWlKjRo1lJIlSyrR0dFGx3/11VcKoCxbtkxRlMxDwR88eKC8++67StmyZRUHBwfFxcVFqVWrlrJ8+XKj86Snpyu+vr7KyJEjTYrdz8/PMDT4v+bMmaM0aNDA8DcuXbq08uGHHyoxMTGGfTL+Dv9+bT7NypUrlfr16ysODg6Kg4ODUrZsWeXdd99VLly4YNgn47115MgRpU6dOoqtra3i7++vfP3115nOd+/ePcPnmFarVSpVqpTl1A1paWnK1KlTlbJlyyparVbx8vJSWrdurRw9etSwT07eP8nJycqHH36ohIaGKk5OToqDg4MSGhqqfPvttzl6/jNmzFAcHR0N0xeY8tp4kpx8bsbGxir+/v5K1apVMw0vf//99xW1Wq0cOHBAUZTMQ8Fz+t0VHR2taLVaZf78+Tm6FrklyU0WnvZhKwq/L7/8UgEyJX0iZzp27KgEBwdbOgyzev3115X69etn+diJEycyzfNUWKxevVqxs7NT7ty5Y+lQntnTfjgUdtHR0Yq7u7vZEwBL+PLLLxVfX98czTv1LKTPjSjS/rsOUVJSEnPmzCEkJIQSJUpYKKrC47/X79KlS2zcuJFGjRpZJqB8MmbMGA4fPpzlkOZ58+bh6OiYow6+Bc3kyZMZOHCgoR+GKJhcXFz46KOPmDp1apFYTy9DamoqM2bMYOTIkXkyCvJppM+NKNK6dOlCqVKlqFKlCjExMSxZsoTz588bhvOKpwsKCqJv374EBQVx/fp1vvvuO7RaLR999JGlQzOrUqVKZeqgu27dOs6ePcvcuXMZOHBgpvlVCoOsOjCLgunjjz/m448/tnQYeUqj0XDjxo18KUuSG1GktWzZkvnz5/Pzzz+Tnp5O+fLlWbp0aaYRHyJrrVq14tdffyUiIgIbGxvq1KnDxIkTM01w+Tx47733uHfvHm3atMlyRI0QouBQKUoBGp4hhBBCCPGMpM+NEEIIIYoUSW6EEEIIUaQ8d31udDodd+7cwcnJKd/WihJCCCHEs1EUhbi4OIoXL57tRJ3PXXJz584d/Pz8LB2GEEIIIXLh5s2blCxZ8qn7PHfJTcZU2zdv3sw0tboQQgghCqbY2Fj8/PwM3+NP89wlNxlNUc7OzpLcCCGEEIVMTrqUSIdiIYQQQhQpktwIIYQQokiR5EYIIYQQRcpz1+dGCCFEwZaenk5qaqqlwxAWoNVqsx3mnROS3AghhCgQFEUhIiKC6OhoS4ciLEStVhMYGIhWq32m80hyI4QQokDISGy8vb2xt7eXiVafMxmT7N69e5dSpUo9099fkhshhBAWl56ebkhsPDw8LB2OsBAvLy/u3LlDWloaGo0m1+eRDsVCCCEsLqOPjb29vYUjEZaU0RyVnp7+TOeR5EYIIUSBIU1Rz7e8+vtLciOEEEKIIkWSGyGEEEIUKZLcCCGEEKJIKRDJzTfffENAQAC2trbUqlWLQ4cOPXHfH3/8EZVKZXSztbXNx2if7PqpE6TJxFNCCPFcS0lJsXQIzz2LJzfLli1j6NChjBkzhmPHjhEaGkrLli2JjIx84jHOzs7cvXvXcLt+/Xo+Rpy1qNs3WfnFaH54//84v38viqJYOiQhhBD5oFGjRgwcOJAhQ4bg6emJjY0NKpWKLVu2EBYWhp2dHU2aNCEyMpJNmzZRrlw5nJ2d6dWrF48fPzacZ8WKFVSqVAk7Ozs8PDxo1qwZCQkJFnxmhZfFk5sZM2bw5ptv0q9fP8qXL8/333+Pvb09CxcufOIxKpWKYsWKGW4+Pj75GHHW4qOiwF5D7P17bPhqCr+OHMbt82ctHZYQQhRKiqLwOCXNIrfc/DhdtGgRWq2Wffv28f333wPw2Wef8fXXX7N//35u3rxJt27dmDlzJr/88gsbNmxg69atzJ49G4C7d+/Ss2dPXn/9dc6dO8fu3bvp0qWL/FDOJYtO4peSksLRo0cZPny4YZtaraZZs2YcOHDgicfFx8fj7++PTqejatWqTJw4kQoVKmS5b3JyMsnJyYb7sbGxefcE/uWiy30W171E6HV3Kl915e7lCywd8xEhtepStXUHvAOC0NrJ/A1CCJETianplB+9xSJlnx3XEnutaV+PISEhTJkyBdAnKgDjx4+nXr16APTv35/hw4cTHh5OUFAQAF27dmXXrl18/PHH3L17l7S0NLp06YK/vz8AlSpVyqun9NyxaHLz4MED0tPTM9W8+Pj4cP78+SyPKVOmDAsXLqRy5crExMQwbdo06taty5kzZyhZsmSm/b/44gvGjh1rlvj/LdQrlBp+tTlgfYCzJaJpc6cszhcSuHRwP5cO7gfAtZgv3v5BePkH4lkqAI+Sfrh4F0NtZWX2+IQQQphPtWrVMm2rXLmy4f8+Pj7Y29sbEpuMbRl9TENDQ2natCmVKlWiZcuWtGjRgq5du+Lm5mb+4IugQrf8Qp06dahTp47hft26dSlXrhxz5szh888/z7T/8OHDGTp0qOF+bGwsfn5+eR5X6pEImq9sQEPCmF5/HiuDzuBfwpOXHlQl5eYD4h9GER1xl+iIu1w8uM9wnJW1NW7FS+Jewg/vgCBKvFAOn9LBaGwKRidpIYSwBDuNFWfHtbRY2aZycHDItO3fyweoVKpMywmoVCp0Oh0AVlZWbNu2jf379xuaqz799FMOHjxIYGCgyfE87yya3Hh6emJlZcW9e/eMtt+7d49ixYrl6BwajYawsDAuX76c5eM2NjbY2Ng8c6zZ0aWmEakqjk1qLL+2/ZVP/viEy9GXmVFiK90ad+PtkIkk3rnP/etXiLx+laibN3h45xZpKck8uHGNBzeucfHAHwCorazw8g+ieJmylChTnhJlK+Do5m725yCEEAWFSqUyuWmosFOpVNSrV4969eoxevRo/P39Wb16tdEPdJEzFn3laLVaqlWrxo4dO+jUqROgXxV0x44dDBw4MEfnSE9P5/Tp07Rp08aMkWbPJagYEE2ytSOl7QNY2m4pM4/OZMm5JSy/uJxdN3fxSc1PaN6us2F6aUWnI/ZBJFG3bvLg5nXuhV/i9sVzJDx6yL0rl7h35RLHN60DwNXHlxJlK1CiXHlKVQjFxdvynaiFEELkjYMHD7Jjxw5atGiBt7c3Bw8e5P79+5QrV87SoRVKFk+Lhw4dSp8+fahevTo1a9Zk5syZJCQk0K9fPwB69+5NiRIl+OKLLwAYN24ctWvXJjg4mOjoaKZOncr169d54403LPk0cPTzQaX7G0VtTWz4XdwrBPBxzY9p5NeIz//6nOux1/lgzwc0LNmQT2t9iq+jLyq1GhfvYrh4FyOoag1AP0Ig7sF9bl88x50L57h94Sz3r18l+t5dou/d5cye7QC4Fy9JYFh1AqtUp0S5Clg/w+qpQgghLMvZ2Zm9e/cyc+ZMYmNj8ff3Z/r06bRu3drSoRVKKqUAjDP7+uuvmTp1KhEREVSpUoVZs2ZRq1YtQD9/QEBAAD/++CMA77//PqtWrSIiIgI3NzeqVavG+PHjCQsLy1FZsbGxuLi4EBMTg7Ozc54+j4X9V5KocaNNewcC29YybE9OT2buqbks/Hshabo07KztGFptKN3LdM/RImHJjxO4c+Ect86f4da5M9y9dB7ln3ZaAI2NLQGhVSlTtwFBVatLfx0hRKGTlJTE1atXCQwMLDATs4r897TXgSnf3wUiuclP5kxufn3zFx5aFaNeWApV3m6V6fHw6HDGHhjL8cjjALQPas/oOqOxtTbtjZyUEM/1Uye4euII104cJSH6keExja0dwdVrUaZuAwJCw7CylhodIUTBJ8mNgLxLbizeLFWUONim8zAV4iLjsny8tGtpfmz1I4vPLubLo1+y7so6Lkdf5svGX1LCsUSOy7F1cKRMnfqUqVMfRacj8toVLv71J+f3/0Hs/Xuc+3M35/7cjb2LK5WbtSK0WWsc3T3y6mkKIYQQBZokN3nIwckaHkL8oyevK6JWqelToQ/l3MsxbM8wzj08R4/1PZjacCq1fWubXKZKrcYnKBifoGDq9+zD3UsXuLB/LxcO/EFC9CP+WrmUQ2t+44Xa9Qlr1Q7fkLI5agoTQgghCiuLL79QlDh52AHw+HH2LX01fWuyrN0yynuUJzo5mre3vc0v5355pvJVKhXFXyhL475v8eY3P9BuyCeUKFseXXo65/ft4ddRH7J09EfcvnDumcoRQgghCjJJbvKQUzF9G+Dj1JxViPk6+rKo1SI6lO6ATtHxxaEvWBe+Lk9isbK2pkyd+vQYO4VXJ31FhUbNsNJouHPxHEtHf8jv0yfy8M7tPClLCCGEKEgkuclDLqW8AEhUZZ6p8klsrW0ZX288fcr3AWD0vtEcuPPkdbVywyewNK0GDOGNWfOp1KQFKpWaS4f2s2jYO+xY+B2PY6LztDwhhBDCkiS5yUOuIcUBSLV2ICU258vUq1QqhlYfSquAVqQpaby/+30uPLyQ5/E5unvQ4u1B9J46m6CqNdClp3NiywZ++OAdLh85mOflCSGEEJYgyU0esvV2wyo9CYDoy6Y1+ahVaibUn0B1n+okpCbwzvZ3iEiIMEeYePr50/njMbw8aiJepQJIiotl7dTP2b7gO1JTkrM/gRBCCFGASXKTh9RqNbY6fY1NzLVIk4/XWmmZ2XgmpV1KE5kYyYDtA4hNic3rMA1KVaxMr4lfUq1dZwBObt3ALyOG8uDGNbOVKYQQQpibJDd5zN5KX/MRezs6V8e72LjwXbPv8LLz4nL0ZYbtHoY551m01mho9Fp/XhoxDnsXVx7cvM6SEe9zavtms5UphBDPs2vXrqFSqThx4oSlQymyJLnJY/Z2+kQk7kHO+9z8l6+jL982+xZbK1sO3D3A6sur8yq8JwoIrUqfqV8TVLUG6ampbJv3Nft/+8WsiZUQQghhDpLc5DFHZ/1yBwkxqc90nrLuZRkYpl8ZfdqRaTxIfPDMsWXH3sWVTh+Npm63VwA4sOIX9iyeLwmOEEKIQkWSmzzm5GUPQELis88C/Eq5VyjvUZ64lDgmHZr0zOfLCZVKRZ2XetK479sAHN2wlq1zZqPTpedL+UIIUdhs3ryZ+vXr4+rqioeHB+3atSM8PNzw+KFDhwgLC8PW1pbq1atz/Phxo+PT09Pp378/gYGB2NnZUaZMGb766iujffr27UunTp2YOHEiPj4+uLq6Mm7cONLS0vjwww9xd3enZMmS/PDDD/nynAs6WX4hjzn5usHZVBLTtM98Lmu1NZ/V+YyeG3qy5doW2gW1o5Ffo2cPMgeqtm6Pjb09W777ir93bSUlKZE2A4fKQpxCiPyhKJD62DJla+zBhGVqEhISGDp0KJUrVyY+Pp7Ro0fTuXNnTpw4wePHj2nXrh3NmzdnyZIlXL16lcGDBxsdr9PpKFmyJL/99hseHh7s37+ft956C19fX7p162bYb+fOnZQsWZK9e/eyb98++vfvz/79+2nQoAEHDx5k2bJlvP322zRv3pySJUvm2eUojGRV8Dx27/AFViy4jVV6Mm/PbZUn6zjNODKDH878gI+9D2s6rsFR65gHkebMxYP72PDVVHTpaQRVq0nHYZ+iVlvlW/lCiOdDptWgUxJgYnHLBDPiDmhzPhnrfz148AAvLy9Onz7N/v37GTFiBLdu3TKscv39998zYMAAjh8/TpUqVbI8x8CBA4mIiGDFihWAvuZm9+7dXLlyBbVa3+hStmxZvL292bt3L6CvAXJxcWH+/Pn06NEj1/FbUl6tCi7NUnksYyK/dCsbEu9H58k5B1QZQEnHktx7fI9Zx2flyTlz6oVa9ej80SistTZcOXqIP35ZlK/lCyFEQXfp0iV69uxJUFAQzs7OBAQEAHDjxg3OnTtH5cqVjb6o69Spk+kc33zzDdWqVcPLywtHR0fmzp3LjRs3jPapUKGCIbEB8PHxoVKlSob7VlZWeHh4EBlp+lQkRY00S+UxG1cnNGkJpFo7EH3pDvbebs98TjtrO0bVGcXb295m6fmltAlsQxXvKs8ebA4FVKlGywGD2fDVFI6sW4V3YGnK1WuYb+ULIZ5DGnt9DYqlyjZB+/bt8ff3Z968eRQvXhydTkfFihVJSUnJ0fFLly5l2LBhTJ8+nTp16uDk5MTUqVM5eNB45niNxrhbgEqlynKbTqczKf6iSGpuzMBW0bcTx964n2fnrFu8Lh1Kd0BB4fO/Pken5O+Lt2zdBtTs2BWArd/P4t7V8GyOEEKIZ6BS6ZuGLHEzoTtBVFQUFy5cYOTIkTRt2pRy5crx6NEjw+PlypXj1KlTJCUlGbb99ddfRufYt28fdevW5Z133iEsLIzg4GCjDsnCdJLcmIG9tT5bj72bt7MLD6s+DEeNIxcfXWTPzT15eu6cqNfjNQKrVCMtJZm108bzODYm32MQQoiCxM3NDQ8PD+bOncvly5fZuXMnQ4cONTzeq1cvVCoVb775JmfPnmXjxo1MmzbN6BwhISEcOXKELVu2cPHiRUaNGsXhw4fz+6kUKZLcmIGDgz7rj4vK257+brZudCuj7zk//3T+zz+jVlvRZtCHuPkWJ+7BfdZ9+QXpaWn5GoMQQhQkarWapUuXcvToUSpWrMj777/P1KlTDY87Ojqybt06Tp8+TVhYGJ9++imTJ082Osfbb79Nly5d6N69O7Vq1SIqKop33nknv59KkSKjpczgj7HLOXXXEz/NHTrMfjVPz/0g8QGtVrYiOT2ZBS0WUNO3Zp6ePyeibt3g508/IDUpkbDW7Wnyz5w4QgiRW08bJSOeHzJaqgBz8tYPIXyclPeX19POk07BnQB97Y0leJQsRZuBHwBwfNM6bp49bZE4hBBCiKxIcmMGziXcAUjU2Zjl/P0q9sNKZcWBuwc4E3XGLGVkJ7hGbSo3bQXA9vnfkp72bMtNCCGEEHlFkhszcAn0ASDJyhFdet6PairhWILWga0BWHB6QZ6fP6fq9+qDnbMLD2/f5Mj6NRaLQwghhPg3SW7MwDm4OCg6dGoN8bfybjj4v/Wv2B+A7de3cyXmilnKyI6doxMNX30dgL9WLiUm8p5F4hBCCCH+TZIbM9DY2WKTFg9ATPhds5QR7BZMY7/GKCgsPL3QLGXkRPkGTShZviJpKcns/HGOxeIQQgghMkhyYyZ2JAIQeyvKbGW8UekNADZc2cDdePMkUdlRqVQ06/8Oaisrrhw9xOXDf2V/kBBCCGFGktyYib1WP/9LbETeTuT3b5W9KlOrWC3SlDQWnbXcmk8eJUtRvX0XAHb+MIeUpESLxSKEEEJIcmMm9g76Sxv/MCmbPZ9N/0r6vjerLq0iITXBrGU9Te0u3XH28iEu6j4HVvxqsTiEEEIISW7MxMlNPww8IS7drOXU9q1NgHMAiWmJbL662axlPY3GxpYm/fST+R3buJbY+7IqrRBCCMuQ5MZMnIo5AZCQYmXWclQqFV1C9E1Cqy6tMmtZ2SldrSalKoaiS0/n0NrfLBqLEEIUVNeuXUOlUnHixAlLh5Ktzz77jCpVqlg6DJNJcmMmziUzJvKzM3tZ7Uu3x1plzakHp7j86LLZy3uaOi/1BOD0zm3EPjDPMHghhHgexcbGMmrUKCpUqICdnR0eHh7UqFGDKVOmGK1ELiS5MRvXoGIAJFs7kp5i3sUlPe08aVCyAQCrLlu29qZk+YqULF8RXXoah39fadFYhBCiqHj48CG1a9fmhx9+YNiwYRw8eJBjx44xYcIEjh8/zi+//PLEY1NSUvIx0oJBkhszcQrwRaVLA5Wa2KvmH6ad0TS1Pnw9qemWXQrhf7U3W4h/aL6h8EIIURBs3ryZ+vXr4+rqioeHB+3atSM8PNzw+KFDhwgLC8PW1pbq1atz/Phxo+PT09Pp378/gYGB2NnZUaZMGb766iujfUaMGMGNGzc4dOgQ/fr1o3Llyvj7+9OiRQt+/fVXo1XEAwIC+Pzzz+nduzfOzs689dZbAHz88ce88MIL2NvbExQUxKhRo0hNNf6+mDRpEj4+Pjg5OdG/f3+Sksw7KMZcJLkxE7XG+n8T+V2JMHt59UrUw9vOm0fJj9h1c5fZy3savwqVKV6mPOmpqVJ7I4TIFUVReJz62CI3RVFMijUhIYGhQ4dy5MgRduzYgVqtpnPnzuh0OuLj42nXrh3ly5fn6NGjfPbZZwwbNszoeJ1OR8mSJfntt984e/Yso0ePZsSIESxfvtzw+LJly3j11VcpXrx4ljGoVCqj+9OmTSM0NJTjx48zatQoAJycnPjxxx85e/YsX331FfPmzePLL780HLN8+XI+++wzJk6cyJEjR/D19eXbb7816VoUFNaWDqAos1cnkoQrMbfN3xZqrbamY3BH5p2ex6pLq2gR0MLsZT6JSqWizks9WDlxNKe2b6Zmp5dxcHWzWDxCiMInMS2RWr/UskjZB3sdxF5jn+P9X3rpJaP7CxcuxMvLi7Nnz7J//350Oh0LFizA1taWChUqcOvWLQYMGGDYX6PRMHbsWMP9wMBADhw4wPLly+nWrRv3798nOjqaMmXKGJVTrVo1Lly4AED79u359df/TcPRpEkTPvjgA6P9R44cafh/QEAAw4YNY+nSpXz00UcAzJw5k/79+9O/v36KkfHjx7N9+/ZCWXsjNTdmZG+jHwYedy8uX8rrHNwZgP139ltsxuIM/pXD8A0pQ1pqCofXWbYfkBBCmNOlS5fo2bMnQUFBODs7ExAQAMCNGzc4d+4clStXxtbW1rB/nTp1Mp3jm2++oVq1anh5eeHo6MjcuXO5cePGU8tdvXo1J06coGXLliQmGk+eWr169Uz7L1u2jHr16lGsWDEcHR0ZOXKkURnnzp2jVi3jhDKrWAsDqbkxIwcnK4iG+OjkfCnPz9mPGsVqcDjiMGvC1zAgdED2B5mJvvamJ6smfcbJrRup2eEl7F1cLRaPEKJwsbO242CvgxYr2xTt27fH39+fefPmUbx4cXQ6HRUrVsxxR96lS5cybNgwpk+fTp06dXBycmLq1KkcPKh//l5eXri6uhpqaTKUKlUK0Dc3RUdHGz3m4OBgdP/AgQO88sorjB07lpYtW+Li4sLSpUuZPn26Sc+1sJCaGzNycte/QRLiTWu/fRYZtTdrLq1Bp+jyrdysBFSphk9QCGkpyRxZv9qisQghCheVSoW9xt4it//2X3maqKgoLly4wMiRI2natCnlypUzGpZdrlw5Tp06ZdS089dfxmvw7du3j7p16/LOO+8QFhZGcHCwUYdktVpNt27dWLJkCXfu3MnV9dy/fz/+/v58+umnVK9enZCQEK5fv260T7ly5QwJ1ZNiLSwkuTEjp2LOADxOzb8Ksub+zXHSOHEn4Q5/3bXsi1KlUlGnaw8ATmzZQFJ8vEXjEUKIvObm5oaHhwdz587l8uXL7Ny5k6FDhxoe79WrFyqVijfffJOzZ8+yceNGpk2bZnSOkJAQjhw5wpYtW7h48SKjRo3i8OHDRvtMnDiREiVKULNmTRYuXMipU6cIDw9n9erVHDhwACurp08YGxISwo0bN1i6dCnh4eHMmjWL1auNf3QOHjyYhQsX8sMPP3Dx4kXGjBnDmTNnnvEKWYYkN2bkVFw/kV+yYpNvZdpa29ImqA0Aqy9ZvrYkqGpNPP38SU1O4tQOyy0PIYQQ5qBWq1m6dClHjx6lYsWKvP/++0ydOtXwuKOjI+vWreP06dOEhYXx6aefMnnyZKNzvP3223Tp0oXu3btTq1YtoqKijIZ2A3h4eHDo0CF69+7N1KlTqVmzJpUqVeKzzz6je/fuzJs376lxdujQgffff5+BAwdSpUoV9u/fbxhFlaF79+6MGjWKjz76iGrVqnH9+nWjjs+FiUoxdcxbIRcbG4uLiwsxMTE4OzubtazIIxf4bf5trNKT+L95bcxa1r+djTpL9/Xd0ag17O6+G2eteZ9ndk7v2srW72fh6OHJG7PmY2UtXb2EEMaSkpK4evUqgYGBRp1vxfPlaa8DU76/pebGjGxc9R260tVadLr86/9Szr0cwa7BpOpS2Xpta76V+8R46jXC3sWV+KgHXDy4z9LhCCGEKOIkuTEjrYuj/j8qNakJ+TdPgEqlol1QOwDWha/Lt3KfxFqrpUqLtgAcXb/G5AmyhBBCCFNIcmNGNq6Ohv8nP8yfuW4ytA1qiwoVxyKPcTv+dr6WnZXQ5q2x0mi4d+USty+ctXQ4QgghijBJbsxIrbHGKl1fY5Mcnb8jhYo5FKNmsZoAbLiyIV/Lzoq9iyvlX2wM6GtvhBBCCHOR5MbMrHT6SZySYx/ne9ltg/RNQevC1xWIpqCqbToCcPnIX0RHWHYGZSGEEEWXJDdmZq3oV1xNiU3MZs+819y/OTZWNlyLvcaZKMvPVeDp509AaFVQFI5t/t3S4QghhCiiJLkxM2uVfn2plLj8T24ctY408WsCwPor6/O9/KxUa9sJgL93bScpQSb1E0IIkfckuTEza/U/yU18/qwv9V/tSutHTW26uolUXapFYvg3/8pheJQsRWpSIqd3Wn6YuhBCiKJHkhsz06j189ukJORsAbW8Vrd4Xdxt3XmY9JADdw5YJIZ/U6lUhtqb45vWoUtPt2xAQgghihxJbszM2lrfkTcl0TLJjbXamtaBrQFYH14wmqbK1W+EnbMLcVH3uXRov6XDEUIIUcRIcmNmGo1+ddnUJMvVULQPag/Azps7iU+xfD8Xa62W0Ob65SiOblxr4WiEEOLZ9O3bF5VKxaRJk4y2r1mzJssVxsuWLYuNjQ0RERFZnm/Xrl20a9cOLy8vbG1tKV26NN27d2fv3r1mib8okuTGzDKSmxQLJjflPcoT6BJIcnoy265vs1gc/1alRRusrK25e/E8dy9dsHQ4QgjxTGxtbZk8eTKPHj166n5//vkniYmJdO3alUWLFmV6/Ntvv6Vp06Z4eHiwbNkyLly4wOrVq6lbty7vv/++ucIvciS5MTONjX4Z+tQUyyU3KpXKUHtTUEZNObi6UbZeQ0Bqb4QQhV+zZs0oVqwYX3zxxVP3W7BgAb169eK1115j4cKFRo/duHGDIUOGMGTIEBYtWkSTJk3w9/encuXKDB48mCNHjpjzKRQpktyYmdZWvwJ2moUHKmVM6Hc44jB34wvGBHphrTsAcPGvP4mLemDhaIQQBYmiKOgeP7bILTeTnlpZWTFx4kRmz57NrVu3stwnLi6O3377jVdffZXmzZsTExPDH3/8YXh85cqVpKam8tFHH2V5fFZNXCJr1pYOoKjT2OkvcWqaZeMo7licGsVqcDjiMOuurOOtym9ZNiDAJ7A0JctX5NbZvzmxZT0v9upr6ZCEEAWEkpjIharVLFJ2mWNHUdnbm3xc586dqVKlCmPGjGHBggWZHl+6dCkhISFUqFABgB49erBgwQJefPFFAC5evIizszPFihUzHLNy5Ur69OljuH/gwAEqVapkcmzPG6m5MTOtvRaA1HTLZ9wdS+uXP/g9/PcCsRwDQLU2nQA4tX0zqUn5t3K6EEKYw+TJk1m0aBHnzp3L9NjChQt59dVXDfdfffVVfvvtN+Li/rew8n9rZ1q2bMmJEyfYsGEDCQkJpMv0GTlSIGpuvvnmG6ZOnUpERAShoaHMnj2bmjVrZnvc0qVL6dmzJx07dmTNmjXmDzQXbBxsAEjTWVk4Ev1yDBMOTuB67HVO3j9JFe8qlg6JoGo1cPEpRsy9CM7+sdMwikoI8XxT2dlR5thRi5WdWw0aNKBly5YMHz6cvn37GrafPXuWv/76i0OHDvHxxx8btqenp7N06VLefPNNQkJCiImJISIiwlB74+joSHBwMNbWBeLrutCweM3NsmXLGDp0KGPGjOHYsWOEhobSsmVLIiMjn3rctWvXGDZsmKE6r6DSOtoCkKZYPrmx19jT3L85AGvDC0YnXrXaiqqt9J2dj238HUWns3BEQoiCQKVSoba3t8jtWfu2TJo0iXXr1nHgwP8mTl2wYAENGjTg5MmTnDhxwnAbOnSooQmra9euaDQaJk+e/EzliwKQ3MyYMYM333yTfv36Ub58eb7//nvs7e0z9SL/t/T0dF555RXGjh1LUFBQPkZrOq2z/hdAWsGoJDM0TW25uoWktILRDFSxcXO0dvY8vHOLa6eOWzocIYR4JpUqVeKVV15h1qxZAKSmprJ48WJ69uxJxYoVjW5vvPEGBw8e5MyZM5QqVYrp06fz1Vdf0adPH3bt2sW1a9c4duyY4VxWVpb/oVwYWDS5SUlJ4ejRozRr1sywTa1W06xZM6OM97/GjRuHt7c3/fv3z7aM5ORkYmNjjW75ycZZ3yktXa3N13KfpHqx6hR3KE5cahy7b+62dDgAaO3sqdREX6N0dMMaywYjhBB5YNy4cej+qYn+/fffiYqKonPnzpn2K1euHOXKlTPU3rz33nts3bqV+/fv07VrV0JCQmjTpg1Xr15l8+bN0pk4hyxanfDgwQPS09Px8fEx2u7j48P58+ezPObPP/9kwYIFnDhxIkdlfPHFF4wdO/ZZQ801G1cHANLUWhRFsfhQPrVKTbvS7Zh7ai5rwtfQKrCVRePJENaqPcc2ruP6qePcv3ENr1IBlg5JCCFy5Mcff8y0LSAggOTk/y2Y/LSOwGfPnjW636xZM6Mf/cJ0Fm+WMkVcXByvvfYa8+bNw9PTM0fHDB8+nJiYGMPt5s2bZo7SmI2ro/4/KjWp8QWjGSijaerAnQNEPn5636b84uJdjJCadQA4sm6VhaMRQghRmFk0ufH09MTKyop79+4Zbb93757ROP8M4eHhXLt2jfbt22NtbY21tTU//fQTv//+O9bW1oSHh2c6xsbGBmdnZ6NbftK6OYGir5pMehSXzd75o5RzKcK8w9ApOjZc2WDpcAyqd+gCwPl9e2RSPyGEELlm0eRGq9VSrVo1duzYYdim0+nYsWMHderUybR/2bJlOX36tFFP8w4dOtC4cWNOnDiBn59ffoafI2qNBqt0/YrgKTEJFo7mfzqU1s8OXJDmvPENLkPJchXRpadzbNPvlg5HCCFEIWXxZqmhQ4cyb948w6RHAwYMICEhgX79+gHQu3dvhg8fDugXJvtvT3NXV1ecnJyoWLEiWm3B6LT7X1aKPrlJKkDJTcuAlthY2XA5+jJno85mf0A+qd5eX3tzavsmkh8XnOslhBCi8LB4ctO9e3emTZvG6NGjqVKlCidOnGDz5s2GTsY3btzg7t2CsRZSblkr+oWlUmIeWziS/3HSOtGkVBOg4Mx5AxAUVh33En6kJCZyascWS4cjhBCiELJ4cgMwcOBArl+/TnJyMgcPHqRWrVqGx3bv3p1lT/QMP/74Y4GdnTiDNfqFpVIKSIfiDBkdizde3UjKP01nlqZSq6neXj9c8tjGtaRbesVRIYQQhU6BSG6KOmu1fghgcnxyNnvmr9q+tfGx9yEmOYZdN3dZOhyDcvUb4+DmTvzDKM7v22vpcIQQQhQyktzkA41aP1oqNaFg1I5ksFJbGToWr7682sLR/I+1RkPYP0syHFm/usB0eBZCCFE4SHKTDzT/TJWYklTwmlg6B+ubgPbf3k9EQoSFo/mf0Oat0dja8eDGNa6dPGbpcIQQQhQiktzkg4zFXFMT0ywbSBb8nP2oUawGCgq/hxec4de2Do5UbtoCgCPrVlo4GiGEEIWJJDf5QKPVX+aUpCdPv21JGbU3qy+tRqcUnFW5q7bpiEqt5sbfp4gIv2TpcIQQ4rmgUqkK/ECd7Ehykw80NvrLnJpScBKHf2vm3wwHjQO34m9x9N5RS4dj4OzpTbl6DQE4tPY3C0cjhBCisJDkJh9obfXtUqmpBbNjrJ21Ha0DWwP62puCpEbHrgBcOnSAh3duWTgaIYTIrFGjRrz33nsMGTIENzc3fHx8mDdvnmFCWicnJ4KDg9m0aZPhmD179lCzZk1sbGzw9fXlk08+IS0t7ZnOCfD333/TunVrHB0d8fHx4bXXXuPBgwdG5x00aBAfffQR7u7uFCtWjM8++8zweEBAAACdO3dGpVIZ7vft25dOnToZlTVkyBAaNWr0zDGbgyQ3+UBjp09u0tIKZnID0CVYPzPwtuvbiEspGGtgAXj6+VO6ei1QFA7/LgtqCvG8UBSF1OR0i9xyM0Jz0aJFeHp6cujQId577z0GDBjAyy+/TN26dTl27BgtWrTgtdde4/Hjx9y+fZs2bdpQo0YNTp48yXfffceCBQsYP358rs8JEB0dTZMmTQgLC+PIkSNs3ryZe/fu0a1bt0zndXBw4ODBg0yZMoVx48axbds2AA4fPgzADz/8wN27dw33zXEdzEmlPGfjbGNjY3FxcSEmJibfFtE88e169p2yx0O5R485PfOlTFMpikKX37twOfoyo2qPoluZbtkflE/uXDzHr6M+RG1lzRuz5+PkkbMV4YUQhUdSUhJXr14lMDAQW1tbUpPTmTt4j0VieeurhmhsrHK8f6NGjUhPT+ePP/4AID09HRcXF7p06cJPP/0EQEREBL6+vhw4cIB169axcuVKzp07h0qlAuDbb7/l448/JiYmBrVabfI5a9euzfjx4/njjz/YsuV/s7vfunULPz8/Lly4wAsvvJDpvAA1a9akSZMmTJo0CdD3uVm9erVRTU3fvn2Jjo426oszZMgQTpw4we7du3N1HWrXrp3pWv73dfBvpnx/S81NPtDa2wCQpiu4l1ulUtEpuBMAay6vsWgs/1X8hXL/LKiZxtENaywdjhBCZFK5cmXD/62srPDw8KBSpUqGbRlLCkVGRnLu3Dnq1KljSGwA6tWrR3x8PLdu3crVOQFOnjzJrl27cHR0NNzKli0LQHh4eJbnBfD19TWc41mZGrO5WJv17AIArZMtkEqakvNfApbQLqgdM4/O5PSD01x6dIkQtxBLh2RQs9PL3Dr3N6e2b6ZW527YOeVPrZsQwjKstWre+qqhxco2lUajMbqvUqmMtmUkMjpdzgeWmHrO+Ph42rdvz+TJkzOdy9fX96nnzS4utVqdqbkuNTXz3G3muA65UXCrEooQfXIDaWiy2dOyPOw8aOin/zApSDMWAwSEVsXLP5DU5CRObNlg6XCEEGamUqnQ2FhZ5PbvGhVzKFeuHAcOHDBKFvbt24eTkxMlS5bM9XmrVq3KmTNnCAgIIDg42Ojm4OCQ4/NoNBrS042nLvHy8sq0iPWJEydyHau5SXKTD2xc7AFIUxXs5AagS4i+Y/H68PWkphecGZVVKhU1/xk5dWzzOlKTCtYipEIIkVPvvPMON2/e5L333uP8+fOsXbuWMWPGMHToUNTq3H8tv/vuuzx8+JCePXty+PBhwsPD2bJlC/369cuUrDxNQEAAO3bsICIigkePHgHQpEkTjhw5wk8//cSlS5cYM2YMf//9d65jNTdJbvKBjYsjAOlqbYFfJ6lu8bp423nzKPkRO2/utHQ4Rl6oXR9XH1+S4mI5vXNL9gcIIUQBVKJECTZu3MihQ4cIDQ3l//7v/+jfvz8jR458pvMWL16cffv2kZ6eTosWLahUqRJDhgzB1dXVpKRp+vTpbNu2DT8/P8LCwgBo2bIlo0aN4qOPPqJGjRrExcXRu3fvZ4rXnGS0VD5IjHjAws9OAfDm9HpoHWzypdzcmnVsFvNOz6Nu8brMaT7H0uEYObltE9vnf4OjhydvzJqHlXXBrw0TQmTvaaNkxPPDIqOlUlNTef3117l69arpET/HbNyc4J9lDZKj4y0cTfY6h+iXYzhw5wC3429bOBpjFRo2xcHVjfioB5z70zLDRIUQQhRsJiU3Go2GlStlEUNTqW1ssEpPASD5UcFPbvyc/KjtWxsFpcDNWGyt1VK1TUcADq9dgWLmHvdCCCEKH5P73HTq1KnQL6hlCda6ZACSYxIsHEnOvPTCS4B+1FSarmCtZh7avA029g48vHOLy0f+snQ4QgghChiT57kJCQlh3Lhx7Nu3j2rVqmUaXjZo0KA8C64osSaVZCAlLtHSoeRIE78muNq4Evk4kv139tOgZANLh2RgY29PlZZtObh6OYfWriC4Rh2zD90UQghReJic3CxYsABXV1eOHj3K0aPGK0irVCpJbp7AGn3tR3IhSW60Vlo6lO7AT2d/YsXFFQUquQGo2roDR9evIeLyRW6eOUWpiqGWDkkIIUQBYXJyI52Jc8dKpe8bkhJfeOZneSnkJX46+xN7b+3l/uP7eNl7WTokA3sXVyo2ac6JLRs4tHaFJDdCCCEMnmmeG0VRCvy8LQWFxuqf5CYhxcKR5FyQaxBh3mGkK+msDV9r6XAyqd6uCyq1muunjnPvymVLhyOEEKKAyFVy89NPP1GpUiXs7Oyws7OjcuXKLF68OK9jK1I0VvokMOVxwZn1NycyZixeeXElOqVgjUxy8fahbD39chGH1vxm4WiEEEIUFCYnNzNmzGDAgAG0adOG5cuXs3z5clq1asX//d//8eWXX5ojxiIhY6651KTCldy08G+Bo8aRW/G3OBxx2NLhZFKzg35U18VD+3l451Y2ewshhHgemJzczJ49m++++47JkyfToUMHOnTowJQpU/j222+ZNWuWOWIsEjQa/aVOTc75+h4Fgb3GnjaBbQB97U1B41kqgKBqNUFROPz7KkuHI4QQogAwObm5e/cudevWzbS9bt26mVYMFf+jtclIbgpW005OZMx5s/3Gdh4lPbJwNJnV7PgyAGf37iT+YZSFoxFCPG/u37/PgAEDKFWqFDY2NhQrVoyWLVuyb98+wz7Hjx+ne/fu+Pr6YmNjg7+/P+3atWPdunWGvqvXrl1DpVIZbk5OTlSoUIF3332XS5cuWerpFUomJzfBwcEsX7480/Zly5YREhKSJ0EVRRpbKwBSUgtfB+zyHuUp516OVF0qv4f/bulwMilRphzFy5RHl57Gia0bLR2OEOI589JLL3H8+HEWLVrExYsX+f3332nUqBFRUfofW2vXrqV27drEx8ezaNEizp07x+bNm+ncuTMjR44kJibG6Hzbt2/n7t27nDx5kokTJ3Lu3DlCQ0PZsWOHJZ5eoWTyUPCxY8fSvXt39u7dS7169QDYt28fO3bsyDLpEXoaO32nm7SCNdlvjr1c5mXGHRjHiosr6F2+d4GbNK9a247cuXCWk9s3UatLNzTagr04qRDi6RRFIS052SJlW9vY5PgzLjo6mj/++IPdu3fTsKF+gIO/vz81a9YEICEhgf79+9O2bVtWrTJuOi9Xrhz9+/fPNOrYw8ODYsWKARAUFET79u1p2rQp/fv3Jzw8HCsrq2d9ikWeycnNSy+9xKFDh5gxY4ZhGYZy5cpx6NAhw9LoIjOtvRaA1LSClRTkVJvANkw9PJVrsdc4eu8o1YtVt3RIRoKr18bZy5vY+5Gc27uLys1aWTokIcQzSEtOZlafrhYpe9CiFWhyuDK5o6Mjjo6OrFmzhtq1a2NjY/zDauvWrURFRfHRRx898RzZJVJqtZrBgwfTuXNnjh49akicxJPlalVwNzc3lixZYpileMmSJZLYZEProE9u0nTPNLWQxThoHAwdi3+7WPCGXautrAhr1R6AoxvXyvxLQoh8YW1tzY8//siiRYtwdXWlXr16jBgxglOnTgFw8eJFAMqUKWM45vDhw4akyNHRkfXr12dbTtmyZQF9vxyRPZNqbjJWBR81apS54imytI62QDppismVZQXGyy+8zMpLK9l2fRvDk4bjautq6ZCMVGrSgv2//cLD2ze5fvIYAVWqWTokIUQuWdvYMGjRCouVbYqXXnqJtm3b8scff/DXX3+xadMmpkyZwvz587Pcv3Llypw4cQLQr9eYloP+Chk/2Apal4CCSlYFzydaZzsA0kxvCSwwKnhWKNAdi23sHajUuDmgr70RQhReKpUKja2tRW65SSBsbW1p3rw5o0aNYv/+/fTt25cxY8YYBtpcuHDBsK+NjQ3BwcEEBwfn+Pznzp0DIDAw0OTYnkcmJzcZq4J37dqVL774glmzZhndRNZsnOwBSFNpLBzJs+n6gr4NfMWlFQWy6SesdQdQqbh28hhRt25YOhwhxHOqfPnyJCQk0KJFC9zd3Zk8eXKuz6XT6Zg1axaBgYHSBSSHZFXwfGLj6gBAulqLoiiFtmqxTWAbph2ZxtWYqwWyY7GrTzGCq9fi8uG/OLpxLS3ees/SIQkhirCoqChefvllXn/9dSpXroyTkxNHjhxhypQpdOzYEUdHR+bPn0/37t1p27YtgwYNIiQkhPj4eDZv3gyQafRTVFQUERERPH78mL///puZM2dy6NAhNmzYICOlcsik5EZRFHbv3o23tzd2dnbmiqlIsnFz1P9HpSY1MdUweqqwcdQ60iawDSsvrWTFpRUFLrkBqNamE5cP/8W5vbuo36M39s4ulg5JCFFEOTo6UqtWLb788kvCw8NJTU3Fz8+PN998kxEjRgDQuXNn9u/fz+TJk+nduzcPHz7ExcWF6tWrs3TpUtq1a2d0zmbNmgFgb2+Pv78/jRs3Zu7cuSY1Yz3vVIoJbQs6nQ5bW1vOnDlTaCfsi42NxcXFhZiYGJydnfOt3PTERL4fsg9UanqProJTcfd8KzuvnXlwhh4beqBVa9nx8o4C17FYURSWDB9C5NVw6nV/jdpduls6JCFENpKSkrh69SqBgYHY5nAYtih6nvY6MOX726Q+N2q1mpCQEMOsiyLn1La2WKXrJ6RKjo63cDTPJmPG4hRdCuuurLN0OJmoVCqqte0EwImtG0hPK1yLlQohhHg2JnconjRpEh9++CF///23OeIpslQqFda6FACSox9bOJpno1KpDB2Lf7v4W4HsWFymTn0cXN1IePSQS4cOWDocIYQQ+cjk5KZ3794cOnSI0NBQ7OzscHd3N7qJJ7NS9DUIybGFO7kBfcdiO2s7rsZc5ci9I5YOJxMraw2VmupnKT4p600JIcRzxeTRUjNnzjRDGM8Ha5V+oqaUuEQLR/LsHLWOtA9qz/KLy/n1/K/UKFbD0iFlUrlpSw6uXsatc3/z4MY1PEsFWDokIYQQ+cDk5KZPnz7miOO5oFGlA5CSYJnF4PJa97LdWX5xOTtv7CTycSTe9t6WDsmIk4cnwdVrc+nQfk5u30TT1wdYOiQhRDYKYjO3yD959ffP1UJH4eHhjBw5kp49exIZGQnApk2bOHPmTJ4EVVRZq3VA0UluXnB7gareVUlX0llx0TLTpGcntIV+Payze3eSklj4mwOFKKo0Gv0Ep48fy/v0eZaSou+b+qzz+Zhcc7Nnzx5at25NvXr12Lt3LxMmTMDb25uTJ0+yYMECVqwomF9yBYG1tQIKpDwuOqN3epTtwbHIY6y4uII3K7+JRl2wZmAuVTEUt+IleXTnFuf+3E1o8zaWDkkIkQUrKytcXV0NP5jt7e0L7WSnInd0Oh3379/H3t4ea+tnW6rI5KM/+eQTxo8fz9ChQ3FycjJsb9KkCV9//fUzBVPUaayBVEhNyn6RtMKiWalmeNh6cD/xPjtv7KRlQEtLh2REpVJRpXlrdi2ax4mtG6ncrLV8YApRQBUrVgzAkOCI549araZUqVLP/DltcnJz+vRpfvnll0zbvb29efDgwTMFU9RptGpIhZSkdEuHkmc0VhpeeuEl5p6ay7ILywpccgNQvmFT/vj1Jx7cuMbtC2cpWbaCpUMSQmRBpVLh6+uLt7c3qalFp4Zb5JxWq0WtzlWPGSMmJzeurq7cvXs308qkx48fp0SJEs8cUFGm0aohAVKTi05yA/DyCy8z//R8DkccJjw6nNKupS0dkhFbB0fK1W/I6Z1bObl1oyQ3QhRwVlZWsoaSeCYmp0c9evTg448/JiIiApVKhU6nY9++fQwbNozevXubI8YiQ2Orf7OmpRat0QDFHIrR2K8xAEvPL7VwNFnL6Gtz8a99JEQ/snA0QgghzMnk5GbixImULVsWPz8/4uPjKV++PA0aNKBu3bqMHDnSHDEWGTZ2+s62qUWny41B9zL69ZvWXVlHQmqChaPJzCcoGN+QMujS0/h71zZLhyOEEMKMTE5utFot8+bN48qVK6xfv54lS5Zw/vx5Fi9eLNWI2dDY/5PcpBe9Dq21fWsT4BxAQmoC68PXWzqcLFVp0RaAk9s2oUsvWk2DQggh/ifXvXb8/Pxo06YN3bp1K7QrhOc3rYMNAGnpz95ZqqBRqVSG2pulF5YWyIm4XqhdH1snZ+Ki7nPleMFbMkIIIUTeKHrfsgWY1lG/fHuaUjRruDoEd8DO2o7L0Zc5GHHQ0uFkYq3VUrFRMwBObt1g4WiEEEKYiyQ3+cgmI7mhYE10l1ectc50LN0RgMVnF1s4mqyFNmsNwLWTx4iOuGvhaIQQQpiDJDf5yMbFHoA0VdFMbgBeLf8qKlTsvbWXqzFXLR1OJq7FfAmoUg2Ak9s3WTgaIYQQ5iDJTT7SujoAkK7WFMg+KXnB39mfhn4NAVhydomFo8laxrDwv3dvJ+2fdUyEEEIUHSZP4nfq1Kkst6tUKmxtbSlVqhQ2NjbPHFhRZOPiqP+PSk1qUhpau6JZg9O7fG9239zN7+G/817Ye7jaulo6JCNBVavj5OlF3IP7XPzrT8o3aGLpkIQQQuQhk2tuqlSpQlhYWKZblSpVKFu2LC4uLvTp04ekpCRzxFuoaV0dQdGvDJ4cU/Dmgskr1X2qU9a9LEnpSay4VPAWUlWrrajctBUAJ6RjsRBCFDkmJzerV68mJCSEuXPncuLECU6cOMHcuXMpU6YMv/zyCwsWLGDnzp0yoV8W1Pb2WKUnA5D8KN7C0ZiPSqXitfKvAfDruV9JTS94a8RUatICtZU1dy9d4N7VcEuHI4QQIg+ZnNxMmDCBr776iv79+1OpUiUqVapE//79+fLLL5k+fTqvvPIKs2fPZvXq1Tk+5zfffENAQAC2trbUqlWLQ4cOPXHfVatWUb16dVxdXXFwcKBKlSosXlwwR+b8l0qlwlqnT25SinDNDUDrgNZ42nkSmRjJlutbLB1OJg6uboTUrAPAyW0bLRyNEEKIvGRycnP69Gn8/f0zbff39+f06dOAvunq7t2cDbNdtmwZQ4cOZcyYMRw7dozQ0FBatmz5xCXv3d3d+fTTTzlw4ACnTp2iX79+9OvXjy1bCt4XaFasFX0tRlLsYwtHYl4aKw09yvQA9MPCC2IH6owZi8/9uZvkx0U72RRCiOeJyclN2bJlmTRpEin/GmWSmprKpEmTKFu2LAC3b9/Gx8cnR+ebMWMGb775Jv369aN8+fJ8//332Nvbs3Dhwiz3b9SoEZ07d6ZcuXKULl2awYMHU7lyZf78809Tn4pFWKNfWCo1ruj3SepWphs2VjacjTrLschjlg4nkxLlKuBRshRpycmc2bPT0uEIIYTIIyYnN9988w3r16+nZMmSNGvWjGbNmlGyZEnWr1/Pd999B8CVK1d45513sj1XSkoKR48epVmzZv8LSK2mWbNmHDhwINvjFUVhx44dXLhwgQYNGpj6VCzCWvVPh+L4op/cuNm60b50ewB+OvOThaPJTKVSEdpCPyz85LaNBbJ2SQghhOlMHgpet25drl69ys8//8zFixcBePnll+nVqxdOTk4AvPbaazk614MHD0hPT89Uy+Pj48P58+efeFxMTAwlSpQgOTkZKysrvv32W5o3b57lvsnJySQnJxvux8bG5ig2c7FW6xdsTElIzmbPouG1cq+x4uIKdt3cxZWYKwS5BFk6JCPlX2zCHz//yMPbN7l55hSlKoZaOiQhhBDPyOTkBsDJyYn/+7//y+tYTCr/xIkTxMfHs2PHDoYOHUpQUBCNGjXKtO8XX3zB2LFj8z/IJ7C21tcOpDwueCOIzCHINYjGfo3ZdXMXC04vYEL9CZYOyYiNvT3lGzTh5LaNHN+8TpIbIYQoAnKV3Fy6dIldu3YRGRmJTqczemz06NE5Po+npydWVlbcu3fPaPu9e/coVqzYE49Tq9UEBwcD+s7L586d44svvsgyuRk+fDhDhw413I+NjcXPzy/HMeY1jbUK0iA16flIbgDeqvwWu27uYsOVDQwIHUBJp5KWDslIWKt2nNy2kfAjh4i9H4mzl7elQxJCCPEMTO5zM2/ePMqVK8fo0aNZsWIFq1evNtzWrFlj0rm0Wi3VqlVjx44dhm06nY4dO3ZQp06dHJ9Hp9MZNT39m42NDc7OzkY3S9JoVQCkJKVbNI78VNGzInWL1yVdSeeHv3+wdDiZeJQsRamKoSiKTib1E0KIIsDk5Gb8+PFMmDCBiIgITpw4wfHjxw23Y8dMHxEzdOhQ5s2bx6JFizh37hwDBgwgISGBfv36AdC7d2+GDx9u2P+LL75g27ZtXLlyhXPnzjF9+nQWL17Mq6++anLZlqDR6i95aooumz2LlrcqvwXA6suruZdwL5u9819Y6w4AnN65ldSU56M/lBBCFFUmN0s9evSIl19+Oc8C6N69O/fv32f06NFERERQpUoVNm/ebOhkfOPGDdTq/+VgCQkJvPPOO9y6dQs7OzvKli3LkiVL6N69e57FZE4aW/0lT015vkbmVPOpRlXvqhyLPMaPZ37k45ofWzokI0FVq+Ps5UPs/Xuc/3MPlZq0sHRIQgghckmlmDj+tX///tSoUcOiHYqfRWxsLC4uLsTExFikierQpN84fM0DH6tIun7TI9/Lt6T9t/fz9va3sbWyZUvXLbjbuls6JCOH161i75KFePkH8trkWahUKkuHJIQQ4h+mfH+bXHMTHBzMqFGj+Ouvv6hUqRIajfHK1oMGDTL1lM8Vrb3+eqWlP39fnHWK16GiR0X+jvqbxWcXM7jqYEuHZKRi4+bsX/4z969f5fb5M5QsV9HSIQkhhMgFk5ObuXPn4ujoyJ49e9izZ4/RYyqVSpKbbGgdbABI05nc3anQU6lUvFn5TQbvGsyv53+lb4W+uNi4WDosAztHJ8q92IjTO7ZwfPN6SW6EEKKQMjm5uXr1qjnieG5oHf9JbhQrC0diGY38GhHiFsKlR5f49fyv/F9owWreDGvVntM7tnDp0H7ioh7g5OFp6ZCEEEKY6PmrPrAwG2c7AFLRZLNn0aRWqXmrkn7k1JJzS4hLibNwRMa8SgVQsnxFFJ2Ok9s2WTocIYQQuZCjmpuhQ4fy+eef4+DgYDQhXlZmzJiRJ4EVVbZuDkAiqWpbFEV5LjutNvdvTpBLEFdirrDozCIGhg20dEhGwlq159bZvzm1YzO1u3THWqu1dEhCCCFMkKPk5vjx46Smphr+/yTP4xe1qey9XYEHKCorUpPT0drmapLoQs1KbcWgsEEM2T2En87+RI+yPfC0KzjNP8HVa+Pk6UXcg/uc/WMXlZu2tHRIQgghTJCjb9Zdu3Zl+X9hOhtPV9S6VHRqDYmPEtD6FpwOtfmpSakmVPKsxOkHp5l3ah7Daw3P/qB8orayomrrDuxZvICjG9ZQqXFzVGppwRVCiMJCPrHzmZWTE9apjwF4fC/assFYkEqlYkjVIQAsv7icW3G3LBvQf1Rq0hKtnT0Pb9/k6omjlg5HCCGECUxObhISEhg1ahR169YlODiYoKAgo5t4OpVKhUZJAiDxfoyFo7Gsmr41qeNbhzRdGt+e+NbS4RixsbencrNWABxZv9rC0QghhDCFyR0+3njjDfbs2cNrr72Gr6+v9LPJBS0pJACJD+MtHYrFDa46mAMbDrD+ynr6VuzLC24vWDokg7BW7Tm2cS03z5zi3pXL+AQFWzokIYQQOWBycrNp0yY2bNhAvXr1zBHPc0FrpV8RPDH6sYUjsbwKnhVo4d+Crde3MvvYbGY3nW3pkAycPb0oU+dFzv25myPrV9N20IeWDkkIIUQOmNws5ebmhrt7wVoTqLCx0eiX80qKTbJwJAXDwLCBWKms2H1rN8cjnzwazxKqtesMwIUDfxD7INLC0QghhMgJk5Obzz//nNGjR/P4sdQ65JZWq2/KS4pPsXAkBUOgSyCdgjsBMPPoTExcy9WsfAJL41ehMopOx7FN6ywdjhBCiBwwObmZPn06W7ZswcfHh0qVKlG1alWjm8ierb1+6YXkxHQLR1Jw/F/o/2FjZcOxyGNsu77N0uEYqd5eX3tzesdmkh8nWDgaIYQQ2TG5z02nTp3MEMbzxdZRA9GQnKSzdCgFRjGHYrxe8XW+O/kd045M48WSL2JnbWfpsAAIDK2Gewk/Ht6+yekdW6jevoulQxJCCPEUJic3Y8aMMUcczxUbJ/3imcmpMtLs3/pV7Meay2u4m3CXhX8v5N0q71o6JABUajXV2nZi29zZHN30O2Gt22Nl/XyuDSaEEIWBTOJnAXauDgCkpD9/Sy88jZ21HcOqDwNg4emFBWpiv/IvNsbexZX4qAec2bPT0uEIIYR4ihwlN+7u7jx48AD432ipJ91E9uzc9clNqiK//v+ruX9zaharSYouhelHpls6HANrrZaaHbsCcHD1MtLTUi0ckRBCiCfJUdXBl19+iZOTEwAzZ840ZzzPBTtPF+ABKc/xyuBPolKp+KTmJ7y87mW239jOgTsHqFO8jqXDAqBys1YcWruC2PuRnNmzg8pNW1k6JCGEEFnIUXLTp0+fLP8vcsfex5XnfWXwpwlxC6FH2R78fO5nJh+azG8dfkOjtnwtl8bGlpodX2b3T/P4a9UyKjRsKn1vhBCiAHqmPjdJSUnExsYa3UT2bLzcUOn0zRqJj2RocVYGhA7AzcaN8Jhwlp1fZulwDCo3b4WDmztxD+7z967tlg5HCCFEFnK1cObAgQPx9vbGwcEBNzc3o5vInpWjIxrDyuCPLBxNweRi48KgqoMA+ObEN0Q+LhizA2u0Nv/qe7Nc+t4IIUQBZHJy89FHH7Fz506+++47bGxsmD9/PmPHjqV48eL89NNP5oixyDFeGVxqu56kc3BnKnpUJD41nokHJ1o6HINKTVvqa2+i7vP3roI14aAQQohcJDfr1q3j22+/5aWXXsLa2poXX3yRkSNHMnHiRH7++WdzxFgkafmnWSpKVgZ/Eiu1FZ/V/QxrlTU7buwoMDMX62tvXgbg4OrfSEuV2hshhChITE5uHj58SFBQEADOzs48fPgQgPr167N37968ja4I01qlAZAYLX1unqaMexn6V+oPwIS/JhCTHGPhiPQqN22Jo9TeCCFEgWRychMUFMTVq1cBKFu2LMuXLwf0NTqurq55GlxRZlgZPC7ZwpEUfG9VfosglyCikqKYdmSapcMB/pn3ptM/tTdrlpOWIougCiFEQWFyctOvXz9OnjwJwCeffMI333yDra0t77//Ph9++GGeB1hUaW1kZfCc0lppGVt3LCpUrLm8hv139ls6JAAqNWmJo4cn8VEPOLbpd0uHI4QQ4h8mJzfvv/8+gwbpR7E0a9aM8+fP88svv3D8+HEGDx6c5wEWVbZ2/6wM/jjNwpEUDlW8q9CjbA8Axh0Yx+N/RptZkrVWS/3urwH6kVOPYwtGk5kQQjzvTEpuUlNTadq0KZcuXTJs8/f3p0uXLlSuXDnPgyvKbB31k78lJysWjqTwGFx1ML4OvtyOv83s47MtHQ6gX3PKO7A0KYmPObDiF0uHI4QQAhOTG41Gw6lTp8wVy3PF1tkWkJXBTeGgcWB0ndEA/HzuZw5HHLZwRPoVwxu+qu/wfHLbJqJu37RwREIIIUxulnr11VdZsGCBOWJ5rti62AOQkm5l4UgKl/ol6tM5uDMKCiP+HEFsiuXnCSpVsTJB1Wqi6HTs/fkHS4cjhBDPPZMXNUpLS2PhwoVs376datWq4eDgYPT4jBkz8iy4oky/MngqqYrW0qEUOp/U/IQj945wM+4m4/8az5QGUywdEg1e6cfV40e4cvQQN/4+RamK0kwrhBCWYnLNzd9//03VqlVxcnLi4sWLHD9+3OgmcsbOywXAsDK4yDl7jT2TXpyElcqKTVc3sf7KekuHhEcJP0KbtwZgz+IFKDqdhSMSQojnl8k1N7t27TJHHM8de283ZGXw3KvsVZm3Q9/m2xPfMuGvCYR5h1HCsYRFY6rTtRdn9+4i8lo4Z//YRYWGTS0ajxBCPK9Mrrl5/fXXiYuLy7Q9ISGB119/PU+Ceh7YeLnKyuDP6M1Kb1LFqwrxqfGM+GME6bp0i8Zj7+xCrc7dAPhz6U+kJCVaNB4hhHhemZzcLFq0iMTEzB/aiYmJsnCmCawcHdGk/bMyeISsDJ4b1mprJr44EQeNA8cij7Hgb8t3dK/augPOXj7EP4xi/28yNFwIISwhx8lNbGwsMTExKIpCXFwcsbGxhtujR4/YuHEj3t7e5oy1SFGpVGh0/6wM/kAmf8stPyc/htccDsC3J77l2L1jFo3HWqulaf//A+DYxrVEXrti0XiEEOJ5lOPkxtXVFXd3d1QqFS+88AJubm6Gm6enJ6+//jrvvvuuOWMtcv63Mrg0Sz2LDqU70CawDelKOsP2DONB4gOLxhMUVoMXatdH0enYNu9rdBZuLhNCiOdNjnux7tq1C0VRaNKkCStXrsTd3d3wmFarxd/fn+LFi5slyKJKVgbPGyqVijF1xnD+4XmuxFzh470fM6f5HKzVluuk3bjPm1w7eYyIyxc5uW0TYS3bWSwWIYR43uT4079hw4YAXL16lVKlSqFSycy6z8qwMnhskoUjKfzsNfZ82ehLemzowaGIQ3xz4hsGV7XcWmeO7h7U79mbnQu/589fFxFSow6O7h4Wi0cIIZ4nJnco9vf3l8QmjxhWBk9ItXAkRUOQaxDj6o4DYP7p+ey+udui8YQ2b02x4BdISUxk16J5Fo1FCCGeJyYnNyLvyMrgea9VYCt6le0FwIg/R3AzznJrPanVVjR/cyAqtZqLf/3JlWOWXwtLCCGeB5LcWJCNo37pBVkZPG8Nqz6Myl6ViUuJ44PdH5CYZrn5ZrwDgqjapiMAOxZ+J3PfCCFEPpDkxoLsnG0AWRk8r2msNExvOB03GzfOPTzH6H2jLbrERd2Xe+Hs5U3s/Uj2LLb8XDxCCFHU5Sq5SUtLY/v27cyZM8cwW/GdO3eIj4/P0+CKOlkZ3HyKORRjeqPpWKus2XxtM9+f+t5isWht7Wj5f/rOzae2b+bq8SMWi0UIIZ4HJic3169fp1KlSnTs2JF3332X+/fvAzB58mSGDRuW5wEWZXYejgCyMriZ1ChWg5G1RwL6Cf62XttqsVhKVQwlrHV7ALbMmUVifOYlTIQQQuQNk5ObwYMHU716dR49eoSdnZ1he+fOndmxY0eeBlfU2Xk6A7IyuDm99MJLvFruVQA+/fNTzkadtVgsL/bsg5tvCRIePWTnQsvVJAkhRFFncnLzxx9/MHLkSLRa49qGgIAAbt++nWeBPQ/0K4NjWBlcmMcH1T+gXol6JKUn8d7O97j/+L5F4tDY2NJ64FBUajXn9+3hwoE/LBKHEEIUdSYnNzqdjvT0zF/Et27dwsnJKU+Cel7YeLn9b2Xwh9JfyVys1dZMbTCVQJdAIh9HMnjXYIuNoPINLkOtTi8DsH3+t8Q/emiROIQQoigzOblp0aIFM2fONNxXqVTEx8czZswY2rRpk5exFXlWjg7/Wxn8XrRlgyninLROfN3ka1xsXDj94DQf7f2INJ1l5heq/VIPvANKkxQfx7a5s6VJUggh8pjJyc306dPZt28f5cuXJykpiV69ehmapCZPnmyOGIssWRk8f5VyLsXsJrOxsbJh983dTDg4wSKJhZW1htbvvo+VtTVXjh3m2Ma1+R6DEEIUZSYnNyVLluTkyZOMGDGC999/n7CwMCZNmsTx48fx9vY2R4xF2v9WBpdmqfwQ5h3G5Bcno1apWXFxBXNOzbFIHJ6lAmj4Wn8A9ixZyK3zZywShxBCFEW5WjbZ2tqaV199Na9jeS5prfT9lxKjH1s4kudHU/+mjKg5gvEHx/PNiW/wtvemS0iXfI+jSst23Ll4nvP79rB+5mRem/QVDq5u+R6HEEIUNSYnN7///nuW21UqFba2tgQHBxMYGPjMgT0vtBodICuD57fuZbtz7/E95p2ex7gD4/C086RByQb5GoNKpaL5WwOJvHaFh7dvsmHWVLp++jlqK5nUUQghnoXJyU2nTp1QqVSZ+ipkbFOpVNSvX581a9bg5ia/QrNjY6OCZEiKl5XB89t7Ye8R+TiSteFr+WD3B3zf/Huq+VTL1xi0tnZ0+GAEP48Yys0zp9i3bDEv9uqbrzEIIURRY3Kfm23btlGjRg22bdtGTEwMMTExbNu2jVq1arF+/Xr27t1LVFSUzFacQ7Z2+vwyOVFWBs9vKpWKMXXH8GKJF0lKT+LdHe9y+v7pfI/Do4QfLf9vEACH1q7g8pGD+R6DEEIUJbmaoXjGjBk0bdoUJycnnJycaNq0KVOnTuXDDz+kXr16zJw5k23btpkj3iLHxlEDyMrglqJRa5jRaAY1i9UkITWBt7e/zYWHF/I9jjJ1XqRq6w4AbP5mBo/uyoSYQgiRWyYnN+Hh4Tg7O2fa7uzszJUrVwAICQnhwYMHzx7dc8CwMniKrAxuKbbWtsxuMpsqXlWIS4njrW1vcSX6Sr7H0eDVfhR/oRzJjxNYM+Vzkh8n5HsMQghRFJic3FSrVo0PP/zQsGAmwP379/noo4+oUaMGAJcuXcLPzy/H5/zmm28ICAjA1taWWrVqcejQoSfuO2/ePF588UXc3Nxwc3OjWbNmT92/oLN1lZXBCwJ7jT3fNvuWcu7leJj0kDe2vsGN2Bv5GoOVtYYOH4zA0cOTh3dusWHWVHQ6WZZDCCFMZXJys2DBAq5evUrJkiUJDg4mODiYkiVLcu3aNebPnw9AfHw8I0eOzNH5li1bxtChQxkzZgzHjh0jNDSUli1bEhkZmeX+u3fvpmfPnuzatYsDBw7g5+dHixYtCu26Vnbu/6wMjqwMbmlOWifmNp9LsGsw9xPv88bWN7gZdzNfY3BwdaPTsJFYa7RcPX6EP3/9KV/LF0KIokCl5GKKVp1Ox9atW7l48SIAZcqUoXnz5qjVJudK1KpVixo1avD1118bzu3n58d7773HJ598ku3x6enpuLm58fXXX9O7d+9s94+NjcXFxYWYmJgsm9fy2/2/TrH8xweodGkMmNMclUqapyztQeID+m3ux7XYaxRzKMaCFgso5VwqX2M4v28PG2ZNBaD1wA8o/2LjfC1fCCEKGlO+v03PRgC1Wk2rVq0YNGgQgwYNomXLlrlKbFJSUjh69CjNmjUzOnezZs04cOBAjs7x+PFjUlNTcXd3z/Lx5ORkYmNjjW4FiYPPPyuDq61JTZIRUwWBp50nC1suJNAlkIiECPpt6ce1mGv5GkPZeg2p1bkbAFvnzOLu5fzv5CyEEIVVrmYoTkhIYM+ePdy4cYOUlBSjxwYNGpTj8zx48ID09HR8fHyMtvv4+HD+/PkcnePjjz+mePHiRgnSv33xxReMHTs2xzHlN62nfmVwRa0h8VECWjtXS4ckAC97Lxa2XMgbW94gPCac17e8zvyW8wlyCcq3GOp1e5X7N65x5egh1k6bQK/x03H29Mq38oUQorAyubrl+PHjBAcH07NnTwYOHMj48eMZMmQII0aMMFotPD9MmjSJpUuXsnr1amxtbbPcZ/jw4Yb5eGJiYrh5M3/7UGTHeGXwRxaORvybp50nC1ouIMQthPuJ93l98+tcfnQ538pXqdW0GTgMj5KlSHj0kFVfjCEpQdYgE0KI7Jic3Lz//vu0b9+eR48eYWdnx19//cX169epVq0a06ZNM+lcnp6eWFlZce/ePaPt9+7do1ixYk89dtq0aUyaNImtW7dSuXLlJ+5nY2ODs7Oz0a0g0a8MngxA4v2C1WQmwMPOgwUtFlDGrQxRSVG8vuV1zkTl3yKXNvb2dPnkMxxc3Yi6dYPfp08kLVVmsxZCiKcxObk5ceIEH3zwAWq1GisrK5KTk/Hz82PKlCmMGDHCpHNptVqqVavGjh07DNt0Oh07duygTp06TzxuypQpfP7552zevJnq1aub+hQKHC36pr3Eh/KrvCBys3VjQcsFlPcoz6PkR/Tf0p+Dd/NvFmFnL286f/IZGls7bp45xdbvv0LR6fKtfCGEKGxMTm40Go2h87C3tzc3bujnAnFxcclVk8/QoUOZN28eixYt4ty5cwwYMICEhAT69esHQO/evRk+fLhh/8mTJzNq1CgWLlxIQEAAERERREREEB9feBMDw8rgj2Rl8ILKxcaFBS0WGGYyHrB9ANuu598s3D6Bpenw/ieo1GrO/bmbP5fKEHEhhHgSk5ObsLAwDh8+DEDDhg0ZPXo0P//8M0OGDKFixYomB9C9e3emTZvG6NGjqVKlCidOnGDz5s2GTsY3btzg7t27hv2/++47UlJS6Nq1K76+voabqU1iBYlWox+NnxQnK4MXZI5aR75t9i3N/ZuTqktl2J5hrLi4It/KD6hSjRZvvQfo16A6sXVjvpUthBCFicnz3Bw5coS4uDgaN25MZGQkvXv3Zv/+/YSEhLBw4UJCQ0PNFWueKGjz3ABsGfwDl5P9KeMdTbNxXSwdjshGui6d8QfHGxKb98Le481Kb+bbHEX7f/uFAyt+QaVS03bwR5SpUz9fyhVCCEsy5fvbpKHgiqLg7e1tqKHx9vZm8+bNuY9UAGBrZwXJkPxY5rkpDKzUVoyuPRp3W3fmnprL7OOziUiIYEStEVirczW7gknqdO1J/KMoTu/YwsbZU7HWaildrabZyxVCiMLCpGYpRVEIDg4ucMOpCztZGbzwUalUvBf2HsNrDkeFit8u/sagnYN4nGr+flMqlYpmb7xD2XoN0aWns+7LL7h++oTZyxVCiMLCpORGrVYTEhJCVFSUueJ5Ltk56+foSU6VpRcKm17levFl4y+xsbLhj9t/0HdzXx4kPjB7uWq1Fa3eeZ/S1WuTnprKmqmfc/v8WbOXK4QQhYHJHYonTZrEhx9+yN9//22OeJ5Ltq52gKwMXlg1LdWUBS0X4GbjxrmH53hlwyuER4ebvVwra2vaDfkY/8phpCUns2rSZ9y7kn+TDAohREFlcnLTu3dvDh06RGhoKHZ2dri7uxvdhOnsPfQrgyeT9SzLouAL9Qrl5zY/4+/sz52EO7y28TX23tpr9nKtNRo6DvuUEmUrkJL4mBUTR3P/+lWzlyuEEAWZyaOlFi1a9NTH+/Tp80wBmVtBHC0Ve/IMi7/Tz9L8xowXsbHXWDgikVuPkh4xeNdgjkceR4WKAVUG8Hblt1GrcrVGbY4lP37Migkjibh8EVsnZ14eOR7vgPxbB0sIIczNlO9vk5Obwq4gJjfpcXEsHLyDFK0zLw2pQLGyPtkfJAqs1PRUJh+ezLILywBoWLIhE1+ciLPWvK+3pIR4Vk4crU9wHJ14edQESXCEEEWGKd/fufo5GR4ezsiRI+nZsyeRkZEAbNq0iTNn8m/NnaLEyskJ+1T9oplR525ZOBrxrDRWGkbWHsm4uuPQqrXsubWHXht6cenRJbOWa+vgSNdPP6dY8Askxcfx2+efcu+q+fv+CCFEQWNycrNnzx4qVarEwYMHWbVqlWHZg5MnTzJmzJg8D/B54ajRL54ZfV1GohUVnUM681PrnyjmUIzrsdd5ZeMrbLiywaxl2tg70PXTz/ENLkNSfBwrJMERQjyHTE5uPvnkE8aPH8+2bdvQarWG7U2aNOGvv/7K0+CeJ05O+mHg0fcSLByJyEsVPCuwrN0yavnWIjEtkU/++IQJf00gJT3FbGXa2Dvw0qfj8A0pQ1JCPCs+/5S7ly+YrTwhhChoTE5uTp8+TefOnTNt9/b25sED88/vUVS5+jgAEBv7XHWBei6427ozp9kc3qz0JgBLLyyl7+a+3I2/m82RuWdj78BLIz7H94WyJCXE89vnI7l59rTZyhNCiILE5OTG1dXVaCHLDMePH6dEiRJ5EtTzyNVfP4w+IVWbzZ6iMLJSWzGo6iC+bvI1TlonTj84Tbf13dh3e5/ZyrSxt6frp59TqmJlUpMSWTVxDFeOHzZbeUIIUVCYnNz06NGDjz/+mIiICFQqFTqdjn379jFs2DB69+5tjhifC+5lSgKQrLYnJUnWmCqqGvo1ZHm75ZRzL0d0cjT/t/3/mHZ4mtmaqbS2dnT++DOCqtUkLTWFtVMncOHAn2YpSwghCgqTk5uJEydStmxZ/Pz8iI+Pp3z58jRo0IC6desycuRIc8T4XHB6IQBNShwAj64/tHA0wpxKOpVkcZvFvPzCywAsOruIXht6cfmReWYXttZq6TB0BGXqvIguPY0NX03h713bzFKWEEIUBLme5+bGjRv8/fffxMfHExYWRkhISF7HZhYFcZ6bDEte+5EYh1I0ae1KuY5VLR2OyAc7b+xkzP4xRCdHY2Nlw9BqQ+lZticqVd6vM6bTpbN93jec3rkVgPo9+1CzY1ezlCWEEHnNrPPc/Pmnvkq7VKlStGnThm7duhWaxKagyxgO/vC6dMx+XjQp1YRVHVZRr0Q9ktOT+eLQFwzYMYDIx5F5XpZabUXzt96jWjv9gIA/f13EjgXfodOl53lZQghhSSYnN02aNCEwMJARI0Zw9qysQpyXnP8ZDh4TIcPBnyde9l581/Q7Pqn5CVq1ln2399FpbSfWha8jrycQV6lUNHqtP416vwkqFSe3beT36RNJTU7K03KEEMKSTE5u7ty5wwcffMCePXuoWLEiVapUYerUqdy6JTPrpusUYhJTc328s7c9ALGxurwKSRQSKpWKV8q9wvL2y6ngUYG4lDhG/DmCIbuG8CAx72vyqrXtSPshH2Ol0RB+5CC/jfuUx7ExeV6OEEJYgsnJjaenJwMHDmTfvn2Eh4fz8ssvs2jRIgICAmjSpIk5YiwU9oc/4IWRm+g+50Cuz+H2z3Dw+BSbvApLFDKlXUuzpM0S3gt7D2u1NTtv7qTz2s5svro5z2txXqhdn64jx2Pr6MTdyxf4deQwHt6RHylCiMLvmZYqDgwM5JNPPmHSpElUqlSJPXv25FVchY6bvZZ0nUJkXHKuz+Hxr+HgqcnSD+J5Za225q3Kb7G07VLKupclOjmaD/d+yKCdg4hIiMjTskqWrUCPcVNw9vIh+t5dfhn5AddPncjTMoQQIr/lOrnZt28f77zzDr6+vvTq1YuKFSuyYYN5180pyHycbQF4mJBCSlrumpWcygRgnapfq0uGg4sy7mX4pc0vDAgdgLXamt23dtNxTUd+Pvcz6XnYCdijhB+vTJiO7wtlSU5IYOUXozmxdWOenV8IIfKbycnN8OHDCQwMpEmTJty4cYOvvvqKiIgIFi9eTKtWrcwRY6HgZq9BY6XvEHw/Pne1N1bOztin6FcHf3jhZp7FJgovjZWGd6q8w4r2K6jiVYXHaY+ZdGgSr216jQsP8269KHsXV7qNmki5Fxuj6HTsWPAtOxZ+jy5dahCFEIWPycnN3r17+fDDD7l9+zbr16+nZ8+e2NvbmyO2QkWlUuHtpK+9uReb+5Enjtb6Yx9eleHg4n9Ku5ZmUetFjKo9CkeNI6cfnKb7+u5MPjSZ2JTYPCnDWqul9btDqd9DP9P4iS3rWTXpMxLj4/Lk/EIIkV9MTm4ymqM8PT3NEU+h5u2s7wgc+QzJjbPzP8PBZXVw8R9qlZpuZbqxttNamvs3J11JZ8m5JbRf3Z7Vl1ajU559lJ1KpaJW5250GDoCaxsbrp86zs/DhxB57UoePAMhhMgf1rk98OzZs9y4cYOUFOM1cTp06PDMQRVWPv/U3DxLp2IXLzu4BbExMhxcZM3b3psZjWaw/85+Jh2axNWYq4zeP5rfLv7G8JrDqeRV6ZnLCKlVl54+xfh9+gRiIu/x68hhNH9rIOUbPL8jIoUQhYfJyy9cuXKFzp07c/r0aVQqlWF4asYU7ukFvI3ebMsv3L/A4aUTOHYvjdgXR/Nhy7K5Os3lxRvZss8WW108/ec+v4miyJnU9FR+Pvcz3538jsdpjwFoF9SOQWGD8HX0febzJ8bHsXH2NK6dOApAlZZtadT7DaysNc98biGEMIVZl18YPHgwgYGBREZGYm9vz5kzZ9i7dy/Vq1dn9+7duY258EuKpUbUWtpaHeRe7DMMBy9bQn86tSNpKQU7URSWp7HS0LdiX9Z3Xk+H0vpkeP2V9bRf056vjn1FfEr8M53fztGJzh+PpvZLPQE4sWUDy8YOJy5K+oQJIQouk5ObAwcOMG7cODw9PVGr1ajVaurXr88XX3zBoEGDzBFj4eBcHAAfHnE/9nGuT+MU4o91qv74RzdkOLjIGS97LybUn8DSdkup7lOd5PRk5p+eT9vVbVl6fimputzPnK1WW1Gv2yt0+mg0NvYO3L14nsUfD+LayWN5+AyEECLvmJzcpKen4+TkBOhnK75z5w4A/v7+XLiQd0NTCx1HHxSVGo0qndSY3E+0Zu3qin2qPqmJOi+zxQrTVPCowMKWC5nVeBYBzgE8THrIhIMT6LC6A+vC1z3T/Dilq9Xk1S9m4h1QmsS4WFZ+MYZ9y3+WhTeFEAWOyclNxYoVOXnyJAC1atViypQp7Nu3j3HjxhEUFJTnARYaVtak2XsDoI6780ynyhgO/kiGg4tcUKlUNC7VmFUdVzGi1gg8bD24FX+LEX+OoOu6ruy4sSPXSzm4FvOl5+dTqdysFSgKf638lZUTx/A4Jjpvn4QQQjwDk5ObkSNHotPpR/KMGzeOq1ev8uKLL7Jx40ZmzZqV5wEWJipnfX8Zh+RIktNy/2vWyVH/b8y9Z+svIZ5vGrWGnmV7srHLRgZXHYyT1onL0ZcZsmsIvTb0Yu+tvblKcqy1Wpq/OZDWAz/A2saGG6dP8NPHg7h6/IgZnoUQQpjO5NFSWXn48CFubm6GEVMFmdlGSwHK8t6ozq7ls9TevDFsMiXdcje54eHxSzh0qzhe1g/p9nXXPI1RPL9iU2L58e8fWXJuCYlpiYC+GWtA6AAalGyQq/dv1K0brPtyElG3bujP16gZjXq/ga2DY57GLoQQZh0tlRV3d/dCkdiYW0bNTTHVw2ea68bNL2N1cG2exCUEgLPWmUFVB7Gxy0b6VuiLnbUdZ6LOMHDnQHps6MGuG7tMngjQo2QpXpk4g2ptO4JKxZnd21k07F2u/jN0XAghLCFPkhvxj39GTPmqHj7TLMXu/wwHT1Q5kJYqnTVF3vK08+SD6h+wqcsm+lXsh521HWejzjJo1yC6rO3C2strTRpdpbGxpVHvN+n+2SRci/kS/zCKVV+MYcv3X5H8WGbaFkLkP0lu8pIhuYl6prlunMv4Y5WWCCoVMbei8yg4IYx52HkwtNpQNr+0mf4V++OgcSA8JpyR+0bSZlUbFp9dzOPUnE9rULJsBXpPmU3V1h1ApeLvXdv4cdi7MmRcCJHvJLnJS/80S/nykMi43NfcWLu6Yp/yz3Dwc7I6uDAvd1t3hlQbwrau2xhSdQiedp5EJEQw5fAUmq9ozoyjM7gbfzdH59LY2NK471t0H/0FLj7FiI96wMqJo9k6dzbJj3M//5MQQphCkpu89E9y46N6SGRM4jOdyrA6+JX7zxyWEDnhpHWif6X+bH5pM2PqjMHf2Z/YlFh++PsHWq9qzbA9wzgReSJHI6xKlq9InylfE9aqPQCnd2xh0Yfvcv3UCTM/CyGEkOQmbzkVQ0GFVpXO4+h7z3aqfwabREfIcHCRv2ysbOj6QlfWdlzLrMazqFWsFulKOluubeG1Ta/Rc0NP1l5eS1La02snNba2NOn3Nt1GT8TF24e4B/dZMWEkG7+eTkL0o3x6NkKI55EkN3nJSkOKnRcAqthnm8jPxcsOgLgY6VAsLMNKbUXjUo2Z33I+K9qvoHNwZ7RqLWeizjBy30iarWjGjCMzuBX39Jm0/SpUpvfUr6nSsi2oVJz7YxcLh7zNsU3r0BXwhXaFEIWTJDd5TPfPSsyahGdLbtxK6YeDxyXLcHBheWXcyzCu3ji2vbyNwVUH4+vgS0xyDD+c+YE2q9owYPsAtl3fRmp61qOstLZ2NH19AK+Mn45PUDApiY/Z9eMclox4nzsXz+fzsxFCFHWS3OQxK9eSADilPNssxd7li4OiI1HtSPyj3HdOFiIvudu680alN9jUZROzGs+ibvG6KCj8eftPhu4eStPfmjL50GQuPMx6nbliwS/Qa8J0mvZ/BxsHB+5fu8Kvo4ax+buZ0lQlhMgzktzkMY2bPrnxVT3k/jNM5OdUuRxOCbcBuLH/Up7EJkReyWiymtN8Dus6reP1iq/jZefFo+RHLDm3hK7rutJtXTeWnF3CwyTj1e3VaiuqtGjD61/OoULDZgCc2b2dhUPe5uiGtaSnpVniKQkhipA8WX6hMDHn8gsA7PsKto1mdXo9Sr3xM9X83XJ9qo39v+aqpjwhxRNpMbptHgYpRN5L06Wx/85+1lxew66bu0jT6ZMUa5U19UvWp2PpjjQo2QCtlXFT652L59ix8Hsir4YD+lmPm/R7m1IVQ/P9OQghCi5Tvr+t8ymm50fGXDeqh9x/hrluAIqXduLqDYi4K79kRcFnrbamQckGNCjZgOikaDZe3cjv4b9zJuoMu2/uZvfN3bjYuNDCvwVtg9oS5h2GWqWm+AvleGXiDP7euY0/lv5E1K0b/Pb5p5SuXosXe/XFo4SfpZ+aEKKQkZqbvHZ9P/zQmus6b3a32kafugG5PlXU3oMs/UU/fX2/KfWwd7bJoyCFyD/h0eH8Hv4768PXE5kYadhe3KE4bYLa0CawDcGuwahUKhLj49i/fAknt21C0elQqdVUbtqKui/3wt7F1XJPQghhcaZ8f0tyk9ceXYOvQklWrPmq9p981Lpcrk+lS07mpzdXkGDvS7NO3pRpVTHv4hQin6Xr0jl87zDrw9ez/cZ2ElL/t+5UgHMAzf2b09S/KeXdy/Pwzi3++OVHwo8cBEBrZ0eNDl2p1qYjGltbSz0FIYQFSXLzFGZPbtJSYLx+rpsxZX5nbM+Gz3S69f2/5fr/t3ef4XFV977Hv9N7VZclWZZ7751uE4pTDBxKDgkESCGUAD4pQAI5CZeYm4QbQiAQkgAhdAgpBAIYgyG44IJtXCXZlq3ep2r6zL4vxh5QbISNJY3K//M8+5mtPbus0bKln9Zeey3dBMaPCLH0js/3RgmFyLpIIsKa+jW8cuAV3mt4L9M/B9ItOktGLmFp2VJy27W8++RjtBxId6o32R3MX34x084+D51eWjKFGE6kz002afVEDDkYox0kvT0PbnY8iiqsHKqD5sbjn6VZiIHOqDVybvm5nFt+LoFYgH/X/5s3a9/kvYb3aOxq5M+7/8yfd/+ZXFMuS754FtM7ltPyxgZ8Lc2seeIPbH75JeZdcAlTzzoHrU6X7Y8jhBhgJNz0gbilCGO0A3Xg+CYb7EnZ4nFseDaEL2UnEopjNMsPcjG02PS2dN+bivOJJCKsbVzL6kOrWVO3hvZwO89VPc9zgGO+nbP8k8jZFiDo6eStRx9m0z/+woILL2Xy6UvRaOXHmRAiTca56QuHn5gyhk8+3OQsnIkp3AoqFfXrqk76fEIMZEatkSVlS/jZqT/jnUvf4aGlD3HR2ItwG934En7+at7Ao/N3sX5yBzETBNrbWPXIAzx687fYueZNmc5BCAFIy02f0LlGwEFwxNOjFBu0ms98LrXBQK7OSx351G6sYczSyb1XUCEGMJ1GxykjTuGUEafwo9SP2Nm+k/ca3mNtw1p2aXaxr6SL8bVWpu53QFsLrz90H28+/0dmLb+QxWddIC05Qgxj8r+/DxjcZQAUqjpp9UcpdZtP6nxFI63UNUBz/Wcf8ViIwUyr1jIjfwYz8mdww8wb6Ix0sq5xHesa1vHW2LXkVyWYut+OsSPApj/+iXefeRzVnFKmLzmPhSNPIceUk+2PIIToRxJu+oDKcXggPzppDZx8uCldNIaNL4TwJu3EIgn0Rqk2Mby5jW4+X/F5Pl/xeVJKisrOSt6rWUP1W2tw7PBjCmng3Xo+XPcwL5T9kuiMPGaOms+cwjnMKZiDw+DI9kcQQvQh+S3ZF+zFQLrlptJ/8pNe5p8yE+OTLxMxuKlfX0XFmZNO+pxCDBVqlZqJOROZmDMR5nwbf5eXVa/+mZrV72LwhJl2wEGyJsrBon/xWvnzdDjjjHONY27hXOYUzGF2wWycRme2P4YQohdJuOkLh8NNkaqTf/dCuFEbDORoPDTgpm7DAQk3QvTAbnFy0cU3olx0Pfu3bGT935+jtbqa0Y1WRjdaaXNE2VNez9PtlTy550kAxrrGMqcg3aozM38meea8LH8KIcTJkHDTF2zpcGNSxfB1tgKjTvqUhWVmGhqhqS580ucSYjhQqdWMmbuAMXMX0HJgH1tfe5m9a98hzwd52w2cWl1I/cgkm/Lqqaaaak81z+x9BoAR1hHpPj556X4+Y51j0ag/+4MBQoj+JeGmL+iMhHVOTHEvMU89MP+kT1m2cDRb/hLCk3AQjyXQ6aXqhDheBRVjOPe6Wzjt8qv4cPXrbH/jFYKeTkr3QOmeEZjLi/GNN7PFWUtVcD8NwQYagg28cuAVAMxaM9PypmUCz7S8adj0tix/KiHEJ8n6b8gHH3yQX/ziFzQ3NzN9+nR+85vfMG/evGPuu2vXLu688062bNnCoUOH+NWvfsXNN9/cvwU+ThFTEaa4F/wNvXK+glNnoH/mNWJ6O40bqhl52mefs0qI4crscLLgwkuZ+8WLOLB1EzvfeoOarVsIHWxEdxBOM5m4asHX0EwvYb+xje3t29netp2ueBcbmjawoWlD5lzl9nImuicyKWdSps+PXd8HU7oIIU5YVsPNc889x4oVK3j44YeZP38+9913H+eccw6VlZXk5+cftX8oFKKiooKLL76YW265JQslPn4pWxH496ALnvxAfgAao5EcdSdN2Kldv0/CjRAnQaPVMnbuQsbOXUigs51da1azc80qfC3N7Hl7NbwNOSVlfOPMcxl/3s9oVNrZ3radra1b2da6jfpgPQf9BznoP8i/Dv4rc94yWxmTciYxOWdyJvRIC48Q/S+rE2fOnz+fuXPn8sADDwCQSqUoLS3lxhtv5NZbb+3x2PLycm6++eYTbrnp84kzD/M8fyOu3U/wCBfxzf99tFfOue4nT7O1qZA8dRuX/PbSXjmnECJNSaWo37OTHW+vonrDWhLxGABqjYaRU2cwYfHpjJm7AL3JTGekkz0de9jdsZs9nenXhuCxW2nLbGXplh33xMyry+jqz48mxJAwKCbOjMVibNmyhdtuuy2zTa1Ws3TpUtavX99r14lGo0SjHw1+5/f7e+3cPTHmlADgSrSd9CjFR5QtGMXWv4ZpT7gItgWw5slfhEL0FpVaTenkaZROnkb06mvZu/Zddr79Bs37q6nZtoWabVvQ6vSMmjWHCYtPZ96M2SwesThzvDfiZXfnbnZ37GZX+y52d+ymsauR2kAttYFaXj/4embffFM+Y91jGecax1hn+rXCUYFOI3PHCdEbshZu2tvbSSaTFBQUdNteUFDA3r17e+06K1eu5Cc/+Umvne94GXNKAShSdfTKKMUAI86ag+Ppp/GZStn8h3c447bPn/Q5hRBHM5gtTD/7PKaffR6djfXsXfsue9e9i6exnur311H9/jq0BgOjZsxm3PzFVMyai9PkZFHxIhYVL8qcxxPxsKdzD3s69rCncw97O/dyyH+I1nArrQ2trG1Ym9lXq9JS7ihnrHMsY13pZZxrHEWWIlQqVTa+DUIMWlnvUNzXbrvtNlasWJH52u/3U1pa2ufXVTnSLTdFqk5aA5FeCTcqnY4p042srYKqAyoWhePoTfKXnhB9yV1cwqKL/5uF//Vl2g7VsHftO1Sufw9/W0sm6Gh0OkZOm8no2fOomDkXqzs93YPL6Doq8HTFu6j2VFPtraaqs4oqTxXVnmoC8QD7vPvY593XrR+PVWdljHNMJvBUOCqocFSQa8qV0CPEJ8hauMnNzUWj0dDS0tJte0tLC4WFhb12HYPBgMFg6LXzHbfDM4MXqjqp8p38QH5HTP76Mj74zquEjbls+9M7zLt2aa+dWwjxyVQqFfnlFeSXV3Dqf3+N1pr9VL2/lur31+JpauTAlo0c2LIRgPzy0VTMnkvFzLkUjh6LSq3OnMeis2TmyTpCURRaQi2ZoFPtrabKU0WNr4ZgPMi2tm1sa9vWrTxWnZVRjlHdlgpHBSW2EnRq+aNHDG9ZCzd6vZ7Zs2ezevVqli9fDqQ7FK9evZobbrghW8XqPbYiACyqKJ7ONqC4V06rs1uZWBbhg1bYuSXI7EQSTS/05xFCHD+VSkVBxRgKKsZwymVX0F53iH2b1lPzwWaa9lfRenA/rQf3s+Evz2KyOxg1fRblM+dQPn0WJuvRfeVUKhWFlkIKLYWcVnJaZns8Geeg/2Am8FR7qjnoP0hdoI5gPMiO9h3saN/R7VxalZZSeymjHaMZ7UwvFY4Kyh3lGDRZ+ENPiCzI6m2pFStWcOWVVzJnzhzmzZvHfffdR1dXF1dddRUAV1xxBSNGjGDlypVAuhPy7t27M+sNDQ1s27YNq9XKmDFjsvY5jklvJqSxY076iXTWA9N77dQzvnUOO+58n7DOzt6/bmLyxQt67dxCiBOjUqnIKysnr6ychRd9mZDPS822LRzYspGDH24l7Pex+99vs/vfb6NSqSkaO57y6bMYOW0mhaPHotZ88h8nOo0uczvq42LJGLX+Wmr8NRzwHqDGX0ONL72EE+HM+pu1b2aOUavUFJoLKbWVUmovpdRWSpmtLP21rRSz7uRvnQsxUGT1UXCABx54IDOI34wZM7j//vuZPz89ou8ZZ5xBeXk5jz/+OAAHDx5k1KijpzI4/fTTWbNmzXFdr78eBQdo/8Uccruq+X3pz/nGNd/q1XOvXvEoe0PlOOnkvx+6SO69CzEAJRMJGqv2ULN1MzXbttBee7Db+waLhbIp0xk5dSalk6fhKio+qf/LR25vHfAeYL9vP/u9+zngO8A+7z4CsUCPx+YYczJBp9RWSomtJLPuNrrlZ4zIuhP5/Z31cNPf+jPcNP32ixS1vsMfXTdzzU29+8SWd8c+nrl/HymNnmWXFVB+xuRePb8Qovf529s4uH0Lh7Zv5dDObUS7urq9b3G5KZk4hdJJUymZNAV30Yhu/XU+K0VR6Ih0UBeooy5QR62/NrNeF6jDG/X2eLxZaz4q9JTYSii2FFNoKcSoNZ50GYX4NINinJthwV4MraDrau71UzunjqFUu4ZDSgVb/rJTwo0Qg4A9N49pS85l2pJzSaWStOzfx8EPP6B2x3aaqvfS5emkct27VK57F0i37BSNGU/hmPEUjx1P4ZhxmGwn/keZSqUi15RLrimXmfkzj3rfH/N/FHb8dd2CT0uohVAiRKWnkkpP5THP7za6KbIUUWwtpsRa0q3Vp9BSiFYtv2pE/5KWmz4UfONnWNf9X55LnMGsG59kbEHvDrrX9PZmXnrWCyo1F98whvwpZb16fiFE/4nHojRXV1K3eyf1u3fQVF2ZGSX541zFJRSPm8CI8ZMoHjcRd3HvtO58kmgySkOggfpgfbfQUx+op6mriXAi3OPxWpWWPHMeBeYC8s35FFgKKDAXUGApoNCc7kSdZ8qTWdfFp5LbUj3oz3DD1qfg79fxbnIqr896iLsvmNrrl3jxmsdo0Y1klL2d839+Sa+fXwiRHclEgvbagzRVV9K0r5KmfVV4GuuP2s9osVI4ZhyFY8YdbuUZh9nu6JcyKoqCL+qjqauJpq4mGoON3QJQQ7CBeCr+qefRqDTkm/MzrT9HXostxZkwZNVb++ETiYFMwk0P+jXc7H8b/ryc/akiPq/cx4bbluAw9+74E/uff4vX3gKUFOddXEDF0t4PUEKIgSEcDNBUtZfGqj00VO6meV81iVj0qP3seQWZMXnyR1WQN7ICW07/D/qXTCVpC7fREmqhpauF1lBrZr051ExzVzOtoVaSSvJTz2XWmjMtP/mmfPLN+eSZ88g3p9fzTfnkmnJlCoshTMJND/o13HR1oPxyLColyVnRX/Ll85bwjdMqevUSSirFy9f+kTr1aPTJLi776anYipy9eg0hxMCUTCRoO1RD874qmvdX0bSvis6GumPua7TayB81mvzyCgpGjSZ/1BhchUV9ekvreCRTSdrD7d1afz6+3hpqxR87/jkB3UY3eaY8cs256dfDfY1yTDnkGnPJM+eRZ8qTR98HIQk3PejXcAPw1CVQ/Tq/SSznOduVvPO9M9Goe/evp0hzG8/duoqgsZAcrYdLfn0Bak12f2AJIbIjGuqi5cB+2g4doPVgeumor0VJpY7aV2c0kVtaRm7pSHLLysktLSe3bGS/3dY6XuFEON3q09VCS6iFtnAbraFWWkOttIUOr4dbSaQSx31Oq85Krim3ewvQ4RahfHM+OaYccow5EoIGEAk3Pej3cLPzL/Di1TSSx+LIr3jkinmcPang0487QY2rNvCP5zpIak1MGhXhzB+c3+vXEEIMTolYjI76Wlpq9tNakx49ue3QwWPe0oJ0K4+7uARX8QjcxSW4i0vIKS3DkV+AeoB2/FUUBW/Umw48h8NPR7iD9nB7t6Ut3PapnaA/zqQ14Ta6cRvduIwunAYnLoMLlzG9uI3udMuQMYccUw56jb4PP+XwJuGmB/0ebuJh+OU4iPq5NHoH2tGn8NTX+2ZE4a0r/8y6Q+k5rT53QR5jz5H+N0KIY0slk3iaGmivO0R77UHaag/RUXcIb2szfMKvBa3eQE5JKTklZeSUlOEqKsZZUISzoAidcXCMdaMoCl3xLtrCbbSF0v2B2sPtmZagI+GoI9xBJHni8wLa9DZyjDm4jW5yTDndgpHL6MJt+GjdaXDKY/InQMJND/o93AD8/XrY+iTPJc/gB/Fv8sYtpzGulx8LB1CSSf517e+o0UxAl4pw6Z0LcZS4ev06QoihKx6N4GlqpLOxHk9jA52N9XQ01NHZUEcy/slPPlmcLpyFRbiKSsgZUYK7pJScEaXYc/Oz3q/ns1AUhXAiTEe4g45IevFGvHiiHrwRL95oev3I++3h9hO6LXaEw+DAZXB1axk6ss1hcOA0OHEanZl1u94+bAORhJseZCXc1Pwb/vR5wmoLM0IPctH8MfysDx4LB4g2t/L891/Dby7BrvZz4d3nYHGZ+uRaQojhI5VK4m1upqP+EO11h+ior8PX0oS3uYlIV/ATj9Pq9DgKCnEWFmVaeZwFhTgKCrHn5aPRDo2nmxRFwR/zZ8JOZ6STzkhn5mtvxEtnpBNP1IMn4sEX9aHw2X792vS2dBj6WChyG904DA4cBgd2vb3bq8PgGBKTpkq46UFWwk0qBb+eBr46ro99h9WaRbx/29Jefyz8iJY31/P3p1uJ622Y6eJLty3GPdLdJ9cSQohIMIi3pQlPcyOdDfV0Hm7p8TQ1kEz00JqhUmF15+DIK8CRX4AjvzD9WlCIM78Qi9M1KFt9jkcilcAX9eGJePBEPengE/HgjXrxRX2ZliFfJL3ui/k+dX6wnhg1RuyGj4UevSP9td6RCUBHFqfBmWklMmlNA2ZeMQk3PchKuAF48yfw3v9jg3YulwVv4fbzJ/DN00b32eXqX3ydN172EjbloUtFOP+6qZTMKO2z6wkhxH9KpZL4W1vxHm7h8bY04m1pxtvchK+15RM7NB+h1emx5eWnA08mABVgceVgcbqwOJ3oDIOjr09vSKQS+GP+dPCJfBSIOiIddIY78cV8+GN+AtEAvpgPXzT9dUo5+km546VVabHpbdgNdux6O1adFaveik1vy6zb9fbM7bT/DEu9ScJND7IWbtoq4cF5pFRa5oQfQGfP480Vp2Mz9l2TbMe/N/LqI3vwW0pRp+KcdXEp48+e1GfXE0KI46UoCiGfF19rC/62FnytLfhamw8vLfjb2475+Pp/0pvMWJwubLl52HPzsecdec3HkV+I1e0esE949YeUkqIr3oUv6usWePxRf2bdF/VlWos+/ppQTrwP0RET3RN5/gvP9+InkYkzB6a88VA0A3XTNq60b+ZX/rP4v6/t5f8s77snmnJOnccFuS5evfst2qxjefPFRvzNAeZ8Zd6AaWYUQgxPKpXqcOuLi+JxE456P5lIEOho/yjsHAlAbS2EvB66PB4S8RixcIhYOISnqeGY19Fotdjz0re6HPmF2NyHW31cbixOF1aXG5PNPmRvf6lVamx6Gza9jRJKjvu4Ix2q/TE/gVggE4iC8SCBWIBgPEgwFiQQD6SDUsyHP+rPBCO7oR8bD45BWm7604aH4LVbCeRMY2rDrQC8cO1C5pb3bX+YaFMLr9/6HHWmKQAUmAN87vtnYC8cWAN1CSHE8VIUhVg4TJfXQ5enA397G/62VvztrenXw+up5KdP7aDWaLG63VhdOVjd6cXidGG2OzA7nZjtTswOB2aHC61uaHSA7mvJVLLXJ0OV21I9yGq4CbbBveNBSfKLsU/x4A4VFXkWXv3OqRh1fdtsmggEWXfrH9gVm0BKo0ebjLDwdBtTLz9FWnGEEENSKpUk2NGR7vPT0oy/rYVgZyddPg9dnk66vB5Cft8njutzLAaLBYvDhdnpTL86nOkQ5HBistsx251Y3W4sTjdavQzo15sk3PQgq+EG4KmLofoNIgtXcOrmxbQFotxw5hi+e874frl83V9WseYfTfhN6ebJYm0LS3+wBFtpfr9cXwghBpJkIkGX10Ows51gZwfBzg4CnR2EfF5Cfl/61ecl5PORSp5YHxSjzY7N5cbizjncKuTG6nJjceVgdbow2R0YrTb0poHzRNJAJuGmB1kPNztehL9cA+Zc3jrrr1z9Yh1atYqXbzyFiUX9U564L8Dau19kt7cERa1BFw8yf2aKadd9ccjedxZCiJOhKAqRriAhrzfd4uPz0OX1Eg74jgpCQU9njwMe/ieVWo3RasNotWE+3Ppjdjgw2R2HW4VcWBzO9C0yhxOD2TIsw5CEmx5kPdwkovDIGdC6G0adxrWqH/Ha7namlzh46brFvT6pZk/q39zM6mcOENTlAlAU3ceZ1y3ENVembRBCiM/qSBA60hIU7Oygy9NJ0NNB0NOZvjXm6SAcDJxQCDpCo9Olg4/Ngclux2Szf+zW2JFA5MBsd6I3m9EbTWh0ukEfiCTc9CDr4QagrSodcOJdBBd8l4Ub5hGIJPjRsol8/dSKfi1KIhzjvV++wu56K4pKgy7mZ1ZBPTN/8BU0Tme/lkUIIYabeCxKNBgkEgwQDgYIB/yEfB9vDfJkWoS6vF5i4dBnuo5ao0FvNKEzmjBaLBhtdkxWG0abDZPNjntEKZNOPbOXP13vknDTgwERbgA+fB5e+gag4q15j3D1uxbMeg1rvnsG+fb+H5Sqcesh3vz9NgKp9JxXed7dTFmQw/hvX4jGMPiH7RZCiKEgHosS8noJ+32EAj7Cfn96/fCSWfel1+PR45v8s2zKNC6+42d9XPqTI+GmBwMm3AD840b44AkUSz5XGf8faxrUXDy7hF9cPD0rxUnGU6z9/Vp2bI+CKt33xhzrYOJ0K9OvOQuTVUKOEEIMJqlUkngkQiwSJhYOEw+HP9ZKFCAS9BMOBHAVFTPrvC9mu7g9knDTgwEVbuJh+P0SaN1FoGgh02uuR1GpefmGU5gyIntj0HTU+djyp7UcOKQieXiyNbWSoKJczZyvzCGn1Jm1sgkhhBieJNz0YECFG+jW/+a1nK9ybcN5zBvl5rlvLsh656+IJ8i2375CZWWcoLk4vVFJUaRrY+aZRZR/YREqGcdBCCFEP5Bw04MBF27gY/1v4MfJa/hTfAkPf2UW504pynLB0uKdnVT/9nl27knRZvtomHRnoIbxpWEmXnUelvF9NwmoEEIIIeGmBwMy3ACs/in8+14UVNwUu45tzrNZteI0DNqBM+GbkkrRtHoTH7x6gNquHBR1emoyTSJCoaaFcWeMYeyFC9EZZMoyIYQQvUvCTQ8GbLhRFHj1e7Dp9yRQ863YLcw753K+dfrAbBEJtIf44JlN7N/lJ4wls12TijEiP8m4M0dTcepYdPqBE86EEEIMXhJuejBgww1AKgV/+zZ8+CxRRce3uY2ff+8GcgfwU0qKotCwvpLdL22irsNExPDRJKCaVJwie5Ax84oYu2w2esvA/RxCCCEGNgk3PRjQ4QYgmUB5/gpUla/QpRj4ef7POfXM81g4OgfLAL/dk/B6OfDnV9m/tZXG1AgixpzMe+pUnDx1O2VjTIz93FScU8dkvcO0EEKIwUPCTQ8GfLgBiEfwPXohjqa1eBULl8Tu5KB6JHNHuThjXD5fnFFMQRYG+jsRCY+H2lfWs29jI/UBB2FDTrf3HeF6RuREGLuonKJzFqKx2bJUUiGEEIOBhJseDIpwAxANEvzD57G2baUDFxdE76RWKQAgx6LnpesWMTLH8iknGRhSiQRN7+1k/zvV1DUoeNW53d63dDUyQtfCqBm5jDh7Hsbx42UCTyGEEN1IuOnBoAk3AGEPPLYMWncRt5Xy0ow/8PvtUfa1BhmVa+Glby/CZRl848wEmn1UvbqNmg87aA3bUFQfdTrWxfyYEj6sNg2OEU5cE0eSP6mEwgoHGp0EHiGEGK4k3PRgUIUbgEALPHYudB6A3PG0XfxXlj9WSYM3zJyRLp78+nyMusH7RFKkK86+t6uoXneI5g4dKdWxP4tGSZBr9FNcrKFsah4Fc8ejy8vr59IKIYTIFgk3PRh04QbAWwuPngv+Biiawb7zn+GCP+4gEEmwbFoRv7lsJmr14O+cG4sk8NT76dxWSefOGny17QQCKfy2cmL67nWlj/pwRRvIc8YpGuOkcPYYLDOmoXFkb9oKIYQQfUfCTQ8GZbiB9DQNj50LoQ7Im0jlxOv54moX0aSKb51WwW3nT8x2CftEqquLSHU1LR/WUl/lo6ldS0fKTUqt67afJhnF7jtAnsHPiPFuSs6YinXubNQyo7kQQgwJEm56MGjDDUDjNnjiSxDxAuC3VvCTzs/x99QivnveFL5xagWaIdCC82kS8STNe1upf/8Ajfu8tHm1JOgedtTJGI7AQezmBBqnM704nKgdDmz5Nipm5OEuHhwdsoUQQki46dGgDjcAoU54/+H0EvEBUK/k8khiGVvdy7jx3OmcPalgWI0ho6QUOhq7qPugjvottTS3KsSUT+9obdOGKCtKUjE9l6IF49Hl5n7qMUIIIbJDwk0PBn24OSLih82Poqx/AFVXGwAexcqTyaVsLfgvvrVsEfMrcj7lJEOToih0NnVRu34f/ppmkp0dJDs6SHR2kgoECdhK6XRNQPnYrS1dLIA5FcBqVWEvtOEaU0TO5JE4i23YXEZUw6BFTAghBjIJNz0YMuHmiHgYtj5Jct0DaLwHAYgqWv6eXEzVyMv41qUXkjfAB/zrT8lAgNj+/QQqa6jd3cmhFj2tqfyj+vB8nJokNmMch1uPo9CKPc+CvdCGvdiJvcCK3jiwR44WQoihQMJND4ZcuDkilYS9rxB77370jZsymw9STGLClxhz5hVQMCmLBRy44rEknfvbaN9ahaeqAX+jj0AgSUjrJGzKy8x+/kkMiQB51jAjJrgpP2sKOWOLhtVtQSGE6A8SbnowZMPNx9VtxL/m1xj2v4GBWGZzMmc8mrFLIX8C5E2EvPFgHKLfg5OkpFIkmpoIV1XTufsQnTXteFtCdEW0hNQWIloHEaOLhM561LGGZJAccxiLTYPNZcRWaMM+wo1jVAHWETlotIN3XCIhhMgWCTc9GBbh5rBol5c3/vonjJV/5zT1dgyqxFH7RMxFMO5cjOf+BIwyRszxUhIJUsEg4Q4/TZv2Ube1jpZWFR5dQbe+PP9JlUpiSAYxEsasjWOzQV65g6KZ5eTMnoDGKLcQhRDiWCTc9GA4hZsjthzq5M7n1jLOu5Zp6gOMUTUwTl1Pgcqb2SdmLkJ/4YMwZkn2CjoERJrbqH1jC+0HOujyxQiGIBTXEcZCVGcD1SdPIaGLB3Ek23Fak5hseswuM5Y8G5YiN/byAuwVxaj1nxychBBiKJNw04PhGG4AuqIJfvfuAfa3BonEk0QSSdQRL/neD7kh9gdGqVvSO86+Cj53Fxhklu7elgxHCdS14jvUTqDJQ6CtC29LiA6vGj+OT+3bo0olMCYDmNURLGYFu0uHu8hKTkUOORNK0BcVotJK52YhxNAk4aYHwzXcfJJQLMFNT6xl0cEHuUr7enqjowzmfws0/9FKYC+GskVgGZ6PmPelRCxJ6846GrccwNvoJxyIEQmliMTVRBU9MY2l2wSj/0mVimMOt2FWAuj0GnQmPTqLAb3NhMFhwZRrw1LgwlzkxuQ0YXEYMFqkFUgIMXhIuOmBhJujxRIpvvfidlo+fJOfa39Hmbqt5wPyJ8HIxVC+GCrOAJOrX8o5nCXjCfz7m/Dsa8BX24G/JYDfk8AX0RPETlJ94rPDG5Qwdm0XDnMcl1OFs9CKY1wZjsmjMbis8sSXEGJAkXDTAwk3x5ZKKfz0n7t5ft1evqV9mVnmDqLJFJFYEgUFFQrj1A2MU9V3P1BngbnXwKLvgFVm6c4GJaUQ6AjTtrcRf207UU+AmDdALBAmHgwTCyeIxlREU1oSWjMxneWYT3l9nCYVw6COYzQomPQpzCYFk0mF2arB4jJjGV2KeWQxBpMOvUmDzqCRMCSE6FMSbnog4eaTKYrCA2/t495VVd226zQqHCY97cEobvx8raSRr5c0YK7/N7Qf3ldnhjlXw+KbwJqfhdKLT6OkUiQ9HhItLYSb2uio8+NpCePxpPB1aQjG9ERTepKaE59sVKtO4TBGceq6sKt82FOd2PQxTKPLMY4bg3H8OHQ5bglAQojPTMJNDyTcfLoPaj3UdoQocZkY4TKRbzOiAp5Yf5CV/9pLNJHCadbxs+VTON+4A9bcA40fpA/WmqB0HqQSkIhAIpp+VVKg0oBaC2rN4UULGv1Hrxo9aLRw4R9AJ49EZ0u4qQ3vjn34q2sJtvgIR1WEo2rCcQ2RhI5oQkM8oSauNZLUmnrsC/SfNEocozqGyZjCbNFgdRmxuM3pp8MceswOA2anCXO+E51T/n8KIT4i4aYHEm5Ozr7WADc/t42dDX4A5pW7GVdgYTHbWFj3B5yd20/+Ij9qA+2J9yER/UeJx4nW1BDZs5euPdX4W4MENDn4cOJL2vDGTEQTJ/HklpLCEA9g0kQxm8DqNGAtsGErcmIry8c2IgeryyBTXwgxjEi46YGEm5MXS6S4f3U1v12zj1S3fz0Kc1WVlKjaiKl0aPUmdAYzeoMRrVaDWkmhIoWaFBolSZ5Zw/QRFiYXmLBoUpCKQzIGc64BuX0x6CXjKVIphVRKIRkIEtlfQ+hgPYEmL8H2Lrp8MUJdKcIJLTGViZjaTFRrIa419zge0MdplDhGVRSDNoFJn8JkBLNVg92lw5FvxlVoxZDrRONyos3Lk9tiQgxiEm56IOGm9+xrDWZuYR3s6OLQ4ddA5OiRkHuiVsGsMhdnTshnQYWbMreFXKtefhENU6mUQleLF++eQ/gONOGv70yHoWA6CEVVZqIGB0mt6bjOp4/5MUQ96VuhegMqvQF0etR6HWYzWK0aLHYNNocOq9tITrkLS6Ebtc2GSn18IUsI0fck3PRAwk3fi8ST+MNxvOE4vnAcT1eMeLL7P7OUorCr0c/be1upbAkcdQ6zXkOZ20yZ20yB3YjVqMVm1GIzaLEatVj0Wkx6DWa9BqNOg1mvJaUoBCIJgpEEgUicQCRBIJrgqkXlqNUSlIaKVCxGormZ0KFGAg0dhLxhQt4o4a4EoZBCKKIimDDSpViJqY8vAH3chMqnKG5aByoVarsdjd2O2mZFY7GittnQ2KyoLVbsX/g85pkz++ATCiGO5UR+f8sNa9HrjLp04Mi399wp+AvTi7n1vAnUe0K8XdnG23tb2dvkp8kfIRRLsrc5wN7mo4PPibp4Tgl2owxYN1So9Xr0ZWXoy8pwfsq+0XACX2uIrs4QyfYOEm0tJJubSbY0E+v0Ek7oCKWMhDETVlkIq62YlGD6YEUh5fOR8vmOeW7j1KkSboQYoCTciKwrcZn56oKRfHXBSACiiST1njC1HSEOdXTR2RUjEE181CoTjdMVTRKJJwnFkoTjScKxJCoV2I06bEYtVkO6pcdq1KGksvwBRdYYTFryR9phpB0oBCYfx1H/hRKLkfT7Sfp8JH0+UsEgyUCAVCBIqiu9bpw0sY9LL4T4rCTciAHHoNUwOs/K6LyeB5oToq+o9Hq0ubloc3OzXRQhxGcgveWEEEIIMaRIuBFCCCHEkDIgws2DDz5IeXk5RqOR+fPns3Hjxh73f+GFF5gwYQJGo5GpU6fy6quv9lNJhRBCCDHQZT3cPPfcc6xYsYIf//jHfPDBB0yfPp1zzjmH1tbWY+6/bt06vvzlL3PNNdewdetWli9fzvLly9m5c2c/l1wIIYQQA1HWx7mZP38+c+fO5YEHHgAglUpRWlrKjTfeyK233nrU/pdeeildXV3885//zGxbsGABM2bM4OGHH/7U68k4N0IIIcTgcyK/v7PachOLxdiyZQtLly7NbFOr1SxdupT169cf85j169d32x/gnHPO+cT9o9Eofr+/2yKEEEKIoSur4aa9vZ1kMklBQUG37QUFBTQ3Nx/zmObm5hPaf+XKlTgcjsxSWlraO4UXQgghxICU9T43fe22227D5/Nllrq6umwXSQghhBB9KKuD+OXm5qLRaGhpaem2vaWlhcLCwmMeU1hYeEL7GwwGDAZD7xRYCCGEEANeVltu9Ho9s2fPZvXq1ZltqVSK1atXs3DhwmMes3Dhwm77A6xateoT9xdCCCHE8JL16RdWrFjBlVdeyZw5c5g3bx733XcfXV1dXHXVVQBcccUVjBgxgpUrVwJw0003cfrpp3PvvfeybNkynn32WTZv3swjjzySzY8hhBBCiAEi6+Hm0ksvpa2tjTvvvJPm5mZmzJjBa6+9luk0XFtbi1r9UQPTokWLePrpp/nRj37E7bffztixY/nb3/7GlClTsvURhBBCCDGAZH2cm/4m49wIIYQQg8+gGedGCCGEEKK3Zf22VH870lAlg/kJIYQQg8eR39vHc8Np2IWbQCAAIIP5CSGEEINQIBDA4XD0uM+w63OTSqVobGzEZrOhUqk+83n8fj+lpaXU1dVJ350BQOpjYJH6GFikPgYeqZMTpygKgUCA4uLibg8aHcuwa7lRq9WUlJT02vnsdrv8wxxApD4GFqmPgUXqY+CROjkxn9Zic4R0KBZCCCHEkCLhRgghhBBDioSbz8hgMPDjH/9Y5q0aIKQ+Bhapj4FF6mPgkTrpW8OuQ7EQQgghhjZpuRFCCCHEkCLhRgghhBBDioQbIYQQQgwpEm6EEEIIMaRIuPkMHnzwQcrLyzEajcyfP5+NGzdmu0jDwsqVK5k7dy42m438/HyWL19OZWVlt30ikQjXX389OTk5WK1WLrroIlpaWrJU4uHlnnvuQaVScfPNN2e2SX30v4aGBr7yla+Qk5ODyWRi6tSpbN68OfO+oijceeedFBUVYTKZWLp0KdXV1Vks8dCVTCa54447GDVqFCaTidGjR3PXXXd1mxtJ6qOPKOKEPPvss4per1ceffRRZdeuXco3vvENxel0Ki0tLdku2pB3zjnnKI899piyc+dOZdu2bcr555+vlJWVKcFgMLPPtddeq5SWliqrV69WNm/erCxYsEBZtGhRFks9PGzcuFEpLy9Xpk2bptx0002Z7VIf/auzs1MZOXKk8rWvfU15//33lQMHDiivv/66sm/fvsw+99xzj+JwOJS//e1vyvbt25UvfvGLyqhRo5RwOJzFkg9Nd999t5KTk6P885//VGpqapQXXnhBsVqtyq9//evMPlIffUPCzQmaN2+ecv3112e+TiaTSnFxsbJy5coslmp4am1tVQDlnXfeURRFUbxer6LT6ZQXXnghs8+ePXsUQFm/fn22ijnkBQIBZezYscqqVauU008/PRNupD763w9+8APllFNO+cT3U6mUUlhYqPziF7/IbPN6vYrBYFCeeeaZ/ijisLJs2TLl6quv7rbtwgsvVC6//HJFUaQ++pLcljoBsViMLVu2sHTp0sw2tVrN0qVLWb9+fRZLNjz5fD4A3G43AFu2bCEej3ernwkTJlBWVib104euv/56li1b1u37DlIf2fCPf/yDOXPmcPHFF5Ofn8/MmTP5/e9/n3m/pqaG5ubmbnXicDiYP3++1EkfWLRoEatXr6aqqgqA7du3895773HeeecBUh99adhNnHky2tvbSSaTFBQUdNteUFDA3r17s1Sq4SmVSnHzzTezePFipkyZAkBzczN6vR6n09lt34KCApqbm7NQyqHv2Wef5YMPPmDTpk1HvSf10f8OHDjAQw89xIoVK7j99tvZtGkT3/nOd9Dr9Vx55ZWZ7/uxfoZJnfS+W2+9Fb/fz4QJE9BoNCSTSe6++24uv/xyAKmPPiThRgxK119/PTt37uS9997LdlGGrbq6Om666SZWrVqF0WjMdnEE6dA/Z84cfvaznwEwc+ZMdu7cycMPP8yVV16Z5dINP88//zxPPfUUTz/9NJMnT2bbtm3cfPPNFBcXS330MbktdQJyc3PRaDRHPe3R0tJCYWFhlko1/Nxwww3885//5O2336akpCSzvbCwkFgshtfr7ba/1E/f2LJlC62trcyaNQutVotWq+Wdd97h/vvvR6vVUlBQIPXRz4qKipg0aVK3bRMnTqS2thYg832Xn2H943vf+x633norl112GVOnTuWrX/0qt9xyCytXrgSkPvqShJsToNfrmT17NqtXr85sS6VSrF69moULF2axZMODoijccMMN/PWvf+Wtt95i1KhR3d6fPXs2Op2uW/1UVlZSW1sr9dMHlixZwo4dO9i2bVtmmTNnDpdffnlmXeqjfy1evPio4RGqqqoYOXIkAKNGjaKwsLBbnfj9ft5//32pkz4QCoVQq7v/mtVoNKRSKUDqo09lu0fzYPPss88qBoNBefzxx5Xdu3cr3/zmNxWn06k0Nzdnu2hD3re//W3F4XAoa9asUZqamjJLKBTK7HPttdcqZWVlyltvvaVs3rxZWbhwobJw4cIslnp4+fjTUooi9dHfNm7cqGi1WuXuu+9Wqqurlaeeekoxm83Kk08+mdnnnnvuUZxOp/L3v/9d+fDDD5UvfelL8uhxH7nyyiuVESNGZB4Ff+mll5Tc3Fzl+9//fmYfqY++IeHmM/jNb36jlJWVKXq9Xpk3b56yYcOGbBdpWACOuTz22GOZfcLhsHLdddcpLpdLMZvNygUXXKA0NTVlr9DDzH+GG6mP/vfyyy8rU6ZMUQwGgzJhwgTlkUce6fZ+KpVS7rjjDqWgoEAxGAzKkiVLlMrKyiyVdmjz+/3KTTfdpJSVlSlGo1GpqKhQfvjDHyrRaDSzj9RH31ApyseGShRCCCGEGOSkz40QQgghhhQJN0IIIYQYUiTcCCGEEGJIkXAjhBBCiCFFwo0QQgghhhQJN0IIIYQYUiTcCCGEEGJIkXAjhBj21qxZg0qlOmoeLCHE4CThRgghhBBDioQbIYQQQgwpEm6EEFmXSqVYuXIlo0aNwmQyMX36dF588UXgo1tGr7zyCtOmTcNoNLJgwQJ27tzZ7Rx/+ctfmDx5MgaDgfLycu69995u70ejUX7wgx9QWlqKwWBgzJgx/PGPf+y2z5YtW5gzZw5ms5lFixYdNcO2EGJwkHAjhMi6lStX8sQTT/Dwww+za9cubrnlFr7yla/wzjvvZPb53ve+x7333sumTZvIy8vjC1/4AvF4HEiHkksuuYTLLruMHTt28L//+7/ccccdPP7445njr7jiCp555hnuv/9+9uzZw+9+9zusVmu3cvzwhz/k3nvvZfPmzWi1Wq6++up++fxCiN4lE2cKIbIqGo3idrt58803WbhwYWb717/+dUKhEN/85jc588wzefbZZ7n00ksB6OzspKSkhMcff5xLLrmEyy+/nLa2Nt54443M8d///vd55ZVX2LVrF1VVVYwfP55Vq1axdOnSo8qwZs0azjzzTN58802WLFkCwKuvvsqyZcsIh8MYjcY+/i4IIXqTtNwIIbJq3759hEIhzj77bKxWa2Z54okn2L9/f2a/jwcft9vN+PHj2bNnDwB79uxh8eLF3c67ePFiqqurSSaTbNu2DY1Gw+mnn95jWaZNm5ZZLyoqAqC1tfWkP6MQon9ps10AIcTwFgwGAXjllVcYMWJEt/cMBkO3gPNZmUym49pPp9Nl1lUqFZDuDySEGFyk5UYIkVWTJk3CYDBQW1vLmDFjui2lpaWZ/TZs2JBZ93g8VFVVMXHiRAAmTpzI2rVru5137dq1jBs3Do1Gw9SpU0mlUt368Aghhi5puRFCZJXNZuO73/0ut9xyC6lUilNOOQWfz8fatWux2+2MHDkSgJ/+9Kfk5ORQUFDAD3/4Q3Jzc1m+fDkA//M//8PcuXO56667uPTSS1m/fj0PPPAAv/3tbwEoLy/nyiuv5Oqrr+b+++9n+vTpHDp0iNbWVi655JJsfXQhRB+RcCOEyLq77rqLvLw8Vq5cyYEDB3A6ncyaNYvbb789c1vonnvu4aabbqK6upoZM2bw8ssvo9frAZg1axbPP/88d955J3fddRdFRUX89Kc/5Wtf+1rmGg899BC333471113HR0dHZSVlXH77bdn4+MKIfqYPC0lhBjQjjzJ5PF4cDqd2S6OEGIQkD43QgghhBhSJNwIIYQQYkiR21JCCCGEGFKk5UYIIYQQQ4qEGyGEEEIMKRJuhBBCCDGkSLgRQgghxJAi4UYIIYQQQ4qEGyGEEEIMKRJuhBBCCDGkSLgRQgghxJAi4UYIIYQQQ8r/BziS1ijSLlCuAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot of loss vs epoch for each optimizer\n",
    "for opt in loss_dict:\n",
    "    if(opt=='VGD'):continue\n",
    "    plt.plot([i for i in range(1, num_epochs_dict[opt]+1)], loss_dict[opt], label = opt)\n",
    "plt.title('plots of average training error (y-axis) vs. epochs (x-axis) ')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('average training error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m bar2 \u001b[39m=\u001b[39m (eval_dict[opt][\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m opt \u001b[39min\u001b[39;00m opts)\n\u001b[0;32m      7\u001b[0m \u001b[39m# Positions of the bars on the x-axis\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m x_pos \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39;49m(bar1))\n\u001b[0;32m     10\u001b[0m \u001b[39m# Set up the figure and axis objects\u001b[39;00m\n\u001b[0;32m     11\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots()\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'generator' has no len()"
     ]
    }
   ],
   "source": [
    "# plot of accuracies of each optimizer\n",
    "# Heights of the bars\n",
    "opts = list(eval_dict.keys())\n",
    "bar1 = (eval_dict[opt][0] for opt in opts)\n",
    "bar2 = (eval_dict[opt][1] for opt in opts)\n",
    "\n",
    "# Positions of the bars on the x-axis\n",
    "x_pos = range(len(bar1))\n",
    "\n",
    "# Set up the figure and axis objects\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the bars\n",
    "ax.bar(x_pos, bar1, width=0.4, color='b', label='training accuracy')\n",
    "ax.bar([i + 0.4 for i in x_pos], bar2, width=0.4, color='g', label='validation accuracy')\n",
    "\n",
    "# Set the x-axis labels and title\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(opts)\n",
    "ax.set_xlabel('optimizer')\n",
    "ax.set_ylabel('accuracy')\n",
    "ax.set_title('plot of accuracies of each optimizer')\n",
    "\n",
    "# Add a legend\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
